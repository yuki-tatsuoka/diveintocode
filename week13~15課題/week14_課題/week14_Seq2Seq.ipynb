{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】機械翻訳の実行とコードリーディング\n",
    "以下のサンプルコードは、短い英語からフランス語への変換を行うものです。これを動かしてください。\n",
    "その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめてください。以下のようにどこからどこの行が何をしているかを記述してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n",
      "Epoch 1/100\n",
      "125/125 [==============================] - 21s 153ms/step - loss: 1.0388 - val_loss: 1.0769\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.9245 - val_loss: 1.0480\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.8770 - val_loss: 0.9532\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.8346 - val_loss: 0.9159\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.8006 - val_loss: 0.8866\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.7751 - val_loss: 0.8823\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.7503 - val_loss: 0.8460\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.7268 - val_loss: 0.8701\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.7139 - val_loss: 0.8039\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.6952 - val_loss: 0.7800\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.6771 - val_loss: 0.7864\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.6643 - val_loss: 0.7892\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.6517 - val_loss: 0.7624\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.6459 - val_loss: 0.7268\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.6273 - val_loss: 0.7138\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.6191 - val_loss: 0.7378\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.6116 - val_loss: 0.7222\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.6050 - val_loss: 0.6901\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.5954 - val_loss: 0.6912\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 23s 181ms/step - loss: 0.5861 - val_loss: 0.6864\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.5797 - val_loss: 0.6965\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5721 - val_loss: 0.6795\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 18s 148ms/step - loss: 0.5655 - val_loss: 0.6654\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5532 - val_loss: 0.6412\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.5356 - val_loss: 0.6289\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.5332 - val_loss: 0.6457\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.5225 - val_loss: 0.6356\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.5140 - val_loss: 0.6179\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.5096 - val_loss: 0.6293\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 17s 138ms/step - loss: 0.5064 - val_loss: 0.6134\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.4988 - val_loss: 0.6150\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 17s 139ms/step - loss: 0.4977 - val_loss: 0.6172\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.4876 - val_loss: 0.6109\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.4874 - val_loss: 0.6075\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 18s 140ms/step - loss: 0.4791 - val_loss: 0.6135\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 17s 136ms/step - loss: 0.4732 - val_loss: 0.6027\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 0.4687 - val_loss: 0.6058\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.4689 - val_loss: 0.5987\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.4669 - val_loss: 0.6053\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.4610 - val_loss: 0.5944\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.4620 - val_loss: 0.5887\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.4538 - val_loss: 0.5961\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 23s 180ms/step - loss: 0.4507 - val_loss: 0.5880\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.4455 - val_loss: 0.5958\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.4439 - val_loss: 0.5925\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.4421 - val_loss: 0.5892\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.4402 - val_loss: 0.5845\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.4347 - val_loss: 0.5797\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.4351 - val_loss: 0.5827\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.4302 - val_loss: 0.5746\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.4251 - val_loss: 0.5852\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.4257 - val_loss: 0.5716\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 18s 147ms/step - loss: 0.4199 - val_loss: 0.5788\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.4187 - val_loss: 0.5712\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.4207 - val_loss: 0.5710\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 18s 144ms/step - loss: 0.4161 - val_loss: 0.5848\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.4132 - val_loss: 0.5688\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 20s 160ms/step - loss: 0.4116 - val_loss: 0.5645\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.4107 - val_loss: 0.5762\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 0.4074 - val_loss: 0.5665\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 0.4074 - val_loss: 0.5672\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.4033 - val_loss: 0.5729\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 18s 146ms/step - loss: 0.4018 - val_loss: 0.5605\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.4015 - val_loss: 0.5640\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 18s 141ms/step - loss: 0.3988 - val_loss: 0.5575\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 19s 156ms/step - loss: 0.3952 - val_loss: 0.5690\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3975 - val_loss: 0.5613\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.3969 - val_loss: 0.5633\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3937 - val_loss: 0.5600\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.3916 - val_loss: 0.5738\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.3862 - val_loss: 0.5519\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.3869 - val_loss: 0.5533\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.3867 - val_loss: 0.5577\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3806 - val_loss: 0.5595\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.3820 - val_loss: 0.5648\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.3834 - val_loss: 0.5511\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.3815 - val_loss: 0.5504\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.3794 - val_loss: 0.5608\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.3769 - val_loss: 0.5662\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.3743 - val_loss: 0.5634\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.3753 - val_loss: 0.5551\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.3724 - val_loss: 0.5540\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.3718 - val_loss: 0.5645\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.3683 - val_loss: 0.5571\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3658 - val_loss: 0.5645\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3690 - val_loss: 0.5505\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.3636 - val_loss: 0.5558\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.3638 - val_loss: 0.5541\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.3650 - val_loss: 0.5489\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.3630 - val_loss: 0.5724\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.3622 - val_loss: 0.5453\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.3616 - val_loss: 0.5472\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.3597 - val_loss: 0.5534\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.3610 - val_loss: 0.5493\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.3568 - val_loss: 0.5667\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 0.3544 - val_loss: 0.5491\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.3526 - val_loss: 0.5500\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.3533 - val_loss: 0.5503\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.3537 - val_loss: 0.5454\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.3542 - val_loss: 0.5517\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Continne !\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Continne !\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Continne !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salle.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salle.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Fule !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Faisen ent.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: File !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Contez !\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Sure.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attende !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attende !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attende !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence: Décante !\n",
      "\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence: Décante !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Continne !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Continne !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprende.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprende.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'es ai .\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai fait.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je l'ai fait.\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: J'ai parti.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Findon.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Détends-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Sourie !\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Sourie !\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Sourie !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attande !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attande !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attande !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sart !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sart !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sart !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sart !\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Mangez-le.\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Mangez-le.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Sore !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Sore !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas cous aime.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas cous aime.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas cous aime.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Ala .\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Ala .\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Ala .\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Alle.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Alle.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Alle.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Sonte.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Sonte.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi la partie.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi la partie.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis troi de toi.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis troi de toi.\n",
      "\n",
      "-\n",
      "Input sentence: I fled.\n",
      "Decoded sentence: Je suis trois !\n",
      "\n",
      "-\n",
      "Input sentence: I knit.\n",
      "Decoded sentence: Je te chante.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: J'ai mala.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai parti.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Je pais !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# レイヤー層のインポート\n",
    "# from __future__ import print_functionは3系の機能を2系で使う事が可能でprintに言及している\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # バッチ\n",
    "epochs = 100  # エポック\n",
    "latent_dim = 256  # エンコーダーの次元 \n",
    "num_samples = 10000 # サンプル数\n",
    "\n",
    "# パス\n",
    "data_path = '/Users/yuki.tatsuoka/diveintocode/week13~15課題/week14_課題/fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "lines = open(data_path).read().split('\\n') # 文章単位で区切る\n",
    "\n",
    "# 文章１万を抽出\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # タブ単位で区切ってEOSにする\n",
    "    input_text, target_text, _ = line.split('\\t') \n",
    "    '''\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    '''\n",
    "    # 細かい文字をappendで追加する\n",
    "    target_text = '\\t' + target_text + '\\n' # '\\n'＝ endシーケンス\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# 文字の長さを保存\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# 学習用のエンコーダ、デコーダデータとラベルデータを用意\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # デコーダの推論の時系列\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# Seq2Seqの層の作成（エンコーダ）\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# 途中経過のencoder_outputsを今回は採用しない\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Seq2Seqの層の作成（デコーダ）\n",
    "# 基本エンコーダと同じ\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# 今回は途中経過を推論に活かす\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# functionalAPIに入れる\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 学習 \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# 保存\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# デコーダを増やす処理\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# inputとtargetの値を保存\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 予測 \n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # EOSを設定\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # バッチシーケンスのループ\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # トークンのサンプル\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 最長か、endで終了する\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # ターゲットシーケンスの更新\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】イメージキャプショニングの学習済みモデルの実行\n",
    "上記実装において 5. Test the model の項目を実行してください。また、自身で用意した画像に対しても文章を生成してください。これらに対してどういった文章が出力されたかを記録して提出してください。\n",
    "\n",
    "\n",
    "データセットからの学習は行わず、学習済みの重みをダウンロードして利用します。\n",
    "\n",
    "\n",
    "注意点として、デフォルトで設定されている重みのファイル名と、ダウンロードできる重みのファイル名は異なっています。ここは書き換える必要があります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> a group of stuffed animals in a room . <end>\r\n"
     ]
    }
   ],
   "source": [
    "!python /Users/yuki.tatsuoka/diveintocode/week13~15課題/week14_課題/pytorch-tutorial/tutorials/03-advanced/image_captioning/sample.py  --image '/Users/yuki.tatsuoka/Desktop/IMG_4802.JPG'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('/Users/yuki.tatsuoka/Desktop/IMG_4802.JPG')\n",
    "cv2.imshow('dorakue',img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Kerasで動かしたい場合はどうするかを調査\n",
    "PyTorchによる実装を動かしましたが、何らかの理由からKerasで動かしたい状況が考えられます。どういった手順を踏むことになるか調査し、できるだけ詳しく説明してください。\n",
    "\n",
    "\n",
    "特に今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMdnnにて、pytorchからkerasに移行する事が可能。\n",
    "# MMdnnはMicrosoft Researchにより開発が進められているオープンソースの深層学習モデルの変換と可視化を行うツール\n",
    "# ちなみにkerasだけでなく、tensorflowにも使う事が可能\n",
    "# 重みについては、VGG19, Resnet, MobileNetなどの事前学習済みの重みも使う事が可能。\n",
    "\n",
    "# 具体的には\n",
    "# mmdownload -f keras を実行すると、kerasで利用できる重みが出現する\n",
    "# 例：Support frameworks: {'yolo2', 'inception_v3', 'xception', 'nasnet', 'densenet', 'vgg19', 'mobilenet', 'resnet50', 'vgg16', 'inception_resnet_v2'}\n",
    "\n",
    "# また、重みを保存する場合\n",
    "# mmdownload -f keras -n inception_v3 -o ./ にて保存する事が可能。\n",
    "# 重みを他でも利用する事が可能なので、互換性を持たせて利用する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
