{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class内で完結するためのクラス\n",
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes, w_x, b, w_h, batch_size):\n",
    "        # n_features = 入力の特徴量\n",
    "        # n_nodes = ノード数\n",
    "        #self.optimizer = optimizer\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.w_x = w_x\n",
    "        self.b = b\n",
    "        self.w_h = w_h\n",
    "        self.batch_size = batch_size\n",
    "        self.h = np.zeros((self.batch_size, self.n_nodes))\n",
    "        \n",
    "    def forward_caluculation(self, X):\n",
    "       # a = (batch_size, n_nodes)⇨x@self.w + ht_1@wh\n",
    "       # h = (batch_size, n_nodes)⇨x@self.wの次元\n",
    "       # ht_1 =  (batch_size, n_nodes)⇨hをそのまま引き継ぐ\n",
    "       # w_h = (n_nodes, n_nodes)→ 次元は消去法？    \n",
    "    \n",
    "        self.a = X@self.w_x + self.h@self.w_h + self.b \n",
    "        self.a = np.tanh(self.a)\n",
    "        self.h = deepcopy(self.a)\n",
    "        return self.a\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            self.forward_caluculation(X[:, i, :])\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes, w_x, b, w_h, batch_size):\n",
    "        # n_features = 入力の特徴量\n",
    "        # n_nodes = ノード数\n",
    "        #self.optimizer = optimizer\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        #self.initial = initial\n",
    "        self.w_x = w_x\n",
    "        self.b = b\n",
    "        self.w_h = w_h\n",
    "        self.batch_size = batch_size\n",
    "        self.h = np.zeros((self.batch_size, self.n_nodes))\n",
    "        \n",
    "    def forward(self, X):\n",
    "       # a = (batch_size, n_nodes)⇨x@self.w + ht_1@wh\n",
    "       # h = (batch_size, n_nodes)⇨x@self.wの次元\n",
    "       # ht_1 =  (batch_size, n_nodes)⇨hをそのまま引き継ぐ\n",
    "       # w_h = (n_nodes, n_nodes)→ 次元は消去法？    \n",
    "    \n",
    "        a = X@self.w_x + self.h@self.w_h + self.b \n",
    "        a = np.tanh(a)\n",
    "        self.h = deepcopy(a)\n",
    "        return a\n",
    "    '''\n",
    "    def backward(self, dA):\n",
    "        dZ = dA@self.W.T \n",
    "        self.dW = self.X.T@dA \n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        \n",
    "        self.optimizer.update(self) # update(self)のselfはこのクラス（FC）のインスタンスを渡したもの\n",
    "        return dZ\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 2), (2, 4), (4, 4))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証用データ\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "\n",
    "x.shape, w_x.shape, w_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォワードの結果と同じもの。\n",
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "[[0.792209   0.8141834  0.83404912 0.84977719]]\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# クラス内ではないが、フォワードの結果と同じもの。\n",
    "rnn = SimpleRNN(n_features=n_features, n_nodes=n_nodes,w_x=w_x, b=b, w_h=w_h, batch_size=batch_size)\n",
    "h = rnn.forward(x[:,0,:])\n",
    "print(h)\n",
    "h1 = rnn.forward(x[:,1,:])\n",
    "print(h1)\n",
    "h2 = rnn.forward(x[:,2,:])\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sequences # 単語の数\n",
    "n_features # ノードの次元\n",
    "#例　\n",
    "# sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class内で完結するためのクラス\n",
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes, w_x, b, w_h, batch_size):\n",
    "        # n_features = 入力の特徴量\n",
    "        # n_nodes = ノード数\n",
    "        #self.optimizer = optimizer\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.w_x = w_x\n",
    "        self.b = b\n",
    "        self.w_h = w_h\n",
    "        self.batch_size = batch_size\n",
    "        self.h = np.zeros((self.batch_size, self.n_nodes))\n",
    "        \n",
    "    def forward_caluculation(self, X):\n",
    "       # a = (batch_size, n_nodes)⇨x@self.w + ht_1@wh\n",
    "       # h = (batch_size, n_nodes)⇨x@self.wの次元\n",
    "       # ht_1 =  (batch_size, n_nodes)⇨hをそのまま引き継ぐ\n",
    "       # w_h = (n_nodes, n_nodes)→ 次元は消去法？    \n",
    "    \n",
    "        self.a = X@self.w_x + self.h@self.w_h + self.b \n",
    "        self.a = np.tanh(self.a)\n",
    "        self.h = deepcopy(self.a)\n",
    "        return self.a\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for t in range(X.shape[1]):\n",
    "            self.forward_caluculation(X[:, t, :])\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上記の検証内容と一致\n",
    "rnn = SimpleRNN(n_features=n_features, n_nodes=n_nodes,w_x=w_x, b=b, w_h=w_h, batch_size=batch_size)\n",
    "h = rnn.forward(x)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
