{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "\n",
    "ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "\n",
    "手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データインポート\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dir_path = \"/Users/yuki.tatsuoka/Downloads/house-prices-advanced-regression-techniques (1)/\"\n",
    "\n",
    "df = pd.read_csv(dir_path + \"train.csv\")\n",
    "\n",
    "# 説明変数と目的変数を抽出\n",
    "X = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegresssion MSE 2495554898.668321\n",
      "SVR MSE 7842006843.379719\n",
      "決定木 MSE 2187354209.919045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X)\n",
    "\n",
    "# 訓練用とテスト用に分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 重回帰モデル\n",
    "model_linear = LinearRegression().fit(X_train, y_train)\n",
    "pred_linear = model_linear.predict(X_test)\n",
    "print(\"LinearRegresssion MSE {}\".format(mean_squared_error(pred_linear, y_test)))\n",
    "\n",
    "# SVM\n",
    "model_svm = SVR().fit(X_train, y_train)\n",
    "pred_svm = model_svm.predict(X_test)\n",
    "print(\"SVR MSE {}\".format(mean_squared_error(pred_svm, y_test)))\n",
    "\n",
    "# 決定木\n",
    "model_tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "pred_tree = model_tree.predict(X_test)\n",
    "print(\"決定木 MSE {}\".format(mean_squared_error(pred_tree, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重回帰、svm MSE 4103105570.4928856\n",
      "重回帰、決定木 MSE 1857217416.7137222\n",
      "svm、決定木 MSE 3249053550.3458395\n"
     ]
    }
   ],
   "source": [
    "p =0.5\n",
    "p2 = 0.5\n",
    "\n",
    "# ブレンディング：重回帰、svm\n",
    "pred = pred_linear*p + pred_svm *p2\n",
    "print(\"重回帰、svm MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：重回帰、決定木\n",
    "pred = pred_linear*p + pred_tree *p2\n",
    "print(\"重回帰、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：svm, 決定木\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "print(\"svm、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "\n",
    "# 単体の結果\n",
    "#LinearRegresssion MSE 2495554898.668321\n",
    "#SVR MSE 7842006843.379719\n",
    "#決定木 MSE 2217101574.2512364\n",
    "\n",
    "# 考察(上回るパターン1)\n",
    "# 重回帰と、決定木のブレンディングが3種類より優秀 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm、決定木、重回帰 MSE 2701398910.915856\n"
     ]
    }
   ],
   "source": [
    "p =1/3\n",
    "p2 = 1/3\n",
    "p3 = 1/3\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2 + pred_linear*p3\n",
    "print(\"svm、決定木、重回帰 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# 単体の結果\n",
    "# LinearRegresssion MSE 2495554898.668321\n",
    "# SVR MSE 7842006843.379719\n",
    "# 決定木 MSE 2217101574.2512364\n",
    "\n",
    "# 考察\n",
    "# 平均値で所得すると、svmと比べて大分良くなる\n",
    "# linearや決定木よりスコアが好ましい訳ではないが、それに近しいスコアが出力されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm、決定木、重回帰 MSE 2158189798.039097\n"
     ]
    }
   ],
   "source": [
    "p =0.2\n",
    "p2 = 0.5\n",
    "p3 = 0.3\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2 + pred_linear*p3\n",
    "print(\"svm、決定木、重回帰 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# 単体の結果\n",
    "# LinearRegresssion MSE 2495554898.668321\n",
    "# SVR MSE 7842006843.379719\n",
    "# 決定木 MSE 2217101574.2512364\n",
    "\n",
    "# 考察(上回るパターン2)\n",
    "# SVMよりは確実に良い。linearより優れており、決定木とほぼ同じスコア\n",
    "# 現在のスコアだけで判断できないが、汎化性を考えるとこちらの方が優秀そう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重回帰、svm MSE 3204323229.6355877\n",
      "重回帰、決定木 MSE 1996335496.476371\n",
      "svm、決定木 MSE 4662484393.246541\n"
     ]
    }
   ],
   "source": [
    "p =0.7\n",
    "p2 = 0.3\n",
    "\n",
    "# ブレンディング：重回帰、svm\n",
    "pred = pred_linear*p + pred_svm *p2\n",
    "print(\"重回帰、svm MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：重回帰、決定木\n",
    "pred = pred_linear*p + pred_tree *p2\n",
    "print(\"重回帰、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：svm, 決定木\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "print(\"svm、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "\n",
    "# 単体の結果\n",
    "# LinearRegresssion MSE 2495554898.668321\n",
    "# SVR MSE 7842006843.379719\n",
    "# 決定木 MSE 2217101574.2512364\n",
    "\n",
    "# ブレンド5:5\n",
    "# 重回帰、svm MSE 4103105570.4928856\n",
    "# 重回帰、決定木 MSE 1916348710.9180086\n",
    "# svm、決定木 MSE 3407691598.4304748\n",
    "\n",
    "# 考察(上回るパターン3)\n",
    "# ブレンドの比率を変更したところ、結果は全て悪化\n",
    "# 汎化性だけを考慮すると、定かではないが。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300638391.428304\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# バギング\n",
    "baging_iter = 3\n",
    "score = np.zeros(baging_iter)\n",
    "\n",
    "# 訓練データとテストデータを分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2) \n",
    "\n",
    "# for分で訓練データの中で、更に分割する\n",
    "for n in range(baging_iter):\n",
    "    X_train_div, X_valid, y_train_div, y_valid = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    model_tree = DecisionTreeRegressor()\n",
    "    model_tree.fit(X_train_div, y_train_div)\n",
    "    pred = model_tree.predict(X_test)\n",
    "    score[n] = mean_squared_error(pred, y_test)\n",
    "    \n",
    "    # nが最後の値ならば、平均を求める\n",
    "    if n == (baging_iter -1):\n",
    "        score_mean = np.mean(score)\n",
    "        print(score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは \n",
    "K\n",
    "0\n",
    "=\n",
    "3\n",
    ",\n",
    "M\n",
    "0\n",
    "=\n",
    "2\n",
    " 程度にします。\n",
    "\n",
    "\n",
    "《学習時》\n",
    "\n",
    "\n",
    "（ステージ \n",
    "0\n",
    " ）\n",
    "\n",
    "\n",
    "学習データを \n",
    "K\n",
    "0\n",
    " 個に分割する。\n",
    "分割した内の \n",
    "(\n",
    "K\n",
    "0\n",
    "−\n",
    "1\n",
    ")\n",
    " 個をまとめて学習用データ、残り \n",
    "1\n",
    " 個を推定用データとする組み合わせが \n",
    "K\n",
    "0\n",
    " 個作れる。\n",
    "あるモデルのインスタンスを \n",
    "K\n",
    "0\n",
    " 個用意し、異なる学習用データを使い学習する。\n",
    "それぞれの学習済みモデルに対して、使っていない残り \n",
    "1\n",
    " 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "さらに、異なるモデルのインスタンスも \n",
    "K\n",
    "0\n",
    " 個用意し、同様のことを行う。モデルが \n",
    "M\n",
    "0\n",
    " 個あれば、 \n",
    "M\n",
    "0\n",
    " 個のブレンドデータが得られる。\n",
    "\n",
    "（ステージ \n",
    "n\n",
    " ）\n",
    "\n",
    "\n",
    "ステージ \n",
    "n\n",
    "−\n",
    "1\n",
    " のブレンドデータを\n",
    "M\n",
    "n\n",
    "−\n",
    "1\n",
    " 次元の特徴量を持つ学習用データと考え、 \n",
    "K\n",
    "n\n",
    " 個に分割する。以下同様である。\n",
    "\n",
    "（ステージ \n",
    "N\n",
    " ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ \n",
    "N\n",
    "−\n",
    "1\n",
    " の \n",
    "M\n",
    "N\n",
    "−\n",
    "1\n",
    " 個のブレンドデータを\n",
    "M\n",
    "N\n",
    "−\n",
    "1\n",
    " 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "《推定時》\n",
    "\n",
    "\n",
    "（ステージ \n",
    "0\n",
    " ）\n",
    "\n",
    "\n",
    "テストデータを \n",
    "K\n",
    "0\n",
    "×\n",
    "M\n",
    "0\n",
    " 個の学習済みモデルに入力し、\n",
    "K\n",
    "0\n",
    "×\n",
    "M\n",
    "0\n",
    " 個の推定値を得る。これを \n",
    "K\n",
    "0\n",
    " の軸で平均値を求め \n",
    "M\n",
    "0\n",
    " 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ \n",
    "n\n",
    " ）\n",
    "\n",
    "\n",
    "ステージ \n",
    "n\n",
    "−\n",
    "1\n",
    " で得たブレンドテストを \n",
    "K\n",
    "n\n",
    "×\n",
    "M\n",
    "n\n",
    " 個の学習済みモデルに入力し、\n",
    "K\n",
    "n\n",
    "×\n",
    "M\n",
    "n\n",
    " 個の推定値を得る。これを \n",
    "K\n",
    "n\n",
    " の軸で平均値を求め \n",
    "M\n",
    "0\n",
    " 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ \n",
    "N\n",
    " ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ \n",
    "N\n",
    "−\n",
    "1\n",
    " で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "y= y[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2) \n",
    "\n",
    "# 再度fitさせる特徴量と、validデータの推定値を出力する\n",
    "def stacking(clf, X_train, y_train, X_test, k):\n",
    "    kf = KFold(n_splits=k,shuffle=True)\n",
    "    pred_list =[]\n",
    "    preds_test_list = []\n",
    "    va_index = []\n",
    "    \n",
    "    # k-fold\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_div, X_valid =  X_train[train_index],  X_train[test_index]\n",
    "        y_train_div, y_valid = y_train[train_index], y_train[test_index] \n",
    "        print(X_train_div.shape,X_valid.shape, y_train_div.shape, y_valid.shape)\n",
    "        \n",
    "        # モデルに学習させ、予測値とインデックスを保存\n",
    "        clf.fit(X_train_div, y_train_div)\n",
    "        pred = clf.predict(X_valid)\n",
    "        pred_list.append(pred)\n",
    "        pred_test = clf.predict(X_test)\n",
    "        preds_test_list.append(pred_test)\n",
    "        va_index.append(test_index)\n",
    "    print(preds_test_list[0].shape)\n",
    "    \n",
    "    # 　バリデーションデータに対する予測値をマージし、その後元の順序に戻す\n",
    "    va_index = np.concatenate(va_index)\n",
    "    #print(va_index)\n",
    "    pred_list = np.concatenate(pred_list, axis=0) # 特徴量だから？\n",
    "    order = np.argsort(va_index)\n",
    "    pred_train = pred_list[order]\n",
    "    \n",
    "    # validデータに対する予測値の平均を取る\n",
    "    #　print(preds_test_list)    \n",
    "    preds_test_list = np.mean(preds_test_list, axis=0) # こっちは不要マージ (292,1)が出力される　kfoldのfor文では(292,1)が３回繰り返されている\n",
    "    return pred_train, preds_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292, 1)\n",
      "(1168, 1)\n",
      "(292, 1)\n",
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292, 1)\n",
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292,)\n"
     ]
    }
   ],
   "source": [
    "pred_train, pred_test = stacking(model_linear, X_train,y_train, X_test, k=3)\n",
    "print(pred_train.shape)# 訓練データをvalidで予測したもの（次の特徴量）→再度fitさせるもの\n",
    "print(pred_test.shape)# 訓練データでtestを予測したもの\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "# 訓練用データと、predで出力された値をsvm、linearで所得する\n",
    "pred_train_linear, pred_test_linear = stacking(model_linear, X_train,y_train, X_test, k=3)\n",
    "pred_train_svm, pred_test_svm = stacking(model_svm, X_train,y_train, X_test, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2308422094.396909\n",
      "6685585201.941444\n",
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292,)\n",
      "3847599546.224315\n"
     ]
    }
   ],
   "source": [
    "# 1層目評価\n",
    "print(mean_squared_error(pred_train_linear, y_train))\n",
    "print(mean_squared_error(pred_train_svm, y_train))\n",
    "\n",
    "# 予測値を特徴量としてデータフレームを作成する\n",
    "train_x_2 = pd.DataFrame({'pred_linear':pred_train_linear.reshape(-1), 'pred_svm': pred_train_svm.reshape(-1)})\n",
    "test_x_2 = pd.DataFrame({'pred_linear':pred_test_linear.reshape(-1), 'pred_svm':pred_test_svm.reshape(-1)})\n",
    "\n",
    "train_x_2 = np.array(train_x_2)\n",
    "\n",
    "# ２層目\n",
    "pred_train_tree, pred_test_tree = stacking(model_tree, train_x_2,y_train, test_x_2, k=3)\n",
    "\n",
    "print(mean_squared_error(pred_train_tree, y_train))\n",
    "\n",
    "# 総論\n",
    "#　決定木を最後のモデルに使った結果、悪化している\n",
    "# 元々決定木単体のスコアが良いので、他のスコアの悪い物を混ぜている分低くなっているだけだと思う。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
