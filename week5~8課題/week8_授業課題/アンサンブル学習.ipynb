{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dir_path = \"/Users/yuki.tatsuoka/Downloads/house-prices-advanced-regression-techniques (1)/\"\n",
    "\n",
    "df = pd.read_csv(dir_path + \"train.csv\")\n",
    "\n",
    "# 説明変数と目的変数を抽出\n",
    "X = df.loc[:, ['GrLivArea','YearBuilt']]\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "\n",
    "ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "\n",
    "手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegresssion MSE 2495554898.668321\n",
      "SVR MSE 7842006843.379719\n",
      "決定木 MSE 2187354209.919045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit_transform(X)\n",
    "\n",
    "# 訓練用とテスト用に分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 重回帰モデル\n",
    "model_linear = LinearRegression().fit(X_train, y_train)\n",
    "pred_linear = model_linear.predict(X_test)\n",
    "print(\"LinearRegresssion MSE {}\".format(mean_squared_error(pred_linear, y_test)))\n",
    "\n",
    "# SVM\n",
    "model_svm = SVR().fit(X_train, y_train)\n",
    "pred_svm = model_svm.predict(X_test)\n",
    "print(\"SVR MSE {}\".format(mean_squared_error(pred_svm, y_test)))\n",
    "\n",
    "# 決定木\n",
    "model_tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "pred_tree = model_tree.predict(X_test)\n",
    "print(\"決定木 MSE {}\".format(mean_squared_error(pred_tree, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重回帰、svm MSE 4103105570.4928856\n",
      "重回帰、決定木 MSE 1857217416.7137222\n",
      "svm、決定木 MSE 3249053550.3458395\n"
     ]
    }
   ],
   "source": [
    "p =0.5\n",
    "p2 = 0.5\n",
    "\n",
    "# ブレンディング：重回帰、svm\n",
    "pred = pred_linear*p + pred_svm *p2\n",
    "print(\"重回帰、svm MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：重回帰、決定木\n",
    "pred = pred_linear*p + pred_tree *p2\n",
    "print(\"重回帰、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：svm, 決定木\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "print(\"svm、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "\n",
    "# 単体の結果\n",
    "#LinearRegresssion MSE 2495554898.668321\n",
    "#SVR MSE 7842006843.379719\n",
    "#決定木 MSE 2217101574.2512364\n",
    "\n",
    "# 考察(上回るパターン1)\n",
    "# 重回帰と、決定木のブレンディングが3種類より優秀 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm、決定木、重回帰 MSE 2701398910.915856\n"
     ]
    }
   ],
   "source": [
    "p =1/3\n",
    "p2 = 1/3\n",
    "p3 = 1/3\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2 + pred_linear*p3\n",
    "print(\"svm、決定木、重回帰 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# 単体の結果\n",
    "# LinearRegresssion MSE 2495554898.668321\n",
    "# SVR MSE 7842006843.379719\n",
    "# 決定木 MSE 2217101574.2512364\n",
    "\n",
    "# 考察\n",
    "# 平均値で所得すると、svmと比べて大分良くなる\n",
    "# linearや決定木よりスコアが好ましい訳ではないが、それに近しいスコアが出力されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm、決定木、重回帰 MSE 2158189798.039097\n"
     ]
    }
   ],
   "source": [
    "p =0.2\n",
    "p2 = 0.5\n",
    "p3 = 0.3\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2 + pred_linear*p3\n",
    "print(\"svm、決定木、重回帰 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# 単体の結果\n",
    "# LinearRegresssion MSE 2495554898.668321\n",
    "# SVR MSE 7842006843.379719\n",
    "# 決定木 MSE 2217101574.2512364\n",
    "\n",
    "# 考察(上回るパターン2)\n",
    "# SVMよりは確実に良い。linearより優れており、決定木とほぼ同じスコア\n",
    "# 現在のスコアだけで判断できないが、汎化性を考えるとこちらの方が優秀そう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重回帰、svm MSE 3204323229.6355877\n",
      "重回帰、決定木 MSE 1996335496.476371\n",
      "svm、決定木 MSE 4662484393.246541\n"
     ]
    }
   ],
   "source": [
    "p =0.7\n",
    "p2 = 0.3\n",
    "\n",
    "# ブレンディング：重回帰、svm\n",
    "pred = pred_linear*p + pred_svm *p2\n",
    "print(\"重回帰、svm MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：重回帰、決定木\n",
    "pred = pred_linear*p + pred_tree *p2\n",
    "print(\"重回帰、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "# ブレンディング：svm, 決定木\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "print(\"svm、決定木 MSE {}\".format(mean_squared_error(pred, y_test)))\n",
    "\n",
    "#  ブレンディング：svm, 決定木、重回帰\n",
    "pred = pred_svm*p + pred_tree *p2\n",
    "\n",
    "# 単体の結果\n",
    "# LinearRegresssion MSE 2495554898.668321\n",
    "# SVR MSE 7842006843.379719\n",
    "# 決定木 MSE 2217101574.2512364\n",
    "\n",
    "# ブレンド5:5\n",
    "# 重回帰、svm MSE 4103105570.4928856\n",
    "# 重回帰、決定木 MSE 1916348710.9180086\n",
    "# svm、決定木 MSE 3407691598.4304748\n",
    "\n",
    "# 考察(上回るパターン3)\n",
    "# ブレンドの比率を変更したところ、結果は全て悪化\n",
    "# 汎化性だけを考慮すると、定かではないが。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300638391.428304\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# バギング\n",
    "baging_iter = 3\n",
    "score = np.zeros(baging_iter)\n",
    "\n",
    "# 訓練データとテストデータを分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2) \n",
    "\n",
    "# for分で訓練データの中で、更に分割する\n",
    "for n in range(baging_iter):\n",
    "    X_train_div, X_valid, y_train_div, y_valid = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    model_tree = DecisionTreeRegressor()\n",
    "    model_tree.fit(X_train_div, y_train_div)\n",
    "    pred = model_tree.predict(X_test)\n",
    "    score[n] = mean_squared_error(pred, y_test)\n",
    "    \n",
    "    # nが最後の値ならば、平均を求める\n",
    "    if n == (baging_iter -1):\n",
    "        score_mean = np.mean(score)\n",
    "        print(score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは \n",
    "K\n",
    "0\n",
    "=\n",
    "3\n",
    ",\n",
    "M\n",
    "0\n",
    "=\n",
    "2\n",
    " 程度にします。\n",
    "\n",
    "\n",
    "《学習時》\n",
    "\n",
    "\n",
    "（ステージ \n",
    "0\n",
    " ）\n",
    "\n",
    "\n",
    "学習データを \n",
    "K\n",
    "0\n",
    " 個に分割する。\n",
    "分割した内の \n",
    "(\n",
    "K\n",
    "0\n",
    "−\n",
    "1\n",
    ")\n",
    " 個をまとめて学習用データ、残り \n",
    "1\n",
    " 個を推定用データとする組み合わせが \n",
    "K\n",
    "0\n",
    " 個作れる。\n",
    "あるモデルのインスタンスを \n",
    "K\n",
    "0\n",
    " 個用意し、異なる学習用データを使い学習する。\n",
    "それぞれの学習済みモデルに対して、使っていない残り \n",
    "1\n",
    " 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "さらに、異なるモデルのインスタンスも \n",
    "K\n",
    "0\n",
    " 個用意し、同様のことを行う。モデルが \n",
    "M\n",
    "0\n",
    " 個あれば、 \n",
    "M\n",
    "0\n",
    " 個のブレンドデータが得られる。\n",
    "\n",
    "（ステージ \n",
    "n\n",
    " ）\n",
    "\n",
    "\n",
    "ステージ \n",
    "n\n",
    "−\n",
    "1\n",
    " のブレンドデータを\n",
    "M\n",
    "n\n",
    "−\n",
    "1\n",
    " 次元の特徴量を持つ学習用データと考え、 \n",
    "K\n",
    "n\n",
    " 個に分割する。以下同様である。\n",
    "\n",
    "（ステージ \n",
    "N\n",
    " ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ \n",
    "N\n",
    "−\n",
    "1\n",
    " の \n",
    "M\n",
    "N\n",
    "−\n",
    "1\n",
    " 個のブレンドデータを\n",
    "M\n",
    "N\n",
    "−\n",
    "1\n",
    " 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "《推定時》\n",
    "\n",
    "\n",
    "（ステージ \n",
    "0\n",
    " ）\n",
    "\n",
    "\n",
    "テストデータを \n",
    "K\n",
    "0\n",
    "×\n",
    "M\n",
    "0\n",
    " 個の学習済みモデルに入力し、\n",
    "K\n",
    "0\n",
    "×\n",
    "M\n",
    "0\n",
    " 個の推定値を得る。これを \n",
    "K\n",
    "0\n",
    " の軸で平均値を求め \n",
    "M\n",
    "0\n",
    " 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ \n",
    "n\n",
    " ）\n",
    "\n",
    "\n",
    "ステージ \n",
    "n\n",
    "−\n",
    "1\n",
    " で得たブレンドテストを \n",
    "K\n",
    "n\n",
    "×\n",
    "M\n",
    "n\n",
    " 個の学習済みモデルに入力し、\n",
    "K\n",
    "n\n",
    "×\n",
    "M\n",
    "n\n",
    " 個の推定値を得る。これを \n",
    "K\n",
    "n\n",
    " の軸で平均値を求め \n",
    "M\n",
    "0\n",
    " 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ \n",
    "N\n",
    " ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ \n",
    "N\n",
    "−\n",
    "1\n",
    " で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# スタッキング\\ndef stacking(clf, X, y, k):\\n    kf = KFold(n_splits=k,shuffle=True)\\n    pred_list =[]\\n    \\n    \\n    # k-fold\\n    for train_index, test_index in kf.split(X):\\n        X_train =  X[train_index]\\n        X_valid = X[test_index]\\n        y_train = y[train_index]\\n        y_valid = y[test_index] \\n        print(X_train.shape,X_valid.shape, y_train.shape, y_valid.shape)\\n        \\n        # モデルに学習させる \\n        clf.fit(X_train, y_train)\\n        pred = clf.predict(X_valid)\\n        pred_list.append(pred)\\n    \\n    \\nstacking(model_linear, X_scaler,y, 3)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# スタッキング\n",
    "def stacking(clf, X, y, k):\n",
    "    kf = KFold(n_splits=k,shuffle=True)\n",
    "    pred_list =[]\n",
    "    \n",
    "    \n",
    "    # k-fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train =  X[train_index]\n",
    "        X_valid = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_valid = y[test_index] \n",
    "        print(X_train.shape,X_valid.shape, y_train.shape, y_valid.shape)\n",
    "        \n",
    "        # モデルに学習させる \n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_valid)\n",
    "        pred_list.append(pred)\n",
    "    \n",
    "    \n",
    "stacking(model_linear, X_scaler,y, 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "y= y[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2) \n",
    "\n",
    "# 再度fitさせる特徴量と、validデータの推定値を出力する\n",
    "def stacking(clf, X_train, y_train, X_test, k):\n",
    "    kf = KFold(n_splits=k,shuffle=True)\n",
    "    pred_list =[]\n",
    "    preds_test_list = []\n",
    "    va_index = []\n",
    "    \n",
    "    # k-fold\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_div, X_valid =  X_train[train_index],  X_train[test_index]\n",
    "        y_train_div, y_valid = y_train[train_index], y_train[test_index] \n",
    "        print(X_train_div.shape,X_valid.shape, y_train_div.shape, y_valid.shape)\n",
    "        \n",
    "        # モデルに学習させ、予測値とインデックスを保存\n",
    "        clf.fit(X_train_div, y_train_div)\n",
    "        pred = clf.predict(X_valid)\n",
    "        pred_list.append(pred)\n",
    "        pred_test = clf.predict(X_test)\n",
    "        preds_test_list.append(pred_test)\n",
    "        va_index.append(test_index)\n",
    "    print(preds_test_list[0].shape)\n",
    "    \n",
    "    # 　バリデーションデータに対する予測値をマージし、その後元の順序に戻す\n",
    "    va_index = np.concatenate(va_index)\n",
    "    #print(va_index)\n",
    "    pred_list = np.concatenate(pred_list, axis=0) # 特徴量だから？\n",
    "    order = np.argsort(va_index)\n",
    "    pred_train = pred_list[order]\n",
    "    \n",
    "    # validデータに対する予測値の平均を取る\n",
    "    #　print(preds_test_list)    \n",
    "    preds_test_list = np.mean(preds_test_list, axis=0) # こっちは不要マージ (292,1)が出力される　kfoldのfor文では(292,1)が３回繰り返されている\n",
    "    return pred_train, preds_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292, 1)\n",
      "[[242569.19035872]\n",
      " [221307.95203906]\n",
      " [201044.14686943]\n",
      " ...\n",
      " [148707.68807106]\n",
      " [133268.45604989]\n",
      " [111008.48737617]]\n",
      "[[160564.79608349]\n",
      " [216456.59112799]\n",
      " [221482.80431785]\n",
      " [215880.03145594]\n",
      " [184287.93649568]\n",
      " [173846.2534274 ]\n",
      " [228622.76058671]\n",
      " [201666.69497221]\n",
      " [149055.11468482]\n",
      " [238922.01906715]\n",
      " [205604.32494488]\n",
      " [320064.49421885]\n",
      " [229611.01614515]\n",
      " [ 81484.69460184]\n",
      " [218688.0940539 ]\n",
      " [318484.47234333]\n",
      " [111254.94259552]\n",
      " [258420.64040391]\n",
      " [135984.69790981]\n",
      " [182244.84932481]\n",
      " [149359.25035969]\n",
      " [236737.62008   ]\n",
      " [121012.50578022]\n",
      " [140361.65559241]\n",
      " [144057.46047401]\n",
      " [140066.60678136]\n",
      " [350651.08704857]\n",
      " [144864.53283019]\n",
      " [246201.24722521]\n",
      " [121181.44942003]\n",
      " [232557.23786075]\n",
      " [246411.17502256]\n",
      " [258598.85652358]\n",
      " [128525.21370501]\n",
      " [130690.14057692]\n",
      " [378187.79218926]\n",
      " [ 59262.94606479]\n",
      " [217284.06275492]\n",
      " [220551.85233354]\n",
      " [296978.55536752]\n",
      " [126799.61454301]\n",
      " [ 45052.76227969]\n",
      " [316641.11333443]\n",
      " [274798.90375956]\n",
      " [ 98313.59807434]\n",
      " [ 87269.76343756]\n",
      " [253668.89113832]\n",
      " [256594.71358317]\n",
      " [ 93537.18406781]\n",
      " [236317.76448531]\n",
      " [124025.30354981]\n",
      " [159365.49995615]\n",
      " [132300.0198191 ]\n",
      " [224288.82698873]\n",
      " [198505.35283356]\n",
      " [176938.05242947]\n",
      " [191637.82056188]\n",
      " [154507.30325059]\n",
      " [131202.16412206]\n",
      " [204521.86150893]\n",
      " [208255.68346548]\n",
      " [186132.22266009]\n",
      " [177369.03481505]\n",
      " [135554.45706371]\n",
      " [122073.45717387]\n",
      " [103075.54697531]\n",
      " [186149.46923221]\n",
      " [236796.96358121]\n",
      " [117159.81120521]\n",
      " [221270.83659343]\n",
      " [141929.43790548]\n",
      " [ 69538.65257584]\n",
      " [134400.4105641 ]\n",
      " [127442.75026905]\n",
      " [146486.65163482]\n",
      " [203827.54198998]\n",
      " [248781.94983767]\n",
      " [173153.97383553]\n",
      " [127380.25406921]\n",
      " [110625.15920348]\n",
      " [ 78590.58383467]\n",
      " [152040.28063088]\n",
      " [194733.51380206]\n",
      " [136904.33748719]\n",
      " [186787.59794858]\n",
      " [305503.07082013]\n",
      " [171621.98305428]\n",
      " [197056.2575229 ]\n",
      " [149698.25041086]\n",
      " [196983.56168768]\n",
      " [205257.16518541]\n",
      " [106385.43348302]\n",
      " [183217.71262217]\n",
      " [227010.84141746]\n",
      " [127582.02215825]\n",
      " [111222.11814628]\n",
      " [137836.21662701]\n",
      " [266362.47640325]\n",
      " [224910.45067246]\n",
      " [123022.76850185]\n",
      " [401602.53145639]\n",
      " [249364.62929094]\n",
      " [260701.28719565]\n",
      " [228599.20861733]\n",
      " [283831.36290315]\n",
      " [173250.22164012]\n",
      " [151213.92177551]\n",
      " [173370.0214141 ]\n",
      " [238412.03544909]\n",
      " [132125.88355358]\n",
      " [ 93198.18401663]\n",
      " [105326.52201644]\n",
      " [120410.35421172]\n",
      " [168592.68025205]\n",
      " [183737.8958756 ]\n",
      " [133579.05871839]\n",
      " [257930.12890108]\n",
      " [110303.59134046]\n",
      " [168320.25625486]\n",
      " [182632.99324182]\n",
      " [174083.81304828]\n",
      " [214911.24801274]\n",
      " [238576.89923475]\n",
      " [107424.87283437]\n",
      " [162196.18736797]\n",
      " [146727.36395432]\n",
      " [138248.83966892]\n",
      " [ 88450.51460519]\n",
      " [ 88666.56218376]\n",
      " [299043.15458069]\n",
      " [195223.09814937]\n",
      " [ 73028.7951303 ]\n",
      " [127427.35800797]\n",
      " [217468.39865581]\n",
      " [186456.7576057 ]\n",
      " [223620.09936623]\n",
      " [169004.37613844]\n",
      " [225053.80241581]\n",
      " [226893.08157056]\n",
      " [145305.90046719]\n",
      " [218068.51029724]\n",
      " [193253.81958528]\n",
      " [ 95017.99105615]\n",
      " [136232.45716606]\n",
      " [264768.73096163]\n",
      " [ 91095.75334456]\n",
      " [ 69495.62849123]\n",
      " [214495.4722722 ]\n",
      " [230238.7596101 ]\n",
      " [207698.59590866]\n",
      " [200325.15987308]\n",
      " [202149.04676674]\n",
      " [199447.43160875]\n",
      " [141267.94283577]\n",
      " [196407.92917115]\n",
      " [196703.90513772]\n",
      " [216683.95111349]\n",
      " [292985.66174779]\n",
      " [112916.0056306 ]\n",
      " [134424.88968899]\n",
      " [248384.5334408 ]\n",
      " [116407.07534057]\n",
      " [121061.64964605]\n",
      " [117810.17948403]\n",
      " [223778.65775463]\n",
      " [216915.39095314]\n",
      " [189588.61360979]\n",
      " [ 81876.918373  ]\n",
      " [187418.49411217]\n",
      " [181526.97783648]\n",
      " [181593.55389047]\n",
      " [250743.06869346]\n",
      " [221928.25180899]\n",
      " [238624.00317351]\n",
      " [151869.297064  ]\n",
      " [132964.66758744]\n",
      " [121700.70551794]\n",
      " [204220.69291665]\n",
      " [263832.9575837 ]\n",
      " [236619.8602331 ]\n",
      " [109660.45561443]\n",
      " [313845.29029892]\n",
      " [260934.76696238]\n",
      " [159303.00375631]\n",
      " [186953.5745058 ]\n",
      " [153007.02414702]\n",
      " [196215.43356196]\n",
      " [111707.62263944]\n",
      " [226020.54593195]\n",
      " [150765.32158574]\n",
      " [151606.14554667]\n",
      " [205465.05305568]\n",
      " [ 62714.14438878]\n",
      " [133260.64355401]\n",
      " [140041.0148849 ]\n",
      " [223875.09117526]\n",
      " [242083.3612058 ]\n",
      " [210165.61852837]\n",
      " [124712.57613202]\n",
      " [ 87130.49154836]\n",
      " [217484.71807241]\n",
      " [122045.82535034]\n",
      " [227885.41698315]\n",
      " [225808.39259149]\n",
      " [275306.84745054]\n",
      " [163444.62736114]\n",
      " [480935.58482   ]\n",
      " [130227.26089763]\n",
      " [225834.17010398]\n",
      " [146448.82017591]\n",
      " [127208.15773077]\n",
      " [206008.78827849]\n",
      " [240289.14606274]\n",
      " [219866.80529445]\n",
      " [ 52485.54181648]\n",
      " [237953.23562394]\n",
      " [119951.55438657]\n",
      " [117955.57115446]\n",
      " [174438.01974449]\n",
      " [121360.59269521]\n",
      " [135039.46643599]\n",
      " [108115.11249917]\n",
      " [280652.58857634]\n",
      " [227332.40928048]\n",
      " [151498.39971911]\n",
      " [118892.64291999]\n",
      " [260808.84740718]\n",
      " [155209.96809388]\n",
      " [144440.59738136]\n",
      " [145437.01264809]\n",
      " [198596.40801245]\n",
      " [ 71856.20367097]\n",
      " [228901.30436512]\n",
      " [296368.24409072]\n",
      " [203803.9900206 ]\n",
      " [232007.38285671]\n",
      " [285504.66550068]\n",
      " [262034.66258649]\n",
      " [152751.10518247]\n",
      " [130597.97262648]\n",
      " [231477.92712342]\n",
      " [121340.19342446]\n",
      " [166260.8496674 ]\n",
      " [203531.56602341]\n",
      " [229818.90401542]\n",
      " [182795.81710041]\n",
      " [189418.55719843]\n",
      " [242606.69715786]\n",
      " [192009.45944625]\n",
      " [225716.41025708]\n",
      " [220123.83703055]\n",
      " [195277.43464092]\n",
      " [154781.76717485]\n",
      " [114751.2049312 ]\n",
      " [177101.80344357]\n",
      " [267862.75550682]\n",
      " [221455.17249432]\n",
      " [179289.35512935]\n",
      " [144982.29267709]\n",
      " [124587.58373234]\n",
      " [182994.43249082]\n",
      " [123316.70454134]\n",
      " [125458.07944387]\n",
      " [216938.94292252]\n",
      " [110364.97476875]\n",
      " [120135.89028746]\n",
      " [216943.02277667]\n",
      " [142279.75036359]\n",
      " [178620.62750686]\n",
      " [203488.5419388 ]\n",
      " [150671.11370822]\n",
      " [129140.71760752]\n",
      " [239380.8188923 ]\n",
      " [250925.36466728]\n",
      " [109772.0956801 ]\n",
      " [230924.91942075]\n",
      " [ 74578.21809971]\n",
      " [ 42511.93105321]\n",
      " [165707.84196473]\n",
      " [225885.35389689]\n",
      " [226110.67395532]\n",
      " [199625.64772842]\n",
      " [109668.61532272]\n",
      " [240328.0902932 ]\n",
      " [152069.02522597]\n",
      " [123663.86430081]\n",
      " [150814.46545157]\n",
      " [100233.7327726 ]\n",
      " [198731.6000475 ]\n",
      " [171920.92610343]\n",
      " [207377.02804564]\n",
      " [195274.28194228]]\n"
     ]
    }
   ],
   "source": [
    "pred_train, pred_test = stacking(model_linear, X_train,y_train, X_test, k=3)\n",
    "\n",
    "print(pred_train)# xの特徴量→再度fitさせるもの\n",
    "print(pred_test)# yの予測値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292, 1)\n",
      "(1168, 1)\n",
      "(292, 1)\n",
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292, 1)\n",
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292,)\n"
     ]
    }
   ],
   "source": [
    "pred_train, pred_test = stacking(model_linear, X_train,y_train, X_test, k=3)\n",
    "print(pred_train.shape)# 訓練データをvalidで予測したもの（次の特徴量）→再度fitさせるもの\n",
    "print(pred_test.shape)# 訓練データでtestを予測したもの\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "# 訓練用データと、predで出力された値をsvm、linearで所得する\n",
    "pred_train_linear, pred_test_linear = stacking(model_linear, X_train,y_train, X_test, k=3)\n",
    "pred_train_svm, pred_test_svm = stacking(model_svm, X_train,y_train, X_test, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2308422094.396909\n",
      "6685585201.941444\n",
      "(778, 2) (390, 2) (778, 1) (390, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(779, 2) (389, 2) (779, 1) (389, 1)\n",
      "(292,)\n",
      "3847599546.224315\n"
     ]
    }
   ],
   "source": [
    "# 1層目評価\n",
    "print(mean_squared_error(pred_train_linear, y_train))\n",
    "print(mean_squared_error(pred_train_svm, y_train))\n",
    "\n",
    "# 予測値を特徴量としてデータフレームを作成する\n",
    "train_x_2 = pd.DataFrame({'pred_linear':pred_train_linear.reshape(-1), 'pred_svm': pred_train_svm.reshape(-1)})\n",
    "test_x_2 = pd.DataFrame({'pred_linear':pred_test_linear.reshape(-1), 'pred_svm':pred_test_svm.reshape(-1)})\n",
    "\n",
    "train_x_2 = np.array(train_x_2)\n",
    "\n",
    "# ２層目\n",
    "pred_train_tree, pred_test_tree = stacking(model_tree, train_x_2,y_train, test_x_2, k=3)\n",
    "\n",
    "print(mean_squared_error(pred_train_tree, y_train))\n",
    "\n",
    "# 総論\n",
    "#　決定木を最後のモデルに使った結果、悪化している\n",
    "# 元々決定木単体のスコアが良いので、他のスコアの悪い物を混ぜている分低くなっているだけだと思う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2) \n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        pred_linear = np.zeros((len(X_valid), k))\n",
    "        X_train_div =  X_train[train_index]\n",
    "        X_valid = X_train[test_index]\n",
    "        y_train_div = y[train_index]\n",
    "        y_valid = y[test_index] \n",
    "        print(X_train_div.shape,X_valid.shape, y_train_div.shape, y_valid.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# k-foldでは関数化する必要がある\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 3 #　分割する数\n",
    "m = 2 # モデルの数\n",
    "stage = 2 # 何回ブレンドデータを作るか\n",
    "\n",
    "# 訓練データとテストデータを分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.2) \n",
    "\n",
    "# kfold\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "pred_linear = 0\n",
    "\n",
    "# モデルの数分for分が回っている\n",
    "for m in range(m):\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "        pred_linear = np.zeros((len(X_valid), k))\n",
    "        X_train_div =  X_scaler[train_index]\n",
    "        X_valid = X_scaler[test_index]\n",
    "        y_train_div = y[train_index]\n",
    "        y_valid = y[test_index] \n",
    "        \n",
    "        \n",
    "        if m == 0:\n",
    "            model_linear = LinearRegression().fit(X_train_div, y_train_div)\n",
    "            pred_linear = model_linear.predict(X_valid)\n",
    "            print(pred_linear.shape)\n",
    "        print(X_train_div.shape,X_valid.shape, y_train_div.shape, y_valid.shape)\n",
    "    \n",
    "    \n",
    "    #    for i in range(m):\n",
    "      #      if i == 0:\n",
    "        \n",
    "        #    if i == 1:\n",
    "            \n",
    "        \n",
    "        # モデルの学習→validデータで推定を行う（ブレンドデータ）→平均化する\n",
    "        # for分オンリーでは無理かも\n",
    "        # どっかで作成されたpredデータ同士をマージする必要性がある\n",
    "        # ラスト１つの学習器でやるが、１つは何でもおk\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
