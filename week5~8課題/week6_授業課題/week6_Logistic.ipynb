{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_hypothesis(X):\n",
    "    np.random.seed(0)\n",
    "    theta = np.random.rand(1,X.shape[1])\n",
    "    hypothesis = 1/(1 + np.exp(-1 * (np.dot(X, theta.T))))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.7, 1.4],\n",
       "       [4.5, 1.5],\n",
       "       [4.9, 1.5],\n",
       "       [4. , 1.3],\n",
       "       [4.6, 1.5],\n",
       "       [4.5, 1.3],\n",
       "       [4.7, 1.6],\n",
       "       [3.3, 1. ],\n",
       "       [4.6, 1.3],\n",
       "       [3.9, 1.4],\n",
       "       [3.5, 1. ],\n",
       "       [4.2, 1.5],\n",
       "       [4. , 1. ],\n",
       "       [4.7, 1.4],\n",
       "       [3.6, 1.3],\n",
       "       [4.4, 1.4],\n",
       "       [4.5, 1.5],\n",
       "       [4.1, 1. ],\n",
       "       [4.5, 1.5],\n",
       "       [3.9, 1.1],\n",
       "       [4.8, 1.8],\n",
       "       [4. , 1.3],\n",
       "       [4.9, 1.5],\n",
       "       [4.7, 1.2],\n",
       "       [4.3, 1.3],\n",
       "       [4.4, 1.4],\n",
       "       [4.8, 1.4],\n",
       "       [5. , 1.7],\n",
       "       [4.5, 1.5],\n",
       "       [3.5, 1. ],\n",
       "       [3.8, 1.1],\n",
       "       [3.7, 1. ],\n",
       "       [3.9, 1.2],\n",
       "       [5.1, 1.6],\n",
       "       [4.5, 1.5],\n",
       "       [4.5, 1.6],\n",
       "       [4.7, 1.5],\n",
       "       [4.4, 1.3],\n",
       "       [4.1, 1.3],\n",
       "       [4. , 1.3],\n",
       "       [4.4, 1.2],\n",
       "       [4.6, 1.4],\n",
       "       [4. , 1.2],\n",
       "       [3.3, 1. ],\n",
       "       [4.2, 1.3],\n",
       "       [4.2, 1.2],\n",
       "       [4.2, 1.3],\n",
       "       [4.3, 1.3],\n",
       "       [3. , 1.1],\n",
       "       [4.1, 1.3],\n",
       "       [6. , 2.5],\n",
       "       [5.1, 1.9],\n",
       "       [5.9, 2.1],\n",
       "       [5.6, 1.8],\n",
       "       [5.8, 2.2],\n",
       "       [6.6, 2.1],\n",
       "       [4.5, 1.7],\n",
       "       [6.3, 1.8],\n",
       "       [5.8, 1.8],\n",
       "       [6.1, 2.5],\n",
       "       [5.1, 2. ],\n",
       "       [5.3, 1.9],\n",
       "       [5.5, 2.1],\n",
       "       [5. , 2. ],\n",
       "       [5.1, 2.4],\n",
       "       [5.3, 2.3],\n",
       "       [5.5, 1.8],\n",
       "       [6.7, 2.2],\n",
       "       [6.9, 2.3],\n",
       "       [5. , 1.5],\n",
       "       [5.7, 2.3],\n",
       "       [4.9, 2. ],\n",
       "       [6.7, 2. ],\n",
       "       [4.9, 1.8],\n",
       "       [5.7, 2.1],\n",
       "       [6. , 1.8],\n",
       "       [4.8, 1.8],\n",
       "       [4.9, 1.8],\n",
       "       [5.6, 2.1],\n",
       "       [5.8, 1.6],\n",
       "       [6.1, 1.9],\n",
       "       [6.4, 2. ],\n",
       "       [5.6, 2.2],\n",
       "       [5.1, 1.5],\n",
       "       [5.6, 1.4],\n",
       "       [6.1, 2.3],\n",
       "       [5.6, 2.4],\n",
       "       [5.5, 1.8],\n",
       "       [4.8, 1.8],\n",
       "       [5.4, 2.1],\n",
       "       [5.6, 2.4],\n",
       "       [5.1, 2.3],\n",
       "       [5.1, 1.9],\n",
       "       [5.9, 2.3],\n",
       "       [5.7, 2.5],\n",
       "       [5.2, 2.3],\n",
       "       [5. , 1.9],\n",
       "       [5.2, 2. ],\n",
       "       [5.4, 2.3],\n",
       "       [5.1, 1.8]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:][50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "logistic_hypothesis(X)\n",
    "\n",
    "ones = np.ones((1,1))\n",
    "theta = np.random.rand(1,X.shape[1])\n",
    "theta = np.insert(theta, 0, ones).reshape(-1,1)\n",
    "\n",
    "(logistic_hypothesis(X) - y).shape\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題2\n",
    "def _gradient_descent(self, X, error, y):\n",
    "    error = (error - y).reshape(-1,1) # shape(100,1) - shape(100,1)\n",
    "    sum_error = np.sum(error * X, axis=0) #shape(100,5) * (100,1)をsumで縦方向に計算 (1,5)\n",
    "    self.theta[0] = self.theta[0] - self.lr*(sum_error[0]/X.shape[0]) \n",
    "    for i in range(1, X.shape[1]):\n",
    "            self.theta[i] = self.theta[i] - self.lr*(sum_error[i]/X.shape[0] + self.lamda*self.theta[i]/X.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def predict(X):\n",
    "    pred = logistic_hypothesis(X)\n",
    "    pred = np.where(pred < 0.5, 0, 1)\n",
    "    return pred\n",
    "\n",
    "predict(X_train).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02812838, 0.97187162],\n",
       "       [0.06685487, 0.93314513],\n",
       "       [0.01082082, 0.98917918],\n",
       "       [0.0154009 , 0.9845991 ],\n",
       "       [0.00601522, 0.99398478],\n",
       "       [0.01331033, 0.98668967],\n",
       "       [0.032314  , 0.967686  ],\n",
       "       [0.03179778, 0.96820222],\n",
       "       [0.05163972, 0.94836028],\n",
       "       [0.0490172 , 0.9509828 ],\n",
       "       [0.01382205, 0.98617795],\n",
       "       [0.01942372, 0.98057628],\n",
       "       [0.04142204, 0.95857796],\n",
       "       [0.01019861, 0.98980139],\n",
       "       [0.00895434, 0.99104566],\n",
       "       [0.07403668, 0.92596332],\n",
       "       [0.01901644, 0.98098356],\n",
       "       [0.02357465, 0.97642535],\n",
       "       [0.03407489, 0.96592511],\n",
       "       [0.00965923, 0.99034077],\n",
       "       [0.01302954, 0.98697046],\n",
       "       [0.02812838, 0.97187162],\n",
       "       [0.04057259, 0.95942741],\n",
       "       [0.02812838, 0.97187162],\n",
       "       [0.01435317, 0.98564683]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def predict_proba(X):\n",
    "    pred_0 = 1- logistic_hypothesis(X)\n",
    "    pred_1 = logistic_hypothesis(X)\n",
    "    pred_proba = np.hstack([pred_0, pred_1])\n",
    "    return pred_proba\n",
    "\n",
    "predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】目的関数\n",
    "以下の数式で表されるロジスティック回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "\n",
    "なお、この数式には正則化項が含まれています。\n",
    "\n",
    "\n",
    "＊数式が見切れる場合、DIVERを全画面にして御覧ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.942317341328838"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(X, y,lamda=0.01):\n",
    "    theta = np.random.rand(1,X.shape[1])\n",
    "    y = y.reshape(-1,1)\n",
    "    cross_entropy = (1/X.shape[0]) * np.sum((-y * np.log(logistic_hypothesis(X))) - ((1- y) * np.log(1-logistic_hypothesis(X))))\n",
    "    regularization_term = lamda/2*X.shape[0] * np.sum(theta**2)\n",
    "    cost_loss =  (cross_entropy + regularization_term).tolist()\n",
    "    return cost_loss\n",
    "\n",
    "cost_function(X_test, y_test)\n",
    "#(-y * np.log(logistic_hypothesis(X))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.14468515128649"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証用に調べたが、あまり間違っていなさそう。\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris.data[:, 2:][50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "pred_sklearn = model.predict(X_test)\n",
    "pred_sklearn\n",
    "\n",
    "log_loss(y_test, pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したirisデータセットのvirgicolorとvirginicaの2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合体版\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.lamda = 0.001\n",
    "        self.list_train_loss = []\n",
    "        self.list_val_loss = []\n",
    "        \n",
    "    # 問題１    \n",
    "    def _logistic_hypothesis(self, X):\n",
    "        z = np.dot(X, self.theta)\n",
    "        y_hot = 1/(1 + np.exp(-z))\n",
    "        return y_hot\n",
    "\n",
    "    # 問題2\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        \n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "        \n",
    "        # バイアスをthetaとX, X_valに追加\n",
    "        if self.bias:\n",
    "            # ブロードキャストで特徴量の先頭に１を追加\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            ones = np.ones((1,1))\n",
    "            self.theta = np.insert(self.theta, 0, ones) ###########\n",
    "        \n",
    "        # 訓練データを勾配計算からpredict〜cost_functionを計算\n",
    "        for i in range(self.iter):\n",
    "            error = self._logistic_hypothesis(X) #(75,1) \n",
    "            self._gradient_descent(X, error, y)\n",
    "            #y_pred = self.predict_proba(X)[:,1]\n",
    "            loss, _ = self.cost_function(X, y, error)\n",
    "            self.list_train_loss.append(loss)\n",
    "            \n",
    "            if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "                print(\"{}/{}, train_loss {}\".format(i+1, self.iter, loss))\n",
    "        \n",
    "\n",
    "        # テストデータを勾配計算からpredict〜cost_functionを計算\n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.val_theta = np.random.rand(X_val.shape[1], 1)\n",
    "            \n",
    "            if self.bias:\n",
    "                X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "                val_ones = np.ones((1,1))\n",
    "                self.val_theta = np.insert(self.val_theta, 0, val_ones)\n",
    "            \n",
    "            for i in range(self.iter):\n",
    "                error = self._logistic_hypothesis(X_val) # (25,1)\n",
    "                self._gradient_descent(X_val, error, y_val)\n",
    "                #y_val_pred = self.predict_proba(X_val)[:, 1]\n",
    "                _, val_loss = self.cost_function(X_val, y_val, error)\n",
    "                self.list_val_loss.append(val_loss)\n",
    "        \n",
    "                if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程を出力\n",
    "                    print(\"{}/{}, test_loss {}\".format(i+1, self.iter, val_loss))\n",
    "             \n",
    "            \n",
    "    # 問題3\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\" \n",
    "        if self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)    \n",
    "            \n",
    "        pred = self._logistic_hypothesis(X)\n",
    "        pred = np.where(pred < 0.5, 0, 1)\n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        pred_proba = self._logistic_hypothesis(X)\n",
    "        return pred_proba\n",
    "    \n",
    "    # 問題2\n",
    "    def _gradient_descent(self, X, error, y):\n",
    "        error = (error - y).reshape(-1,1) # shape(100,1) - shape(100,1)\n",
    "        sum_error = np.sum(error * X, axis=0) #shape(100,5) * (100,1)をsumで縦方向に計算 (1,5)\n",
    "        self.theta[0] = self.theta[0] - self.lr*(sum_error[0]/X.shape[0]) \n",
    "        for i in range(1, X.shape[1]):\n",
    "             self.theta[i] = self.theta[i] - self.lr*(sum_error[i]/X.shape[0] + self.lamda*self.theta[i]/X.shape[0]) \n",
    "    \n",
    "    \n",
    "    # 問題4\n",
    "    def cost_function(self, X,y, y_pred):\n",
    "        y = y.reshape(-1,1)\n",
    "        cross_entropy = 1/X.shape[0] * np.sum(-y * np.log(y_pred) - (1- y) * np.log(1-y_pred))\n",
    "        regularization_term = self.lamda * np.sum(self.theta**2) / 2*X.shape[0]\n",
    "        cost_loss =  cross_entropy + regularization_term\n",
    "        self.loss = cost_loss\n",
    "        self.val_loss = cost_loss\n",
    "        return self.loss, self.val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000, train_loss 184.8402954883731\n",
      "2/1000, train_loss 184.4101289996841\n",
      "3/1000, train_loss 183.98012861491594\n",
      "4/1000, train_loss 183.55029607550946\n",
      "5/1000, train_loss 183.12063314027858\n",
      "6/1000, train_loss 182.6911415855582\n",
      "7/1000, train_loss 182.26182320535224\n",
      "8/1000, train_loss 181.83267981148364\n",
      "9/1000, train_loss 181.40371323374384\n",
      "10/1000, train_loss 180.97492532004244\n",
      "11/1000, train_loss 180.54631793655966\n",
      "12/1000, train_loss 180.11789296789652\n",
      "13/1000, train_loss 179.6896523172272\n",
      "14/1000, train_loss 179.26159790645204\n",
      "15/1000, train_loss 178.8337316763501\n",
      "16/1000, train_loss 178.4060555867323\n",
      "17/1000, train_loss 177.97857161659664\n",
      "18/1000, train_loss 177.55128176428124\n",
      "19/1000, train_loss 177.12418804761973\n",
      "20/1000, train_loss 176.69729250409677\n",
      "21/1000, train_loss 176.2705971910024\n",
      "22/1000, train_loss 175.84410418558957\n",
      "23/1000, train_loss 175.4178155852291\n",
      "24/1000, train_loss 174.99173350756672\n",
      "25/1000, train_loss 174.56586009067982\n",
      "26/1000, train_loss 174.14019749323407\n",
      "27/1000, train_loss 173.71474789464122\n",
      "28/1000, train_loss 173.2895134952162\n",
      "29/1000, train_loss 172.8644965163347\n",
      "30/1000, train_loss 172.43969920059084\n",
      "31/1000, train_loss 172.01512381195536\n",
      "32/1000, train_loss 171.5907726359328\n",
      "33/1000, train_loss 171.16664797971998\n",
      "34/1000, train_loss 170.7427521723634\n",
      "35/1000, train_loss 170.3190875649177\n",
      "36/1000, train_loss 169.89565653060325\n",
      "37/1000, train_loss 169.47246146496317\n",
      "38/1000, train_loss 169.04950478602186\n",
      "39/1000, train_loss 168.62678893444215\n",
      "40/1000, train_loss 168.20431637368162\n",
      "41/1000, train_loss 167.7820895901505\n",
      "42/1000, train_loss 167.36011109336778\n",
      "43/1000, train_loss 166.93838341611786\n",
      "44/1000, train_loss 166.51690911460557\n",
      "45/1000, train_loss 166.0956907686126\n",
      "46/1000, train_loss 165.6747309816517\n",
      "47/1000, train_loss 165.25403238112173\n",
      "48/1000, train_loss 164.8335976184613\n",
      "49/1000, train_loss 164.4134293693016\n",
      "50/1000, train_loss 163.9935303336198\n",
      "51/1000, train_loss 163.57390323589044\n",
      "52/1000, train_loss 163.1545508252367\n",
      "53/1000, train_loss 162.73547587558036\n",
      "54/1000, train_loss 162.31668118579208\n",
      "55/1000, train_loss 161.898169579839\n",
      "56/1000, train_loss 161.47994390693273\n",
      "57/1000, train_loss 161.0620070416757\n",
      "58/1000, train_loss 160.64436188420666\n",
      "59/1000, train_loss 160.22701136034476\n",
      "60/1000, train_loss 159.80995842173266\n",
      "61/1000, train_loss 159.393206045978\n",
      "62/1000, train_loss 158.9767572367943\n",
      "63/1000, train_loss 158.56061502413908\n",
      "64/1000, train_loss 158.14478246435237\n",
      "65/1000, train_loss 157.72926264029167\n",
      "66/1000, train_loss 157.31405866146673\n",
      "67/1000, train_loss 156.89917366417217\n",
      "68/1000, train_loss 156.4846108116185\n",
      "69/1000, train_loss 156.0703732940604\n",
      "70/1000, train_loss 155.65646432892495\n",
      "71/1000, train_loss 155.2428871609357\n",
      "72/1000, train_loss 154.82964506223672\n",
      "73/1000, train_loss 154.41674133251254\n",
      "74/1000, train_loss 154.0041792991079\n",
      "75/1000, train_loss 153.59196231714353\n",
      "76/1000, train_loss 153.18009376963008\n",
      "77/1000, train_loss 152.76857706758048\n",
      "78/1000, train_loss 152.35741565011816\n",
      "79/1000, train_loss 151.94661298458374\n",
      "80/1000, train_loss 151.5361725666389\n",
      "81/1000, train_loss 151.1260979203668\n",
      "82/1000, train_loss 150.71639259837053\n",
      "83/1000, train_loss 150.3070601818675\n",
      "84/1000, train_loss 149.89810428078215\n",
      "85/1000, train_loss 149.48952853383332\n",
      "86/1000, train_loss 149.08133660862083\n",
      "87/1000, train_loss 148.67353220170696\n",
      "88/1000, train_loss 148.2661190386949\n",
      "89/1000, train_loss 147.8591008743039\n",
      "90/1000, train_loss 147.4524814924403\n",
      "91/1000, train_loss 147.04626470626516\n",
      "92/1000, train_loss 146.64045435825784\n",
      "93/1000, train_loss 146.2350543202752\n",
      "94/1000, train_loss 145.83006849360746\n",
      "95/1000, train_loss 145.42550080902944\n",
      "96/1000, train_loss 145.02135522684708\n",
      "97/1000, train_loss 144.61763573694034\n",
      "98/1000, train_loss 144.2143463588007\n",
      "99/1000, train_loss 143.81149114156474\n",
      "100/1000, train_loss 143.40907416404238\n",
      "101/1000, train_loss 143.00709953474092\n",
      "102/1000, train_loss 142.60557139188262\n",
      "103/1000, train_loss 142.20449390341906\n",
      "104/1000, train_loss 141.803871267039\n",
      "105/1000, train_loss 141.4037077101709\n",
      "106/1000, train_loss 141.00400748998058\n",
      "107/1000, train_loss 140.60477489336267\n",
      "108/1000, train_loss 140.20601423692656\n",
      "109/1000, train_loss 139.80772986697642\n",
      "110/1000, train_loss 139.4099261594853\n",
      "111/1000, train_loss 139.01260752006309\n",
      "112/1000, train_loss 138.61577838391798\n",
      "113/1000, train_loss 138.21944321581213\n",
      "114/1000, train_loss 137.82360651000985\n",
      "115/1000, train_loss 137.42827279022038\n",
      "116/1000, train_loss 137.0334466095328\n",
      "117/1000, train_loss 136.6391325503446\n",
      "118/1000, train_loss 136.2453352242832\n",
      "119/1000, train_loss 135.85205927212002\n",
      "120/1000, train_loss 135.4593093636775\n",
      "121/1000, train_loss 135.06709019772867\n",
      "122/1000, train_loss 134.67540650188883\n",
      "123/1000, train_loss 134.28426303250018\n",
      "124/1000, train_loss 133.89366457450782\n",
      "125/1000, train_loss 133.5036159413282\n",
      "126/1000, train_loss 133.1141219747097\n",
      "127/1000, train_loss 132.7251875445843\n",
      "128/1000, train_loss 132.3368175489116\n",
      "129/1000, train_loss 131.94901691351365\n",
      "130/1000, train_loss 131.56179059190166\n",
      "131/1000, train_loss 131.17514356509375\n",
      "132/1000, train_loss 130.78908084142358\n",
      "133/1000, train_loss 130.40360745634018\n",
      "134/1000, train_loss 130.01872847219866\n",
      "135/1000, train_loss 129.63444897804115\n",
      "136/1000, train_loss 129.25077408936897\n",
      "137/1000, train_loss 128.86770894790467\n",
      "138/1000, train_loss 128.48525872134456\n",
      "139/1000, train_loss 128.1034286031015\n",
      "140/1000, train_loss 127.72222381203748\n",
      "141/1000, train_loss 127.34164959218644\n",
      "142/1000, train_loss 126.96171121246685\n",
      "143/1000, train_loss 126.58241396638326\n",
      "144/1000, train_loss 126.20376317171856\n",
      "145/1000, train_loss 125.82576417021481\n",
      "146/1000, train_loss 125.44842232724362\n",
      "147/1000, train_loss 125.07174303146577\n",
      "148/1000, train_loss 124.69573169447985\n",
      "149/1000, train_loss 124.32039375046004\n",
      "150/1000, train_loss 123.94573465578239\n",
      "151/1000, train_loss 123.57175988864027\n",
      "152/1000, train_loss 123.19847494864814\n",
      "153/1000, train_loss 122.82588535643414\n",
      "154/1000, train_loss 122.45399665322084\n",
      "155/1000, train_loss 122.08281440039471\n",
      "156/1000, train_loss 121.71234417906344\n",
      "157/1000, train_loss 121.34259158960177\n",
      "158/1000, train_loss 120.97356225118544\n",
      "159/1000, train_loss 120.60526180131268\n",
      "160/1000, train_loss 120.23769589531426\n",
      "161/1000, train_loss 119.87087020585106\n",
      "162/1000, train_loss 119.50479042239932\n",
      "163/1000, train_loss 119.1394622507241\n",
      "164/1000, train_loss 118.7748914123397\n",
      "165/1000, train_loss 118.41108364395849\n",
      "166/1000, train_loss 118.04804469692668\n",
      "167/1000, train_loss 117.68578033664787\n",
      "168/1000, train_loss 117.32429634199401\n",
      "169/1000, train_loss 116.96359850470404\n",
      "170/1000, train_loss 116.60369262876927\n",
      "171/1000, train_loss 116.24458452980718\n",
      "172/1000, train_loss 115.8862800344216\n",
      "173/1000, train_loss 115.52878497955079\n",
      "174/1000, train_loss 115.1721052118027\n",
      "175/1000, train_loss 114.81624658677771\n",
      "176/1000, train_loss 114.46121496837839\n",
      "177/1000, train_loss 114.10701622810708\n",
      "178/1000, train_loss 113.75365624435045\n",
      "179/1000, train_loss 113.40114090165166\n",
      "180/1000, train_loss 113.04947608997009\n",
      "181/1000, train_loss 112.698667703928\n",
      "182/1000, train_loss 112.34872164204545\n",
      "183/1000, train_loss 111.99964380596231\n",
      "184/1000, train_loss 111.651440099648\n",
      "185/1000, train_loss 111.30411642859896\n",
      "186/1000, train_loss 110.9576786990239\n",
      "187/1000, train_loss 110.61213281701704\n",
      "188/1000, train_loss 110.26748468771913\n",
      "189/1000, train_loss 109.92374021446648\n",
      "190/1000, train_loss 109.58090529792842\n",
      "191/1000, train_loss 109.23898583523282\n",
      "192/1000, train_loss 108.89798771908026\n",
      "193/1000, train_loss 108.55791683684633\n",
      "194/1000, train_loss 108.2187790696731\n",
      "195/1000, train_loss 107.88058029154918\n",
      "196/1000, train_loss 107.54332636837876\n",
      "197/1000, train_loss 107.20702315703991\n",
      "198/1000, train_loss 106.87167650443236\n",
      "199/1000, train_loss 106.53729224651451\n",
      "200/1000, train_loss 106.20387620733037\n",
      "201/1000, train_loss 105.87143419802655\n",
      "202/1000, train_loss 105.53997201585925\n",
      "203/1000, train_loss 105.20949544319176\n",
      "204/1000, train_loss 104.8800102464824\n",
      "205/1000, train_loss 104.55152217526357\n",
      "206/1000, train_loss 104.22403696111182\n",
      "207/1000, train_loss 103.89756031660933\n",
      "208/1000, train_loss 103.572097934297\n",
      "209/1000, train_loss 103.24765548561956\n",
      "210/1000, train_loss 102.92423861986296\n",
      "211/1000, train_loss 102.6018529630842\n",
      "212/1000, train_loss 102.280504117034\n",
      "213/1000, train_loss 101.96019765807297\n",
      "214/1000, train_loss 101.64093913608077\n",
      "215/1000, train_loss 101.32273407335953\n",
      "216/1000, train_loss 101.0055879635313\n",
      "217/1000, train_loss 100.6895062704302\n",
      "218/1000, train_loss 100.37449442698926\n",
      "219/1000, train_loss 100.06055783412275\n",
      "220/1000, train_loss 99.74770185960415\n",
      "221/1000, train_loss 99.43593183694026\n",
      "222/1000, train_loss 99.1252530642416\n",
      "223/1000, train_loss 98.81567080309004\n",
      "224/1000, train_loss 98.50719027740342\n",
      "225/1000, train_loss 98.19981667229806\n",
      "226/1000, train_loss 97.89355513294952\n",
      "227/1000, train_loss 97.58841076345183\n",
      "228/1000, train_loss 97.28438862567597\n",
      "229/1000, train_loss 96.98149373812772\n",
      "230/1000, train_loss 96.6797310748056\n",
      "231/1000, train_loss 96.37910556405932\n",
      "232/1000, train_loss 96.07962208744895\n",
      "233/1000, train_loss 95.78128547860597\n",
      "234/1000, train_loss 95.48410052209584\n",
      "235/1000, train_loss 95.18807195228308\n",
      "236/1000, train_loss 94.89320445219948\n",
      "237/1000, train_loss 94.5995026524155\n",
      "238/1000, train_loss 94.30697112991571\n",
      "239/1000, train_loss 94.0156144069789\n",
      "240/1000, train_loss 93.72543695006276\n",
      "241/1000, train_loss 93.43644316869434\n",
      "242/1000, train_loss 93.14863741436666\n",
      "243/1000, train_loss 92.86202397944164\n",
      "244/1000, train_loss 92.5766070960602\n",
      "245/1000, train_loss 92.29239093506015\n",
      "246/1000, train_loss 92.0093796049021\n",
      "247/1000, train_loss 91.72757715060418\n",
      "248/1000, train_loss 91.44698755268604\n",
      "249/1000, train_loss 91.16761472612251\n",
      "250/1000, train_loss 90.88946251930783\n",
      "251/1000, train_loss 90.61253471303068\n",
      "252/1000, train_loss 90.33683501946051\n",
      "253/1000, train_loss 90.06236708114608\n",
      "254/1000, train_loss 89.7891344700263\n",
      "255/1000, train_loss 89.51714068645434\n",
      "256/1000, train_loss 89.24638915823508\n",
      "257/1000, train_loss 88.9768832396769\n",
      "258/1000, train_loss 88.70862621065793\n",
      "259/1000, train_loss 88.44162127570763\n",
      "260/1000, train_loss 88.175871563104\n",
      "261/1000, train_loss 87.9113801239869\n",
      "262/1000, train_loss 87.64814993148829\n",
      "263/1000, train_loss 87.38618387987945\n",
      "264/1000, train_loss 87.12548478373617\n",
      "265/1000, train_loss 86.86605537712192\n",
      "266/1000, train_loss 86.60789831278986\n",
      "267/1000, train_loss 86.35101616140405\n",
      "268/1000, train_loss 86.09541141078013\n",
      "269/1000, train_loss 85.84108646514625\n",
      "270/1000, train_loss 85.58804364442457\n",
      "271/1000, train_loss 85.3362851835336\n",
      "272/1000, train_loss 85.0858132317121\n",
      "273/1000, train_loss 84.83662985186481\n",
      "274/1000, train_loss 84.58873701993038\n",
      "275/1000, train_loss 84.34213662427202\n",
      "276/1000, train_loss 84.09683046509117\n",
      "277/1000, train_loss 83.85282025386462\n",
      "278/1000, train_loss 83.61010761280527\n",
      "279/1000, train_loss 83.36869407434729\n",
      "280/1000, train_loss 83.12858108065552\n",
      "281/1000, train_loss 82.88976998315981\n",
      "282/1000, train_loss 82.65226204211429\n",
      "283/1000, train_loss 82.41605842618229\n",
      "284/1000, train_loss 82.18116021204676\n",
      "285/1000, train_loss 81.9475683840466\n",
      "286/1000, train_loss 81.71528383383946\n",
      "287/1000, train_loss 81.48430736009065\n",
      "288/1000, train_loss 81.25463966818887\n",
      "289/1000, train_loss 81.02628136998877\n",
      "290/1000, train_loss 80.7992329835806\n",
      "291/1000, train_loss 80.57349493308693\n",
      "292/1000, train_loss 80.349067548487\n",
      "293/1000, train_loss 80.1259510654682\n",
      "294/1000, train_loss 79.90414562530557\n",
      "295/1000, train_loss 79.68365127476876\n",
      "296/1000, train_loss 79.46446796605686\n",
      "297/1000, train_loss 79.2465955567612\n",
      "298/1000, train_loss 79.03003380985602\n",
      "299/1000, train_loss 78.81478239371702\n",
      "300/1000, train_loss 78.60084088216809\n",
      "301/1000, train_loss 78.38820875455585\n",
      "302/1000, train_loss 78.17688539585212\n",
      "303/1000, train_loss 77.96687009678455\n",
      "304/1000, train_loss 77.75816205399477\n",
      "305/1000, train_loss 77.55076037022461\n",
      "306/1000, train_loss 77.34466405452986\n",
      "307/1000, train_loss 77.13987202252179\n",
      "308/1000, train_loss 76.93638309663578\n",
      "309/1000, train_loss 76.73419600642772\n",
      "310/1000, train_loss 76.53330938889722\n",
      "311/1000, train_loss 76.333721788838\n",
      "312/1000, train_loss 76.13543165921494\n",
      "313/1000, train_loss 75.93843736156774\n",
      "314/1000, train_loss 75.74273716644099\n",
      "315/1000, train_loss 75.54832925384027\n",
      "316/1000, train_loss 75.35521171371406\n",
      "317/1000, train_loss 75.16338254646142\n",
      "318/1000, train_loss 74.97283966346454\n",
      "319/1000, train_loss 74.78358088764675\n",
      "320/1000, train_loss 74.59560395405474\n",
      "321/1000, train_loss 74.40890651046539\n",
      "322/1000, train_loss 74.22348611801642\n",
      "323/1000, train_loss 74.03934025186061\n",
      "324/1000, train_loss 73.85646630184341\n",
      "325/1000, train_loss 73.67486157320303\n",
      "326/1000, train_loss 73.49452328729345\n",
      "327/1000, train_loss 73.31544858232887\n",
      "328/1000, train_loss 73.13763451415034\n",
      "329/1000, train_loss 72.96107805701291\n",
      "330/1000, train_loss 72.78577610439395\n",
      "331/1000, train_loss 72.61172546982151\n",
      "332/1000, train_loss 72.4389228877224\n",
      "333/1000, train_loss 72.26736501428961\n",
      "334/1000, train_loss 72.09704842836852\n",
      "335/1000, train_loss 71.9279696323613\n",
      "336/1000, train_loss 71.76012505314928\n",
      "337/1000, train_loss 71.59351104303242\n",
      "338/1000, train_loss 71.42812388068545\n",
      "339/1000, train_loss 71.26395977213056\n",
      "340/1000, train_loss 71.10101485172541\n",
      "341/1000, train_loss 70.93928518316645\n",
      "342/1000, train_loss 70.7787667605068\n",
      "343/1000, train_loss 70.61945550918819\n",
      "344/1000, train_loss 70.46134728708631\n",
      "345/1000, train_loss 70.30443788556923\n",
      "346/1000, train_loss 70.14872303056798\n",
      "347/1000, train_loss 69.99419838365911\n",
      "348/1000, train_loss 69.8408595431584\n",
      "349/1000, train_loss 69.68870204522516\n",
      "350/1000, train_loss 69.53772136497679\n",
      "351/1000, train_loss 69.38791291761271\n",
      "352/1000, train_loss 69.2392720595473\n",
      "353/1000, train_loss 69.09179408955113\n",
      "354/1000, train_loss 68.94547424990013\n",
      "355/1000, train_loss 68.8003077275319\n",
      "356/1000, train_loss 68.65628965520868\n",
      "357/1000, train_loss 68.5134151126864\n",
      "358/1000, train_loss 68.37167912788937\n",
      "359/1000, train_loss 68.23107667808976\n",
      "360/1000, train_loss 68.0916026910917\n",
      "361/1000, train_loss 67.95325204641905\n",
      "362/1000, train_loss 67.81601957650653\n",
      "363/1000, train_loss 67.67990006789374\n",
      "364/1000, train_loss 67.54488826242108\n",
      "365/1000, train_loss 67.41097885842775\n",
      "366/1000, train_loss 67.27816651195059\n",
      "367/1000, train_loss 67.14644583792376\n",
      "368/1000, train_loss 67.01581141137821\n",
      "369/1000, train_loss 66.88625776864123\n",
      "370/1000, train_loss 66.75777940853457\n",
      "371/1000, train_loss 66.63037079357157\n",
      "372/1000, train_loss 66.50402635115209\n",
      "373/1000, train_loss 66.37874047475525\n",
      "374/1000, train_loss 66.25450752512924\n",
      "375/1000, train_loss 66.13132183147778\n",
      "376/1000, train_loss 66.00917769264284\n",
      "377/1000, train_loss 65.88806937828313\n",
      "378/1000, train_loss 65.76799113004793\n",
      "379/1000, train_loss 65.64893716274575\n",
      "380/1000, train_loss 65.53090166550753\n",
      "381/1000, train_loss 65.4138788029439\n",
      "382/1000, train_loss 65.2978627162961\n",
      "383/1000, train_loss 65.18284752458008\n",
      "384/1000, train_loss 65.0688273257237\n",
      "385/1000, train_loss 64.95579619769623\n",
      "386/1000, train_loss 64.84374819963004\n",
      "387/1000, train_loss 64.73267737293422\n",
      "388/1000, train_loss 64.62257774239951\n",
      "389/1000, train_loss 64.51344331729447\n",
      "390/1000, train_loss 64.40526809245232\n",
      "391/1000, train_loss 64.2980460493484\n",
      "392/1000, train_loss 64.1917711571678\n",
      "393/1000, train_loss 64.08643737386286\n",
      "394/1000, train_loss 63.982038647200305\n",
      "395/1000, train_loss 63.8785689157977\n",
      "396/1000, train_loss 63.77602211014917\n",
      "397/1000, train_loss 63.67439215363979\n",
      "398/1000, train_loss 63.57367296354867\n",
      "399/1000, train_loss 63.47385845204044\n",
      "400/1000, train_loss 63.374942527144924\n",
      "401/1000, train_loss 63.276919093724786\n",
      "402/1000, train_loss 63.179782054430895\n",
      "403/1000, train_loss 63.08352531064536\n",
      "404/1000, train_loss 62.98814276341205\n",
      "405/1000, train_loss 62.89362831435414\n",
      "406/1000, train_loss 62.79997586657911\n",
      "407/1000, train_loss 62.707179325570316\n",
      "408/1000, train_loss 62.61523260006578\n",
      "409/1000, train_loss 62.52412960292339\n",
      "410/1000, train_loss 62.43386425197289\n",
      "411/1000, train_loss 62.34443047085423\n",
      "412/1000, train_loss 62.25582218984237\n",
      "413/1000, train_loss 62.16803334665842\n",
      "414/1000, train_loss 62.081057887266965\n",
      "415/1000, train_loss 61.99488976665955\n",
      "416/1000, train_loss 61.909522949624304\n",
      "417/1000, train_loss 61.824951411501644\n",
      "418/1000, train_loss 61.741169138925926\n",
      "419/1000, train_loss 61.65817013055308\n",
      "420/1000, train_loss 61.57594839777431\n",
      "421/1000, train_loss 61.49449796541551\n",
      "422/1000, train_loss 61.41381287242294\n",
      "423/1000, train_loss 61.33388717253445\n",
      "424/1000, train_loss 61.25471493493697\n",
      "425/1000, train_loss 61.17629024490963\n",
      "426/1000, train_loss 61.09860720445318\n",
      "427/1000, train_loss 61.02165993290509\n",
      "428/1000, train_loss 60.945442567540866\n",
      "429/1000, train_loss 60.86994926416142\n",
      "430/1000, train_loss 60.795174197666384\n",
      "431/1000, train_loss 60.72111156261377\n",
      "432/1000, train_loss 60.647755573765764\n",
      "433/1000, train_loss 60.57510046662086\n",
      "434/1000, train_loss 60.503140497932215\n",
      "435/1000, train_loss 60.43186994621261\n",
      "436/1000, train_loss 60.361283112225735\n",
      "437/1000, train_loss 60.29137431946426\n",
      "438/1000, train_loss 60.22213791461442\n",
      "439/1000, train_loss 60.15356826800745\n",
      "440/1000, train_loss 60.08565977405789\n",
      "441/1000, train_loss 60.018406851688894\n",
      "442/1000, train_loss 59.95180394474456\n",
      "443/1000, train_loss 59.885845522389516\n",
      "444/1000, train_loss 59.82052607949583\n",
      "445/1000, train_loss 59.75584013701725\n",
      "446/1000, train_loss 59.691782242351124\n",
      "447/1000, train_loss 59.62834696968791\n",
      "448/1000, train_loss 59.56552892034857\n",
      "449/1000, train_loss 59.50332272310981\n",
      "450/1000, train_loss 59.44172303451747\n",
      "451/1000, train_loss 59.38072453918807\n",
      "452/1000, train_loss 59.32032195009877\n",
      "453/1000, train_loss 59.26051000886575\n",
      "454/1000, train_loss 59.201283486011235\n",
      "455/1000, train_loss 59.14263718121935\n",
      "456/1000, train_loss 59.08456592358087\n",
      "457/1000, train_loss 59.027064571826976\n",
      "458/1000, train_loss 58.97012801455242\n",
      "459/1000, train_loss 58.91375117042781\n",
      "460/1000, train_loss 58.857928988401675\n",
      "461/1000, train_loss 58.80265644789204\n",
      "462/1000, train_loss 58.74792855896796\n",
      "463/1000, train_loss 58.69374036252093\n",
      "464/1000, train_loss 58.64008693042654\n",
      "465/1000, train_loss 58.58696336569649\n",
      "466/1000, train_loss 58.53436480262086\n",
      "467/1000, train_loss 58.48228640690128\n",
      "468/1000, train_loss 58.43072337577465\n",
      "469/1000, train_loss 58.37967093812799\n",
      "470/1000, train_loss 58.32912435460431\n",
      "471/1000, train_loss 58.279078917699714\n",
      "472/1000, train_loss 58.22952995185193\n",
      "473/1000, train_loss 58.18047281352063\n",
      "474/1000, train_loss 58.131902891259095\n",
      "475/1000, train_loss 58.083815605778206\n",
      "476/1000, train_loss 58.03620641000217\n",
      "477/1000, train_loss 57.989070789116624\n",
      "478/1000, train_loss 57.94240426060912\n",
      "479/1000, train_loss 57.89620237430205\n",
      "480/1000, train_loss 57.85046071237828\n",
      "481/1000, train_loss 57.80517488939967\n",
      "482/1000, train_loss 57.76034055231867\n",
      "483/1000, train_loss 57.715953380482745\n",
      "484/1000, train_loss 57.67200908563248\n",
      "485/1000, train_loss 57.62850341189289\n",
      "486/1000, train_loss 57.58543213575845\n",
      "487/1000, train_loss 57.54279106607177\n",
      "488/1000, train_loss 57.50057604399626\n",
      "489/1000, train_loss 57.4587829429828\n",
      "490/1000, train_loss 57.41740766873058\n",
      "491/1000, train_loss 57.37644615914229\n",
      "492/1000, train_loss 57.33589438427377\n",
      "493/1000, train_loss 57.29574834627832\n",
      "494/1000, train_loss 57.25600407934567\n",
      "495/1000, train_loss 57.216657649635906\n",
      "496/1000, train_loss 57.17770515520844\n",
      "497/1000, train_loss 57.139142725946115\n",
      "498/1000, train_loss 57.1009665234745\n",
      "499/1000, train_loss 57.063172741076826\n",
      "500/1000, train_loss 57.02575760360432\n",
      "501/1000, train_loss 56.98871736738225\n",
      "502/1000, train_loss 56.952048320111835\n",
      "503/1000, train_loss 56.91574678076799\n",
      "504/1000, train_loss 56.87980909949321\n",
      "505/1000, train_loss 56.84423165748768\n",
      "506/1000, train_loss 56.80901086689554\n",
      "507/1000, train_loss 56.77414317068764\n",
      "508/1000, train_loss 56.73962504254094\n",
      "509/1000, train_loss 56.705452986714334\n",
      "510/1000, train_loss 56.67162353792143\n",
      "511/1000, train_loss 56.63813326120011\n",
      "512/1000, train_loss 56.6049787517791\n",
      "513/1000, train_loss 56.572156634941614\n",
      "514/1000, train_loss 56.53966356588621\n",
      "515/1000, train_loss 56.507496229584945\n",
      "516/1000, train_loss 56.4756513406389\n",
      "517/1000, train_loss 56.44412564313126\n",
      "518/1000, train_loss 56.41291591047795\n",
      "519/1000, train_loss 56.382018945276\n",
      "520/1000, train_loss 56.351431579149654\n",
      "521/1000, train_loss 56.321150672594456\n",
      "522/1000, train_loss 56.291173114819216\n",
      "523/1000, train_loss 56.261495823586124\n",
      "524/1000, train_loss 56.232115745048894\n",
      "525/1000, train_loss 56.20302985358938\n",
      "526/1000, train_loss 56.174235151652276\n",
      "527/1000, train_loss 56.14572866957833\n",
      "528/1000, train_loss 56.117507465436134\n",
      "529/1000, train_loss 56.0895686248523\n",
      "530/1000, train_loss 56.06190926084046\n",
      "531/1000, train_loss 56.0345265136289\n",
      "532/1000, train_loss 56.007417550487006\n",
      "533/1000, train_loss 55.98057956555064\n",
      "534/1000, train_loss 55.95400977964632\n",
      "535/1000, train_loss 55.92770544011468\n",
      "536/1000, train_loss 55.901663820632635\n",
      "537/1000, train_loss 55.875882221035134\n",
      "538/1000, train_loss 55.85035796713572\n",
      "539/1000, train_loss 55.82508841054672\n",
      "540/1000, train_loss 55.80007092849854\n",
      "541/1000, train_loss 55.775302923658366\n",
      "542/1000, train_loss 55.75078182394862\n",
      "543/1000, train_loss 55.726505082364476\n",
      "544/1000, train_loss 55.70247017679144\n",
      "545/1000, train_loss 55.67867460982214\n",
      "546/1000, train_loss 55.65511590857312\n",
      "547/1000, train_loss 55.63179162450117\n",
      "548/1000, train_loss 55.60869933321956\n",
      "549/1000, train_loss 55.585836634314084\n",
      "550/1000, train_loss 55.56320115115898\n",
      "551/1000, train_loss 55.54079053073276\n",
      "552/1000, train_loss 55.51860244343414\n",
      "553/1000, train_loss 55.496634582897855\n",
      "554/1000, train_loss 55.474884665810585\n",
      "555/1000, train_loss 55.45335043172711\n",
      "556/1000, train_loss 55.432029642886384\n",
      "557/1000, train_loss 55.410920084028085\n",
      "558/1000, train_loss 55.39001956220917\n",
      "559/1000, train_loss 55.36932590662077\n",
      "560/1000, train_loss 55.348836968405465\n",
      "561/1000, train_loss 55.32855062047479\n",
      "562/1000, train_loss 55.30846475732721\n",
      "563/1000, train_loss 55.288577294866386\n",
      "564/1000, train_loss 55.26888617022003\n",
      "565/1000, train_loss 55.24938934155915\n",
      "566/1000, train_loss 55.23008478791778\n",
      "567/1000, train_loss 55.210970509013194\n",
      "568/1000, train_loss 55.19204452506694\n",
      "569/1000, train_loss 55.17330487662605\n",
      "570/1000, train_loss 55.154749624385225\n",
      "571/1000, train_loss 55.136376849009515\n",
      "572/1000, train_loss 55.11818465095757\n",
      "573/1000, train_loss 55.10017115030574\n",
      "574/1000, train_loss 55.082334486572826\n",
      "575/1000, train_loss 55.06467281854541\n",
      "576/1000, train_loss 55.04718432410429\n",
      "577/1000, train_loss 55.029867200051335\n",
      "578/1000, train_loss 55.012719661937226\n",
      "579/1000, train_loss 54.99573994389014\n",
      "580/1000, train_loss 54.97892629844518\n",
      "581/1000, train_loss 54.96227699637451\n",
      "582/1000, train_loss 54.94579032651865\n",
      "583/1000, train_loss 54.92946459561832\n",
      "584/1000, train_loss 54.913298128147424\n",
      "585/1000, train_loss 54.89728926614681\n",
      "586/1000, train_loss 54.881436369059024\n",
      "587/1000, train_loss 54.865737813563946\n",
      "588/1000, train_loss 54.85019199341533\n",
      "589/1000, train_loss 54.83479731927845\n",
      "590/1000, train_loss 54.8195522185686\n",
      "591/1000, train_loss 54.804455135290496\n",
      "592/1000, train_loss 54.789504529878926\n",
      "593/1000, train_loss 54.774698879040116\n",
      "594/1000, train_loss 54.76003667559427\n",
      "595/1000, train_loss 54.745516428319085\n",
      "596/1000, train_loss 54.73113666179427\n",
      "597/1000, train_loss 54.716895916247125\n",
      "598/1000, train_loss 54.70279274739917\n",
      "599/1000, train_loss 54.688825726313716\n",
      "600/1000, train_loss 54.67499343924466\n",
      "601/1000, train_loss 54.66129448748615\n",
      "602/1000, train_loss 54.64772748722347\n",
      "603/1000, train_loss 54.63429106938491\n",
      "604/1000, train_loss 54.62098387949465\n",
      "605/1000, train_loss 54.607804577526885\n",
      "606/1000, train_loss 54.594751837760846\n",
      "607/1000, train_loss 54.58182434863704\n",
      "608/1000, train_loss 54.56902081261442\n",
      "609/1000, train_loss 54.55633994602885\n",
      "610/1000, train_loss 54.543780478952456\n",
      "611/1000, train_loss 54.53134115505419\n",
      "612/1000, train_loss 54.519020731461396\n",
      "613/1000, train_loss 54.506817978622564\n",
      "614/1000, train_loss 54.49473168017108\n",
      "615/1000, train_loss 54.48276063279012\n",
      "616/1000, train_loss 54.47090364607865\n",
      "617/1000, train_loss 54.45915954241843\n",
      "618/1000, train_loss 54.44752715684225\n",
      "619/1000, train_loss 54.43600533690308\n",
      "620/1000, train_loss 54.42459294254445\n",
      "621/1000, train_loss 54.413288845971934\n",
      "622/1000, train_loss 54.40209193152549\n",
      "623/1000, train_loss 54.3910010955532\n",
      "624/1000, train_loss 54.380015246285865\n",
      "625/1000, train_loss 54.36913330371277\n",
      "626/1000, train_loss 54.35835419945846\n",
      "627/1000, train_loss 54.34767687666073\n",
      "628/1000, train_loss 54.33710028984947\n",
      "629/1000, train_loss 54.32662340482683\n",
      "630/1000, train_loss 54.3162451985482\n",
      "631/1000, train_loss 54.305964659004474\n",
      "632/1000, train_loss 54.29578078510513\n",
      "633/1000, train_loss 54.28569258656266\n",
      "634/1000, train_loss 54.275699083777766\n",
      "635/1000, train_loss 54.26579930772577\n",
      "636/1000, train_loss 54.255992299844024\n",
      "637/1000, train_loss 54.246277111920314\n",
      "638/1000, train_loss 54.23665280598241\n",
      "639/1000, train_loss 54.22711845418845\n",
      "640/1000, train_loss 54.21767313871856\n",
      "641/1000, train_loss 54.20831595166735\n",
      "642/1000, train_loss 54.199045994937464\n",
      "643/1000, train_loss 54.18986238013417\n",
      "644/1000, train_loss 54.18076422846089\n",
      "645/1000, train_loss 54.17175067061576\n",
      "646/1000, train_loss 54.16282084668917\n",
      "647/1000, train_loss 54.153973906062355\n",
      "648/1000, train_loss 54.14520900730684\n",
      "649/1000, train_loss 54.136525318084956\n",
      "650/1000, train_loss 54.127922015051304\n",
      "651/1000, train_loss 54.119398283755174\n",
      "652/1000, train_loss 54.110953318543885\n",
      "653/1000, train_loss 54.1025863224672\n",
      "654/1000, train_loss 54.09429650718249\n",
      "655/1000, train_loss 54.086083092861024\n",
      "656/1000, train_loss 54.07794530809506\n",
      "657/1000, train_loss 54.069882389805954\n",
      "658/1000, train_loss 54.06189358315314\n",
      "659/1000, train_loss 54.053978141444\n",
      "660/1000, train_loss 54.04613532604474\n",
      "661/1000, train_loss 54.038364406292\n",
      "662/1000, train_loss 54.03066465940558\n",
      "663/1000, train_loss 54.02303537040182\n",
      "664/1000, train_loss 54.015475832008065\n",
      "665/1000, train_loss 54.00798534457786\n",
      "666/1000, train_loss 54.0005632160071\n",
      "667/1000, train_loss 53.99320876165096\n",
      "668/1000, train_loss 53.98592130424181\n",
      "669/1000, train_loss 53.978700173807795\n",
      "670/1000, train_loss 53.97154470759248\n",
      "671/1000, train_loss 53.964454249975105\n",
      "672/1000, train_loss 53.95742815239187\n",
      "673/1000, train_loss 53.950465773257825\n",
      "674/1000, train_loss 53.94356647788987\n",
      "675/1000, train_loss 53.93672963843018\n",
      "676/1000, train_loss 53.92995463377087\n",
      "677/1000, train_loss 53.92324084947906\n",
      "678/1000, train_loss 53.91658767772295\n",
      "679/1000, train_loss 53.909994517198676\n",
      "680/1000, train_loss 53.903460773057816\n",
      "681/1000, train_loss 53.8969858568358\n",
      "682/1000, train_loss 53.89056918638097\n",
      "683/1000, train_loss 53.884210185784546\n",
      "684/1000, train_loss 53.877908285311136\n",
      "685/1000, train_loss 53.871662921330106\n",
      "686/1000, train_loss 53.86547353624779\n",
      "687/1000, train_loss 53.859339578440235\n",
      "688/1000, train_loss 53.853260502186686\n",
      "689/1000, train_loss 53.84723576760401\n",
      "690/1000, train_loss 53.84126484058157\n",
      "691/1000, train_loss 53.83534719271694\n",
      "692/1000, train_loss 53.82948230125223\n",
      "693/1000, train_loss 53.823669649011244\n",
      "694/1000, train_loss 53.817908724337215\n",
      "695/1000, train_loss 53.81219902103115\n",
      "696/1000, train_loss 53.80654003829104\n",
      "697/1000, train_loss 53.800931280651575\n",
      "698/1000, train_loss 53.79537225792459\n",
      "699/1000, train_loss 53.78986248514016\n",
      "700/1000, train_loss 53.78440148248829\n",
      "701/1000, train_loss 53.77898877526134\n",
      "702/1000, train_loss 53.77362389379694\n",
      "703/1000, train_loss 53.768306373421716\n",
      "704/1000, train_loss 53.76303575439552\n",
      "705/1000, train_loss 53.75781158185624\n",
      "706/1000, train_loss 53.75263340576538\n",
      "707/1000, train_loss 53.747500780854054\n",
      "708/1000, train_loss 53.7424132665697\n",
      "709/1000, train_loss 53.73737042702341\n",
      "710/1000, train_loss 53.732371830937666\n",
      "711/1000, train_loss 53.72741705159491\n",
      "712/1000, train_loss 53.7225056667865\n",
      "713/1000, train_loss 53.71763725876226\n",
      "714/1000, train_loss 53.71281141418072\n",
      "715/1000, train_loss 53.70802772405974\n",
      "716/1000, train_loss 53.70328578372779\n",
      "717/1000, train_loss 53.69858519277578\n",
      "718/1000, train_loss 53.693925555009386\n",
      "719/1000, train_loss 53.689306478401896\n",
      "720/1000, train_loss 53.68472757504765\n",
      "721/1000, train_loss 53.68018846111602\n",
      "722/1000, train_loss 53.67568875680575\n",
      "723/1000, train_loss 53.67122808630001\n",
      "724/1000, train_loss 53.666806077721844\n",
      "725/1000, train_loss 53.66242236309013\n",
      "726/1000, train_loss 53.65807657827606\n",
      "727/1000, train_loss 53.65376836296011\n",
      "728/1000, train_loss 53.64949736058948\n",
      "729/1000, train_loss 53.645263218336055\n",
      "730/1000, train_loss 53.641065587054754\n",
      "731/1000, train_loss 53.6369041212425\n",
      "732/1000, train_loss 53.63277847899748\n",
      "733/1000, train_loss 53.62868832197901\n",
      "734/1000, train_loss 53.62463331536776\n",
      "735/1000, train_loss 53.62061312782654\n",
      "736/1000, train_loss 53.616627431461374\n",
      "737/1000, train_loss 53.61267590178317\n",
      "738/1000, train_loss 53.60875821766976\n",
      "739/1000, train_loss 53.60487406132836\n",
      "740/1000, train_loss 53.60102311825846\n",
      "741/1000, train_loss 53.597205077215214\n",
      "742/1000, train_loss 53.59341963017309\n",
      "743/1000, train_loss 53.58966647229012\n",
      "744/1000, train_loss 53.58594530187247\n",
      "745/1000, train_loss 53.582255820339306\n",
      "746/1000, train_loss 53.57859773218831\n",
      "747/1000, train_loss 53.574970744961384\n",
      "748/1000, train_loss 53.57137456921079\n",
      "749/1000, train_loss 53.56780891846578\n",
      "750/1000, train_loss 53.564273509199445\n",
      "751/1000, train_loss 53.560768060796114\n",
      "752/1000, train_loss 53.55729229551898\n",
      "753/1000, train_loss 53.553845938478176\n",
      "754/1000, train_loss 53.55042871759924\n",
      "755/1000, train_loss 53.54704036359185\n",
      "756/1000, train_loss 53.54368060991901\n",
      "757/1000, train_loss 53.540349192766534\n",
      "758/1000, train_loss 53.53704585101292\n",
      "759/1000, train_loss 53.533770326199516\n",
      "760/1000, train_loss 53.53052236250112\n",
      "761/1000, train_loss 53.527301706696775\n",
      "762/1000, train_loss 53.52410810814108\n",
      "763/1000, train_loss 53.520941318735666\n",
      "764/1000, train_loss 53.517801092901124\n",
      "765/1000, train_loss 53.51468718754916\n",
      "766/1000, train_loss 53.51159936205515\n",
      "767/1000, train_loss 53.50853737823094\n",
      "768/1000, train_loss 53.50550100029804\n",
      "769/1000, train_loss 53.50248999486105\n",
      "770/1000, train_loss 53.49950413088143\n",
      "771/1000, train_loss 53.49654317965154\n",
      "772/1000, train_loss 53.49360691476912\n",
      "773/1000, train_loss 53.49069511211175\n",
      "774/1000, train_loss 53.48780754981203\n",
      "775/1000, train_loss 53.48494400823264\n",
      "776/1000, train_loss 53.482104269941956\n",
      "777/1000, train_loss 53.47928811968984\n",
      "778/1000, train_loss 53.47649534438375\n",
      "779/1000, train_loss 53.47372573306507\n",
      "780/1000, train_loss 53.47097907688578\n",
      "781/1000, train_loss 53.4682551690853\n",
      "782/1000, train_loss 53.465553804967776\n",
      "783/1000, train_loss 53.46287478187938\n",
      "784/1000, train_loss 53.460217899186134\n",
      "785/1000, train_loss 53.45758295825177\n",
      "786/1000, train_loss 53.454969762415985\n",
      "787/1000, train_loss 53.452378116972895\n",
      "788/1000, train_loss 53.44980782914972\n",
      "789/1000, train_loss 53.44725870808577\n",
      "790/1000, train_loss 53.444730564811636\n",
      "791/1000, train_loss 53.44222321222857\n",
      "792/1000, train_loss 53.43973646508824\n",
      "793/1000, train_loss 53.437270139972625\n",
      "794/1000, train_loss 53.434824055274056\n",
      "795/1000, train_loss 53.43239803117572\n",
      "796/1000, train_loss 53.42999188963218\n",
      "797/1000, train_loss 53.427605454350186\n",
      "798/1000, train_loss 53.42523855076974\n",
      "799/1000, train_loss 53.42289100604535\n",
      "800/1000, train_loss 53.4205626490275\n",
      "801/1000, train_loss 53.41825331024435\n",
      "802/1000, train_loss 53.41596282188358\n",
      "803/1000, train_loss 53.41369101777457\n",
      "804/1000, train_loss 53.41143773337065\n",
      "805/1000, train_loss 53.40920280573165\n",
      "806/1000, train_loss 53.406986073506616\n",
      "807/1000, train_loss 53.4047873769167\n",
      "808/1000, train_loss 53.4026065577383\n",
      "809/1000, train_loss 53.40044345928635\n",
      "810/1000, train_loss 53.398297926397824\n",
      "811/1000, train_loss 53.396169805415404\n",
      "812/1000, train_loss 53.39405894417137\n",
      "813/1000, train_loss 53.391965191971686\n",
      "814/1000, train_loss 53.38988839958018\n",
      "815/1000, train_loss 53.38782841920304\n",
      "816/1000, train_loss 53.38578510447334\n",
      "817/1000, train_loss 53.38375831043589\n",
      "818/1000, train_loss 53.381747893532136\n",
      "819/1000, train_loss 53.37975371158534\n",
      "820/1000, train_loss 53.37777562378577\n",
      "821/1000, train_loss 53.375813490676336\n",
      "822/1000, train_loss 53.373867174138034\n",
      "823/1000, train_loss 53.37193653737589\n",
      "824/1000, train_loss 53.370021444904815\n",
      "825/1000, train_loss 53.36812176253579\n",
      "826/1000, train_loss 53.36623735736214\n",
      "827/1000, train_loss 53.36436809774588\n",
      "828/1000, train_loss 53.362513853304456\n",
      "829/1000, train_loss 53.360674494897324\n",
      "830/1000, train_loss 53.358849894612966\n",
      "831/1000, train_loss 53.357039925755885\n",
      "832/1000, train_loss 53.35524446283379\n",
      "833/1000, train_loss 53.35346338154494\n",
      "834/1000, train_loss 53.351696558765646\n",
      "835/1000, train_loss 53.34994387253786\n",
      "836/1000, train_loss 53.34820520205695\n",
      "837/1000, train_loss 53.34648042765962\n",
      "838/1000, train_loss 53.34476943081195\n",
      "839/1000, train_loss 53.34307209409754\n",
      "840/1000, train_loss 53.34138830120585\n",
      "841/1000, train_loss 53.33971793692067\n",
      "842/1000, train_loss 53.33806088710861\n",
      "843/1000, train_loss 53.33641703870792\n",
      "844/1000, train_loss 53.33478627971723\n",
      "845/1000, train_loss 53.33316849918453\n",
      "846/1000, train_loss 53.33156358719631\n",
      "847/1000, train_loss 53.32997143486668\n",
      "848/1000, train_loss 53.328391934326774\n",
      "849/1000, train_loss 53.32682497871417\n",
      "850/1000, train_loss 53.325270462162486\n",
      "851/1000, train_loss 53.323728279791\n",
      "852/1000, train_loss 53.322198327694515\n",
      "853/1000, train_loss 53.32068050293328\n",
      "854/1000, train_loss 53.31917470352301\n",
      "855/1000, train_loss 53.317680828425026\n",
      "856/1000, train_loss 53.316198777536485\n",
      "857/1000, train_loss 53.31472845168083\n",
      "858/1000, train_loss 53.313269752598146\n",
      "859/1000, train_loss 53.31182258293588\n",
      "860/1000, train_loss 53.31038684623939\n",
      "861/1000, train_loss 53.30896244694283\n",
      "862/1000, train_loss 53.307549290360036\n",
      "863/1000, train_loss 53.30614728267547\n",
      "864/1000, train_loss 53.304756330935376\n",
      "865/1000, train_loss 53.30337634303896\n",
      "866/1000, train_loss 53.30200722772967\n",
      "867/1000, train_loss 53.30064889458659\n",
      "868/1000, train_loss 53.29930125401597\n",
      "869/1000, train_loss 53.29796421724278\n",
      "870/1000, train_loss 53.29663769630239\n",
      "871/1000, train_loss 53.29532160403232\n",
      "872/1000, train_loss 53.29401585406418\n",
      "873/1000, train_loss 53.29272036081552\n",
      "874/1000, train_loss 53.29143503948199\n",
      "875/1000, train_loss 53.290159806029344\n",
      "876/1000, train_loss 53.288894577185765\n",
      "877/1000, train_loss 53.28763927043414\n",
      "878/1000, train_loss 53.28639380400439\n",
      "879/1000, train_loss 53.285158096866084\n",
      "880/1000, train_loss 53.283932068720844\n",
      "881/1000, train_loss 53.28271563999509\n",
      "882/1000, train_loss 53.28150873183274\n",
      "883/1000, train_loss 53.28031126608796\n",
      "884/1000, train_loss 53.279123165318154\n",
      "885/1000, train_loss 53.27794435277683\n",
      "886/1000, train_loss 53.27677475240667\n",
      "887/1000, train_loss 53.27561428883267\n",
      "888/1000, train_loss 53.2744628873553\n",
      "889/1000, train_loss 53.27332047394379\n",
      "890/1000, train_loss 53.27218697522946\n",
      "891/1000, train_loss 53.27106231849917\n",
      "892/1000, train_loss 53.26994643168871\n",
      "893/1000, train_loss 53.26883924337649\n",
      "894/1000, train_loss 53.267740682777074\n",
      "895/1000, train_loss 53.26665067973492\n",
      "896/1000, train_loss 53.26556916471814\n",
      "897/1000, train_loss 53.264496068812306\n",
      "898/1000, train_loss 53.26343132371442\n",
      "899/1000, train_loss 53.2623748617268\n",
      "900/1000, train_loss 53.261326615751194\n",
      "901/1000, train_loss 53.260286519282836\n",
      "902/1000, train_loss 53.25925450640459\n",
      "903/1000, train_loss 53.258230511781264\n",
      "904/1000, train_loss 53.257214470653814\n",
      "905/1000, train_loss 53.25620631883371\n",
      "906/1000, train_loss 53.25520599269743\n",
      "907/1000, train_loss 53.25421342918081\n",
      "908/1000, train_loss 53.253228565773675\n",
      "909/1000, train_loss 53.25225134051442\n",
      "910/1000, train_loss 53.251281691984595\n",
      "911/1000, train_loss 53.25031955930372\n",
      "912/1000, train_loss 53.249364882123956\n",
      "913/1000, train_loss 53.24841760062499\n",
      "914/1000, train_loss 53.2474776555089\n",
      "915/1000, train_loss 53.24654498799507\n",
      "916/1000, train_loss 53.245619539815216\n",
      "917/1000, train_loss 53.2447012532084\n",
      "918/1000, train_loss 53.243790070916106\n",
      "919/1000, train_loss 53.24288593617746\n",
      "920/1000, train_loss 53.24198879272436\n",
      "921/1000, train_loss 53.24109858477679\n",
      "922/1000, train_loss 53.240215257038074\n",
      "923/1000, train_loss 53.23933875469024\n",
      "924/1000, train_loss 53.238469023389435\n",
      "925/1000, train_loss 53.23760600926137\n",
      "926/1000, train_loss 53.23674965889685\n",
      "927/1000, train_loss 53.23589991934725\n",
      "928/1000, train_loss 53.2350567381202\n",
      "929/1000, train_loss 53.234220063175165\n",
      "930/1000, train_loss 53.23338984291916\n",
      "931/1000, train_loss 53.23256602620249\n",
      "932/1000, train_loss 53.23174856231451\n",
      "933/1000, train_loss 53.23093740097943\n",
      "934/1000, train_loss 53.23013249235228\n",
      "935/1000, train_loss 53.229333787014696\n",
      "936/1000, train_loss 53.228541235970994\n",
      "937/1000, train_loss 53.22775479064404\n",
      "938/1000, train_loss 53.22697440287145\n",
      "939/1000, train_loss 53.226200024901495\n",
      "940/1000, train_loss 53.22543160938938\n",
      "941/1000, train_loss 53.224669109393325\n",
      "942/1000, train_loss 53.22391247837077\n",
      "943/1000, train_loss 53.22316167017466\n",
      "944/1000, train_loss 53.22241663904969\n",
      "945/1000, train_loss 53.221677339628656\n",
      "946/1000, train_loss 53.22094372692881\n",
      "947/1000, train_loss 53.2202157563482\n",
      "948/1000, train_loss 53.21949338366226\n",
      "949/1000, train_loss 53.21877656502009\n",
      "950/1000, train_loss 53.21806525694111\n",
      "951/1000, train_loss 53.217359416311545\n",
      "952/1000, train_loss 53.21665900038101\n",
      "953/1000, train_loss 53.21596396675916\n",
      "954/1000, train_loss 53.21527427341231\n",
      "955/1000, train_loss 53.214589878660156\n",
      "956/1000, train_loss 53.21391074117243\n",
      "957/1000, train_loss 53.21323681996574\n",
      "958/1000, train_loss 53.21256807440033\n",
      "959/1000, train_loss 53.211904464176875\n",
      "960/1000, train_loss 53.21124594933336\n",
      "961/1000, train_loss 53.210592490241964\n",
      "962/1000, train_loss 53.20994404760601\n",
      "963/1000, train_loss 53.209300582456855\n",
      "964/1000, train_loss 53.208662056150914\n",
      "965/1000, train_loss 53.20802843036668\n",
      "966/1000, train_loss 53.20739966710174\n",
      "967/1000, train_loss 53.20677572866986\n",
      "968/1000, train_loss 53.20615657769814\n",
      "969/1000, train_loss 53.205542177124045\n",
      "970/1000, train_loss 53.20493249019265\n",
      "971/1000, train_loss 53.20432748045381\n",
      "972/1000, train_loss 53.20372711175938\n",
      "973/1000, train_loss 53.20313134826048\n",
      "974/1000, train_loss 53.202540154404716\n",
      "975/1000, train_loss 53.20195349493356\n",
      "976/1000, train_loss 53.20137133487964\n",
      "977/1000, train_loss 53.2007936395641\n",
      "978/1000, train_loss 53.20022037459398\n",
      "979/1000, train_loss 53.19965150585968\n",
      "980/1000, train_loss 53.19908699953231\n",
      "981/1000, train_loss 53.198526822061254\n",
      "982/1000, train_loss 53.197970940171565\n",
      "983/1000, train_loss 53.19741932086154\n",
      "984/1000, train_loss 53.19687193140026\n",
      "985/1000, train_loss 53.19632873932511\n",
      "986/1000, train_loss 53.195789712439414\n",
      "987/1000, train_loss 53.19525481881002\n",
      "988/1000, train_loss 53.19472402676497\n",
      "989/1000, train_loss 53.19419730489112\n",
      "990/1000, train_loss 53.19367462203184\n",
      "991/1000, train_loss 53.19315594728475\n",
      "992/1000, train_loss 53.192641249999376\n",
      "993/1000, train_loss 53.192130499775004\n",
      "994/1000, train_loss 53.19162366645836\n",
      "995/1000, train_loss 53.19112072014148\n",
      "996/1000, train_loss 53.19062163115948\n",
      "997/1000, train_loss 53.190126370088414\n",
      "998/1000, train_loss 53.18963490774314\n",
      "999/1000, train_loss 53.189147215175204\n",
      "1000/1000, train_loss 53.188663263670726\n",
      "1/1000, test_loss 18.016491882191254\n",
      "2/1000, test_loss 18.012797984436336\n",
      "3/1000, test_loss 18.009139230786033\n",
      "4/1000, test_loss 18.005515258216946\n",
      "5/1000, test_loss 18.00192570760932\n",
      "6/1000, test_loss 17.998370223706278\n",
      "7/1000, test_loss 17.994848455073363\n",
      "8/1000, test_loss 17.99136005405853\n",
      "9/1000, test_loss 17.987904676752365\n",
      "10/1000, test_loss 17.984481982948786\n",
      "11/1000, test_loss 17.981091636106022\n",
      "12/1000, test_loss 17.977733303307986\n",
      "13/1000, test_loss 17.974406655225952\n",
      "14/1000, test_loss 17.971111366080667\n",
      "15/1000, test_loss 17.967847113604712\n",
      "16/1000, test_loss 17.964613579005288\n",
      "17/1000, test_loss 17.961410446927307\n",
      "18/1000, test_loss 17.958237405416828\n",
      "19/1000, test_loss 17.955094145884885\n",
      "20/1000, test_loss 17.95198036307157\n",
      "21/1000, test_loss 17.948895755010525\n",
      "22/1000, test_loss 17.945840022993746\n",
      "23/1000, test_loss 17.942812871536717\n",
      "24/1000, test_loss 17.939814008343877\n",
      "25/1000, test_loss 17.936843144274427\n",
      "26/1000, test_loss 17.933899993308483\n",
      "27/1000, test_loss 17.930984272513506\n",
      "28/1000, test_loss 17.928095702011074\n",
      "29/1000, test_loss 17.925234004944034\n",
      "30/1000, test_loss 17.922398907443903\n",
      "31/1000, test_loss 17.919590138598593\n",
      "32/1000, test_loss 17.916807430420523\n",
      "33/1000, test_loss 17.91405051781494\n",
      "34/1000, test_loss 17.91131913854866\n",
      "35/1000, test_loss 17.908613033219027\n",
      "36/1000, test_loss 17.905931945223237\n",
      "37/1000, test_loss 17.903275620727964\n",
      "38/1000, test_loss 17.900643808639245\n",
      "39/1000, test_loss 17.898036260572724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/1000, test_loss 17.895452730824157\n",
      "41/1000, test_loss 17.892892976340253\n",
      "42/1000, test_loss 17.89035675668973\n",
      "43/1000, test_loss 17.88784383403478\n",
      "44/1000, test_loss 17.885353973102706\n",
      "45/1000, test_loss 17.88288694115796\n",
      "46/1000, test_loss 17.880442507974344\n",
      "47/1000, test_loss 17.878020445807625\n",
      "48/1000, test_loss 17.87562052936833\n",
      "49/1000, test_loss 17.873242535794866\n",
      "50/1000, test_loss 17.870886244626906\n",
      "51/1000, test_loss 17.868551437779075\n",
      "52/1000, test_loss 17.866237899514857\n",
      "53/1000, test_loss 17.863945416420815\n",
      "54/1000, test_loss 17.86167377738107\n",
      "55/1000, test_loss 17.85942277355204\n",
      "56/1000, test_loss 17.85719219833744\n",
      "57/1000, test_loss 17.854981847363554\n",
      "58/1000, test_loss 17.852791518454758\n",
      "59/1000, test_loss 17.85062101160932\n",
      "60/1000, test_loss 17.848470128975393\n",
      "61/1000, test_loss 17.84633867482736\n",
      "62/1000, test_loss 17.844226455542344\n",
      "63/1000, test_loss 17.842133279577002\n",
      "64/1000, test_loss 17.84005895744456\n",
      "65/1000, test_loss 17.838003301692094\n",
      "66/1000, test_loss 17.83596612687806\n",
      "67/1000, test_loss 17.833947249550032\n",
      "68/1000, test_loss 17.831946488222727\n",
      "69/1000, test_loss 17.82996366335623\n",
      "70/1000, test_loss 17.827998597334442\n",
      "71/1000, test_loss 17.826051114443793\n",
      "72/1000, test_loss 17.824121040852184\n",
      "73/1000, test_loss 17.822208204588097\n",
      "74/1000, test_loss 17.820312435520027\n",
      "75/1000, test_loss 17.818433565336026\n",
      "76/1000, test_loss 17.81657142752358\n",
      "77/1000, test_loss 17.8147258573496\n",
      "78/1000, test_loss 17.812896691840713\n",
      "79/1000, test_loss 17.811083769763687\n",
      "80/1000, test_loss 17.809286931606188\n",
      "81/1000, test_loss 17.807506019557593\n",
      "82/1000, test_loss 17.805740877490127\n",
      "83/1000, test_loss 17.80399135094018\n",
      "84/1000, test_loss 17.802257287089795\n",
      "85/1000, test_loss 17.80053853474839\n",
      "86/1000, test_loss 17.798834944334683\n",
      "87/1000, test_loss 17.797146367858765\n",
      "88/1000, test_loss 17.79547265890447\n",
      "89/1000, test_loss 17.793813672611808\n",
      "90/1000, test_loss 17.792169265659727\n",
      "91/1000, test_loss 17.790539296248923\n",
      "92/1000, test_loss 17.788923624085\n",
      "93/1000, test_loss 17.787322110361647\n",
      "94/1000, test_loss 17.78573461774414\n",
      "95/1000, test_loss 17.78416101035295\n",
      "96/1000, test_loss 17.78260115374756\n",
      "97/1000, test_loss 17.781054914910435\n",
      "98/1000, test_loss 17.779522162231217\n",
      "99/1000, test_loss 17.77800276549103\n",
      "100/1000, test_loss 17.77649659584705\n",
      "101/1000, test_loss 17.775003525817134\n",
      "102/1000, test_loss 17.773523429264714\n",
      "103/1000, test_loss 17.772056181383824\n",
      "104/1000, test_loss 17.770601658684267\n",
      "105/1000, test_loss 17.769159738977024\n",
      "106/1000, test_loss 17.767730301359716\n",
      "107/1000, test_loss 17.76631322620234\n",
      "108/1000, test_loss 17.764908395133073\n",
      "109/1000, test_loss 17.763515691024317\n",
      "110/1000, test_loss 17.762134997978816\n",
      "111/1000, test_loss 17.760766201316\n",
      "112/1000, test_loss 17.75940918755844\n",
      "113/1000, test_loss 17.758063844418484\n",
      "114/1000, test_loss 17.75673006078499\n",
      "115/1000, test_loss 17.7554077267103\n",
      "116/1000, test_loss 17.75409673339725\n",
      "117/1000, test_loss 17.75279697318643\n",
      "118/1000, test_loss 17.751508339543502\n",
      "119/1000, test_loss 17.75023072704672\n",
      "120/1000, test_loss 17.74896403137456\n",
      "121/1000, test_loss 17.747708149293498\n",
      "122/1000, test_loss 17.74646297864593\n",
      "123/1000, test_loss 17.745228418338236\n",
      "124/1000, test_loss 17.74400436832893\n",
      "125/1000, test_loss 17.74279072961702\n",
      "126/1000, test_loss 17.74158740423045\n",
      "127/1000, test_loss 17.740394295214678\n",
      "128/1000, test_loss 17.739211306621367\n",
      "129/1000, test_loss 17.738038343497287\n",
      "130/1000, test_loss 17.736875311873224\n",
      "131/1000, test_loss 17.735722118753095\n",
      "132/1000, test_loss 17.73457867210316\n",
      "133/1000, test_loss 17.73344488084137\n",
      "134/1000, test_loss 17.732320654826797\n",
      "135/1000, test_loss 17.731205904849254\n",
      "136/1000, test_loss 17.730100542618967\n",
      "137/1000, test_loss 17.729004480756377\n",
      "138/1000, test_loss 17.727917632782106\n",
      "139/1000, test_loss 17.726839913106957\n",
      "140/1000, test_loss 17.72577123702213\n",
      "141/1000, test_loss 17.724711520689414\n",
      "142/1000, test_loss 17.723660681131644\n",
      "143/1000, test_loss 17.72261863622313\n",
      "144/1000, test_loss 17.7215853046803\n",
      "145/1000, test_loss 17.720560606052373\n",
      "146/1000, test_loss 17.719544460712186\n",
      "147/1000, test_loss 17.7185367898471\n",
      "148/1000, test_loss 17.717537515450033\n",
      "149/1000, test_loss 17.716546560310565\n",
      "150/1000, test_loss 17.715563848006163\n",
      "151/1000, test_loss 17.71458930289352\n",
      "152/1000, test_loss 17.713622850099952\n",
      "153/1000, test_loss 17.712664415514944\n",
      "154/1000, test_loss 17.711713925781734\n",
      "155/1000, test_loss 17.710771308289058\n",
      "156/1000, test_loss 17.70983649116292\n",
      "157/1000, test_loss 17.70890940325855\n",
      "158/1000, test_loss 17.707989974152312\n",
      "159/1000, test_loss 17.707078134133862\n",
      "160/1000, test_loss 17.70617381419829\n",
      "161/1000, test_loss 17.705276946038385\n",
      "162/1000, test_loss 17.704387462036998\n",
      "163/1000, test_loss 17.70350529525945\n",
      "164/1000, test_loss 17.70263037944611\n",
      "165/1000, test_loss 17.70176264900495\n",
      "166/1000, test_loss 17.700902039004287\n",
      "167/1000, test_loss 17.70004848516552\n",
      "168/1000, test_loss 17.69920192385603\n",
      "169/1000, test_loss 17.698362292082095\n",
      "170/1000, test_loss 17.697529527481926\n",
      "171/1000, test_loss 17.696703568318764\n",
      "172/1000, test_loss 17.69588435347407\n",
      "173/1000, test_loss 17.695071822440784\n",
      "174/1000, test_loss 17.69426591531664\n",
      "175/1000, test_loss 17.693466572797618\n",
      "176/1000, test_loss 17.692673736171397\n",
      "177/1000, test_loss 17.691887347310942\n",
      "178/1000, test_loss 17.691107348668115\n",
      "179/1000, test_loss 17.69033368326741\n",
      "180/1000, test_loss 17.689566294699727\n",
      "181/1000, test_loss 17.6888051271162\n",
      "182/1000, test_loss 17.68805012522216\n",
      "183/1000, test_loss 17.687301234271086\n",
      "184/1000, test_loss 17.686558400058697\n",
      "185/1000, test_loss 17.685821568917056\n",
      "186/1000, test_loss 17.685090687708783\n",
      "187/1000, test_loss 17.684365703821296\n",
      "188/1000, test_loss 17.683646565161165\n",
      "189/1000, test_loss 17.682933220148485\n",
      "190/1000, test_loss 17.682225617711325\n",
      "191/1000, test_loss 17.68152370728026\n",
      "192/1000, test_loss 17.680827438782952\n",
      "193/1000, test_loss 17.680136762638792\n",
      "194/1000, test_loss 17.6794516297536\n",
      "195/1000, test_loss 17.678771991514378\n",
      "196/1000, test_loss 17.67809779978417\n",
      "197/1000, test_loss 17.677429006896894\n",
      "198/1000, test_loss 17.676765565652342\n",
      "199/1000, test_loss 17.67610742931113\n",
      "200/1000, test_loss 17.675454551589784\n",
      "201/1000, test_loss 17.674806886655833\n",
      "202/1000, test_loss 17.674164389122982\n",
      "203/1000, test_loss 17.673527014046353\n",
      "204/1000, test_loss 17.672894716917746\n",
      "205/1000, test_loss 17.67226745366096\n",
      "206/1000, test_loss 17.6716451806272\n",
      "207/1000, test_loss 17.6710278545905\n",
      "208/1000, test_loss 17.670415432743216\n",
      "209/1000, test_loss 17.669807872691567\n",
      "210/1000, test_loss 17.66920513245123\n",
      "211/1000, test_loss 17.668607170442954\n",
      "212/1000, test_loss 17.668013945488298\n",
      "213/1000, test_loss 17.667425416805326\n",
      "214/1000, test_loss 17.66684154400442\n",
      "215/1000, test_loss 17.66626228708411\n",
      "216/1000, test_loss 17.665687606426932\n",
      "217/1000, test_loss 17.66511746279542\n",
      "218/1000, test_loss 17.664551817327993\n",
      "219/1000, test_loss 17.663990631535068\n",
      "220/1000, test_loss 17.663433867295062\n",
      "221/1000, test_loss 17.66288148685052\n",
      "222/1000, test_loss 17.66233345280428\n",
      "223/1000, test_loss 17.66178972811566\n",
      "224/1000, test_loss 17.661250276096716\n",
      "225/1000, test_loss 17.6607150604085\n",
      "226/1000, test_loss 17.66018404505741\n",
      "227/1000, test_loss 17.65965719439153\n",
      "228/1000, test_loss 17.65913447309708\n",
      "229/1000, test_loss 17.658615846194827\n",
      "230/1000, test_loss 17.658101279036586\n",
      "231/1000, test_loss 17.65759073730176\n",
      "232/1000, test_loss 17.657084186993888\n",
      "233/1000, test_loss 17.656581594437274\n",
      "234/1000, test_loss 17.65608292627361\n",
      "235/1000, test_loss 17.655588149458662\n",
      "236/1000, test_loss 17.65509723125903\n",
      "237/1000, test_loss 17.654610139248835\n",
      "238/1000, test_loss 17.654126841306567\n",
      "239/1000, test_loss 17.653647305611894\n",
      "240/1000, test_loss 17.653171500642532\n",
      "241/1000, test_loss 17.652699395171133\n",
      "242/1000, test_loss 17.652230958262255\n",
      "243/1000, test_loss 17.65176615926928\n",
      "244/1000, test_loss 17.651304967831454\n",
      "245/1000, test_loss 17.65084735387093\n",
      "246/1000, test_loss 17.6503932875898\n",
      "247/1000, test_loss 17.649942739467242\n",
      "248/1000, test_loss 17.64949568025662\n",
      "249/1000, test_loss 17.64905208098269\n",
      "250/1000, test_loss 17.648611912938748\n",
      "251/1000, test_loss 17.648175147683908\n",
      "252/1000, test_loss 17.647741757040343\n",
      "253/1000, test_loss 17.647311713090588\n",
      "254/1000, test_loss 17.646884988174833\n",
      "255/1000, test_loss 17.646461554888322\n",
      "256/1000, test_loss 17.64604138607869\n",
      "257/1000, test_loss 17.645624454843414\n",
      "258/1000, test_loss 17.64521073452721\n",
      "259/1000, test_loss 17.644800198719555\n",
      "260/1000, test_loss 17.644392821252122\n",
      "261/1000, test_loss 17.643988576196367\n",
      "262/1000, test_loss 17.64358743786103\n",
      "263/1000, test_loss 17.643189380789742\n",
      "264/1000, test_loss 17.64279437975862\n",
      "265/1000, test_loss 17.64240240977392\n",
      "266/1000, test_loss 17.642013446069654\n",
      "267/1000, test_loss 17.64162746410533\n",
      "268/1000, test_loss 17.641244439563607\n",
      "269/1000, test_loss 17.64086434834808\n",
      "270/1000, test_loss 17.640487166581014\n",
      "271/1000, test_loss 17.640112870601126\n",
      "272/1000, test_loss 17.639741436961405\n",
      "273/1000, test_loss 17.63937284242696\n",
      "274/1000, test_loss 17.63900706397285\n",
      "275/1000, test_loss 17.638644078781983\n",
      "276/1000, test_loss 17.638283864243032\n",
      "277/1000, test_loss 17.637926397948323\n",
      "278/1000, test_loss 17.63757165769185\n",
      "279/1000, test_loss 17.637219621467175\n",
      "280/1000, test_loss 17.636870267465493\n",
      "281/1000, test_loss 17.636523574073596\n",
      "282/1000, test_loss 17.63617951987195\n",
      "283/1000, test_loss 17.635838083632738\n",
      "284/1000, test_loss 17.635499244317938\n",
      "285/1000, test_loss 17.63516298107746\n",
      "286/1000, test_loss 17.63482927324722\n",
      "287/1000, test_loss 17.634498100347326\n",
      "288/1000, test_loss 17.63416944208023\n",
      "289/1000, test_loss 17.633843278328893\n",
      "290/1000, test_loss 17.63351958915504\n",
      "291/1000, test_loss 17.63319835479733\n",
      "292/1000, test_loss 17.632879555669636\n",
      "293/1000, test_loss 17.63256317235929\n",
      "294/1000, test_loss 17.632249185625383\n",
      "295/1000, test_loss 17.631937576397046\n",
      "296/1000, test_loss 17.631628325771782\n",
      "297/1000, test_loss 17.6313214150138\n",
      "298/1000, test_loss 17.631016825552372\n",
      "299/1000, test_loss 17.630714538980207\n",
      "300/1000, test_loss 17.630414537051834\n",
      "301/1000, test_loss 17.63011680168204\n",
      "302/1000, test_loss 17.62982131494424\n",
      "303/1000, test_loss 17.629528059068992\n",
      "304/1000, test_loss 17.62923701644238\n",
      "305/1000, test_loss 17.628948169604566\n",
      "306/1000, test_loss 17.628661501248228\n",
      "307/1000, test_loss 17.62837699421709\n",
      "308/1000, test_loss 17.628094631504442\n",
      "309/1000, test_loss 17.627814396251686\n",
      "310/1000, test_loss 17.627536271746912\n",
      "311/1000, test_loss 17.62726024142341\n",
      "312/1000, test_loss 17.62698628885834\n",
      "313/1000, test_loss 17.62671439777126\n",
      "314/1000, test_loss 17.626444552022793\n",
      "315/1000, test_loss 17.626176735613242\n",
      "316/1000, test_loss 17.625910932681244\n",
      "317/1000, test_loss 17.625647127502408\n",
      "318/1000, test_loss 17.62538530448803\n",
      "319/1000, test_loss 17.625125448183763\n",
      "320/1000, test_loss 17.62486754326831\n",
      "321/1000, test_loss 17.62461157455216\n",
      "322/1000, test_loss 17.62435752697632\n",
      "323/1000, test_loss 17.62410538561105\n",
      "324/1000, test_loss 17.623855135654626\n",
      "325/1000, test_loss 17.623606762432114\n",
      "326/1000, test_loss 17.623360251394164\n",
      "327/1000, test_loss 17.623115588115777\n",
      "328/1000, test_loss 17.622872758295152\n",
      "329/1000, test_loss 17.62263174775249\n",
      "330/1000, test_loss 17.622392542428834\n",
      "331/1000, test_loss 17.622155128384918\n",
      "332/1000, test_loss 17.621919491800018\n",
      "333/1000, test_loss 17.621685618970858\n",
      "334/1000, test_loss 17.621453496310437\n",
      "335/1000, test_loss 17.62122311034698\n",
      "336/1000, test_loss 17.620994447722808\n",
      "337/1000, test_loss 17.62076749519328\n",
      "338/1000, test_loss 17.620542239625724\n",
      "339/1000, test_loss 17.620318667998355\n",
      "340/1000, test_loss 17.620096767399243\n",
      "341/1000, test_loss 17.619876525025298\n",
      "342/1000, test_loss 17.6196579281812\n",
      "343/1000, test_loss 17.619440964278432\n",
      "344/1000, test_loss 17.61922562083423\n",
      "345/1000, test_loss 17.61901188547063\n",
      "346/1000, test_loss 17.61879974591346\n",
      "347/1000, test_loss 17.61858918999138\n",
      "348/1000, test_loss 17.618380205634914\n",
      "349/1000, test_loss 17.618172780875494\n",
      "350/1000, test_loss 17.617966903844525\n",
      "351/1000, test_loss 17.617762562772445\n",
      "352/1000, test_loss 17.617559745987815\n",
      "353/1000, test_loss 17.617358441916387\n",
      "354/1000, test_loss 17.617158639080223\n",
      "355/1000, test_loss 17.616960326096763\n",
      "356/1000, test_loss 17.61676349167799\n",
      "357/1000, test_loss 17.61656812462951\n",
      "358/1000, test_loss 17.61637421384971\n",
      "359/1000, test_loss 17.616181748328884\n",
      "360/1000, test_loss 17.61599071714841\n",
      "361/1000, test_loss 17.615801109479868\n",
      "362/1000, test_loss 17.615612914584247\n",
      "363/1000, test_loss 17.615426121811097\n",
      "364/1000, test_loss 17.615240720597733\n",
      "365/1000, test_loss 17.61505670046841\n",
      "366/1000, test_loss 17.614874051033535\n",
      "367/1000, test_loss 17.61469276198888\n",
      "368/1000, test_loss 17.614512823114786\n",
      "369/1000, test_loss 17.61433422427541\n",
      "370/1000, test_loss 17.61415695541794\n",
      "371/1000, test_loss 17.613981006571848\n",
      "372/1000, test_loss 17.613806367848124\n",
      "373/1000, test_loss 17.61363302943856\n",
      "374/1000, test_loss 17.613460981615\n",
      "375/1000, test_loss 17.6132902147286\n",
      "376/1000, test_loss 17.613120719209117\n",
      "377/1000, test_loss 17.612952485564218\n",
      "378/1000, test_loss 17.61278550437873\n",
      "379/1000, test_loss 17.61261976631399\n",
      "380/1000, test_loss 17.61245526210711\n",
      "381/1000, test_loss 17.612291982570323\n",
      "382/1000, test_loss 17.612129918590284\n",
      "383/1000, test_loss 17.611969061127418\n",
      "384/1000, test_loss 17.61180940121524\n",
      "385/1000, test_loss 17.6116509299597\n",
      "386/1000, test_loss 17.611493638538555\n",
      "387/1000, test_loss 17.611337518200692\n",
      "388/1000, test_loss 17.6111825602655\n",
      "389/1000, test_loss 17.611028756122256\n",
      "390/1000, test_loss 17.610876097229482\n",
      "391/1000, test_loss 17.61072457511432\n",
      "392/1000, test_loss 17.610574181371955\n",
      "393/1000, test_loss 17.610424907664957\n",
      "394/1000, test_loss 17.61027674572273\n",
      "395/1000, test_loss 17.610129687340876\n",
      "396/1000, test_loss 17.609983724380637\n",
      "397/1000, test_loss 17.609838848768298\n",
      "398/1000, test_loss 17.609695052494622\n",
      "399/1000, test_loss 17.60955232761425\n",
      "400/1000, test_loss 17.609410666245175\n",
      "401/1000, test_loss 17.609270060568146\n",
      "402/1000, test_loss 17.60913050282615\n",
      "403/1000, test_loss 17.608991985323815\n",
      "404/1000, test_loss 17.60885450042693\n",
      "405/1000, test_loss 17.608718040561836\n",
      "406/1000, test_loss 17.60858259821495\n",
      "407/1000, test_loss 17.608448165932227\n",
      "408/1000, test_loss 17.60831473631859\n",
      "409/1000, test_loss 17.608182302037488\n",
      "410/1000, test_loss 17.608050855810326\n",
      "411/1000, test_loss 17.607920390415977\n",
      "412/1000, test_loss 17.60779089869029\n",
      "413/1000, test_loss 17.607662373525578\n",
      "414/1000, test_loss 17.607534807870127\n",
      "415/1000, test_loss 17.607408194727732\n",
      "416/1000, test_loss 17.607282527157157\n",
      "417/1000, test_loss 17.607157798271736\n",
      "418/1000, test_loss 17.607034001238823\n",
      "419/1000, test_loss 17.60691112927939\n",
      "420/1000, test_loss 17.606789175667505\n",
      "421/1000, test_loss 17.60666813372991\n",
      "422/1000, test_loss 17.60654799684556\n",
      "423/1000, test_loss 17.606428758445148\n",
      "424/1000, test_loss 17.606310412010693\n",
      "425/1000, test_loss 17.60619295107508\n",
      "426/1000, test_loss 17.606076369221608\n",
      "427/1000, test_loss 17.605960660083586\n",
      "428/1000, test_loss 17.605845817343884\n",
      "429/1000, test_loss 17.605731834734524\n",
      "430/1000, test_loss 17.60561870603623\n",
      "431/1000, test_loss 17.60550642507803\n",
      "432/1000, test_loss 17.60539498573685\n",
      "433/1000, test_loss 17.605284381937093\n",
      "434/1000, test_loss 17.605174607650234\n",
      "435/1000, test_loss 17.60506565689442\n",
      "436/1000, test_loss 17.60495752373408\n",
      "437/1000, test_loss 17.6048502022795\n",
      "438/1000, test_loss 17.604743686686493\n",
      "439/1000, test_loss 17.604637971155952\n",
      "440/1000, test_loss 17.604533049933494\n",
      "441/1000, test_loss 17.604428917309075\n",
      "442/1000, test_loss 17.60432556761664\n",
      "443/1000, test_loss 17.6042229952337\n",
      "444/1000, test_loss 17.604121194581015\n",
      "445/1000, test_loss 17.604020160122193\n",
      "446/1000, test_loss 17.603919886363343\n",
      "447/1000, test_loss 17.603820367852727\n",
      "448/1000, test_loss 17.603721599180382\n",
      "449/1000, test_loss 17.603623574977778\n",
      "450/1000, test_loss 17.603526289917504\n",
      "451/1000, test_loss 17.60342973871284\n",
      "452/1000, test_loss 17.603333916117506\n",
      "453/1000, test_loss 17.603238816925273\n",
      "454/1000, test_loss 17.603144435969647\n",
      "455/1000, test_loss 17.603050768123524\n",
      "456/1000, test_loss 17.60295780829886\n",
      "457/1000, test_loss 17.602865551446357\n",
      "458/1000, test_loss 17.602773992555143\n",
      "459/1000, test_loss 17.602683126652437\n",
      "460/1000, test_loss 17.602592948803235\n",
      "461/1000, test_loss 17.60250345411001\n",
      "462/1000, test_loss 17.602414637712382\n",
      "463/1000, test_loss 17.602326494786833\n",
      "464/1000, test_loss 17.60223902054637\n",
      "465/1000, test_loss 17.602152210240252\n",
      "466/1000, test_loss 17.602066059153678\n",
      "467/1000, test_loss 17.601980562607487\n",
      "468/1000, test_loss 17.601895715957873\n",
      "469/1000, test_loss 17.601811514596086\n",
      "470/1000, test_loss 17.60172795394813\n",
      "471/1000, test_loss 17.601645029474536\n",
      "472/1000, test_loss 17.601562736669987\n",
      "473/1000, test_loss 17.60148107106311\n",
      "474/1000, test_loss 17.60140002821617\n",
      "475/1000, test_loss 17.60131960372479\n",
      "476/1000, test_loss 17.60123979321769\n",
      "477/1000, test_loss 17.601160592356425\n",
      "478/1000, test_loss 17.601081996835074\n",
      "479/1000, test_loss 17.601004002380023\n",
      "480/1000, test_loss 17.600926604749677\n",
      "481/1000, test_loss 17.600849799734206\n",
      "482/1000, test_loss 17.600773583155277\n",
      "483/1000, test_loss 17.600697950865822\n",
      "484/1000, test_loss 17.600622898749734\n",
      "485/1000, test_loss 17.600548422721687\n",
      "486/1000, test_loss 17.600474518726823\n",
      "487/1000, test_loss 17.600401182740534\n",
      "488/1000, test_loss 17.600328410768217\n",
      "489/1000, test_loss 17.600256198845024\n",
      "490/1000, test_loss 17.600184543035624\n",
      "491/1000, test_loss 17.600113439433965\n",
      "492/1000, test_loss 17.600042884163045\n",
      "493/1000, test_loss 17.59997287337466\n",
      "494/1000, test_loss 17.59990340324919\n",
      "495/1000, test_loss 17.59983446999536\n",
      "496/1000, test_loss 17.59976606985001\n",
      "497/1000, test_loss 17.599698199077885\n",
      "498/1000, test_loss 17.599630853971377\n",
      "499/1000, test_loss 17.59956403085036\n",
      "500/1000, test_loss 17.599497726061895\n",
      "501/1000, test_loss 17.599431935980093\n",
      "502/1000, test_loss 17.599366657005824\n",
      "503/1000, test_loss 17.59930188556656\n",
      "504/1000, test_loss 17.59923761811614\n",
      "505/1000, test_loss 17.599173851134545\n",
      "506/1000, test_loss 17.599110581127718\n",
      "507/1000, test_loss 17.59904780462734\n",
      "508/1000, test_loss 17.598985518190638\n",
      "509/1000, test_loss 17.59892371840016\n",
      "510/1000, test_loss 17.598862401863602\n",
      "511/1000, test_loss 17.59880156521358\n",
      "512/1000, test_loss 17.598741205107455\n",
      "513/1000, test_loss 17.598681318227136\n",
      "514/1000, test_loss 17.598621901278864\n",
      "515/1000, test_loss 17.598562950993045\n",
      "516/1000, test_loss 17.598504464124037\n",
      "517/1000, test_loss 17.598446437449994\n",
      "518/1000, test_loss 17.598388867772634\n",
      "519/1000, test_loss 17.598331751917083\n",
      "520/1000, test_loss 17.598275086731693\n",
      "521/1000, test_loss 17.598218869087844\n",
      "522/1000, test_loss 17.598163095879766\n",
      "523/1000, test_loss 17.598107764024366\n",
      "524/1000, test_loss 17.598052870461053\n",
      "525/1000, test_loss 17.59799841215154\n",
      "526/1000, test_loss 17.597944386079703\n",
      "527/1000, test_loss 17.597890789251384\n",
      "528/1000, test_loss 17.597837618694218\n",
      "529/1000, test_loss 17.59778487145747\n",
      "530/1000, test_loss 17.597732544611876\n",
      "531/1000, test_loss 17.59768063524946\n",
      "532/1000, test_loss 17.59762914048337\n",
      "533/1000, test_loss 17.597578057447716\n",
      "534/1000, test_loss 17.597527383297418\n",
      "535/1000, test_loss 17.597477115208022\n",
      "536/1000, test_loss 17.59742725037556\n",
      "537/1000, test_loss 17.59737778601638\n",
      "538/1000, test_loss 17.597328719366992\n",
      "539/1000, test_loss 17.597280047683903\n",
      "540/1000, test_loss 17.59723176824348\n",
      "541/1000, test_loss 17.597183878341784\n",
      "542/1000, test_loss 17.597136375294426\n",
      "543/1000, test_loss 17.597089256436394\n",
      "544/1000, test_loss 17.59704251912193\n",
      "545/1000, test_loss 17.596996160724373\n",
      "546/1000, test_loss 17.596950178636003\n",
      "547/1000, test_loss 17.596904570267913\n",
      "548/1000, test_loss 17.59685933304984\n",
      "549/1000, test_loss 17.596814464430036\n",
      "550/1000, test_loss 17.596769961875143\n",
      "551/1000, test_loss 17.59672582287001\n",
      "552/1000, test_loss 17.596682044917586\n",
      "553/1000, test_loss 17.596638625538773\n",
      "554/1000, test_loss 17.596595562272284\n",
      "555/1000, test_loss 17.596552852674524\n",
      "556/1000, test_loss 17.596510494319418\n",
      "557/1000, test_loss 17.596468484798315\n",
      "558/1000, test_loss 17.596426821719835\n",
      "559/1000, test_loss 17.596385502709737\n",
      "560/1000, test_loss 17.596344525410807\n",
      "561/1000, test_loss 17.5963038874827\n",
      "562/1000, test_loss 17.596263586601843\n",
      "563/1000, test_loss 17.596223620461267\n",
      "564/1000, test_loss 17.59618398677052\n",
      "565/1000, test_loss 17.59614468325552\n",
      "566/1000, test_loss 17.59610570765844\n",
      "567/1000, test_loss 17.59606705773759\n",
      "568/1000, test_loss 17.596028731267253\n",
      "569/1000, test_loss 17.595990726037634\n",
      "570/1000, test_loss 17.59595303985468\n",
      "571/1000, test_loss 17.595915670539995\n",
      "572/1000, test_loss 17.595878615930694\n",
      "573/1000, test_loss 17.595841873879316\n",
      "574/1000, test_loss 17.59580544225369\n",
      "575/1000, test_loss 17.595769318936814\n",
      "576/1000, test_loss 17.595733501826757\n",
      "577/1000, test_loss 17.59569798883654\n",
      "578/1000, test_loss 17.595662777894006\n",
      "579/1000, test_loss 17.59562786694174\n",
      "580/1000, test_loss 17.595593253936936\n",
      "581/1000, test_loss 17.595558936851294\n",
      "582/1000, test_loss 17.595524913670904\n",
      "583/1000, test_loss 17.59549118239614\n",
      "584/1000, test_loss 17.595457741041578\n",
      "585/1000, test_loss 17.59542458763585\n",
      "586/1000, test_loss 17.59539172022156\n",
      "587/1000, test_loss 17.595359136855176\n",
      "588/1000, test_loss 17.59532683560693\n",
      "589/1000, test_loss 17.595294814560713\n",
      "590/1000, test_loss 17.595263071813967\n",
      "591/1000, test_loss 17.595231605477593\n",
      "592/1000, test_loss 17.59520041367584\n",
      "593/1000, test_loss 17.595169494546223\n",
      "594/1000, test_loss 17.595138846239397\n",
      "595/1000, test_loss 17.595108466919097\n",
      "596/1000, test_loss 17.59507835476199\n",
      "597/1000, test_loss 17.59504850795763\n",
      "598/1000, test_loss 17.595018924708338\n",
      "599/1000, test_loss 17.594989603229106\n",
      "600/1000, test_loss 17.59496054174749\n",
      "601/1000, test_loss 17.59493173850356\n",
      "602/1000, test_loss 17.59490319174977\n",
      "603/1000, test_loss 17.594874899750856\n",
      "604/1000, test_loss 17.5948468607838\n",
      "605/1000, test_loss 17.594819073137675\n",
      "606/1000, test_loss 17.5947915351136\n",
      "607/1000, test_loss 17.594764245024642\n",
      "608/1000, test_loss 17.594737201195706\n",
      "609/1000, test_loss 17.594710401963475\n",
      "610/1000, test_loss 17.594683845676304\n",
      "611/1000, test_loss 17.594657530694153\n",
      "612/1000, test_loss 17.594631455388477\n",
      "613/1000, test_loss 17.59460561814216\n",
      "614/1000, test_loss 17.594580017349433\n",
      "615/1000, test_loss 17.59455465141577\n",
      "616/1000, test_loss 17.59452951875783\n",
      "617/1000, test_loss 17.594504617803352\n",
      "618/1000, test_loss 17.594479946991093\n",
      "619/1000, test_loss 17.594455504770732\n",
      "620/1000, test_loss 17.59443128960281\n",
      "621/1000, test_loss 17.594407299958622\n",
      "622/1000, test_loss 17.594383534320166\n",
      "623/1000, test_loss 17.59435999118003\n",
      "624/1000, test_loss 17.594336669041375\n",
      "625/1000, test_loss 17.59431356641778\n",
      "626/1000, test_loss 17.59429068183323\n",
      "627/1000, test_loss 17.59426801382201\n",
      "628/1000, test_loss 17.594245560928623\n",
      "629/1000, test_loss 17.594223321707737\n",
      "630/1000, test_loss 17.59420129472411\n",
      "631/1000, test_loss 17.59417947855249\n",
      "632/1000, test_loss 17.594157871777572\n",
      "633/1000, test_loss 17.594136472993902\n",
      "634/1000, test_loss 17.594115280805838\n",
      "635/1000, test_loss 17.594094293827435\n",
      "636/1000, test_loss 17.594073510682406\n",
      "637/1000, test_loss 17.594052930004043\n",
      "638/1000, test_loss 17.59403255043514\n",
      "639/1000, test_loss 17.594012370627954\n",
      "640/1000, test_loss 17.59399238924408\n",
      "641/1000, test_loss 17.593972604954445\n",
      "642/1000, test_loss 17.593953016439194\n",
      "643/1000, test_loss 17.59393362238766\n",
      "644/1000, test_loss 17.593914421498255\n",
      "645/1000, test_loss 17.593895412478446\n",
      "646/1000, test_loss 17.59387659404467\n",
      "647/1000, test_loss 17.593857964922258\n",
      "648/1000, test_loss 17.5938395238454\n",
      "649/1000, test_loss 17.59382126955706\n",
      "650/1000, test_loss 17.593803200808914\n",
      "651/1000, test_loss 17.593785316361284\n",
      "652/1000, test_loss 17.593767614983097\n",
      "653/1000, test_loss 17.5937500954518\n",
      "654/1000, test_loss 17.593732756553305\n",
      "655/1000, test_loss 17.59371559708194\n",
      "656/1000, test_loss 17.59369861584036\n",
      "657/1000, test_loss 17.593681811639517\n",
      "658/1000, test_loss 17.593665183298594\n",
      "659/1000, test_loss 17.593648729644933\n",
      "660/1000, test_loss 17.593632449513983\n",
      "661/1000, test_loss 17.593616341749247\n",
      "662/1000, test_loss 17.593600405202217\n",
      "663/1000, test_loss 17.593584638732313\n",
      "664/1000, test_loss 17.593569041206855\n",
      "665/1000, test_loss 17.59355361150096\n",
      "666/1000, test_loss 17.593538348497514\n",
      "667/1000, test_loss 17.593523251087134\n",
      "668/1000, test_loss 17.593508318168062\n",
      "669/1000, test_loss 17.593493548646155\n",
      "670/1000, test_loss 17.593478941434824\n",
      "671/1000, test_loss 17.593464495454953\n",
      "672/1000, test_loss 17.59345020963487\n",
      "673/1000, test_loss 17.593436082910298\n",
      "674/1000, test_loss 17.593422114224282\n",
      "675/1000, test_loss 17.59340830252714\n",
      "676/1000, test_loss 17.59339464677644\n",
      "677/1000, test_loss 17.59338114593691\n",
      "678/1000, test_loss 17.593367798980402\n",
      "679/1000, test_loss 17.593354604885842\n",
      "680/1000, test_loss 17.593341562639196\n",
      "681/1000, test_loss 17.593328671233397\n",
      "682/1000, test_loss 17.59331592966829\n",
      "683/1000, test_loss 17.5933033369506\n",
      "684/1000, test_loss 17.593290892093894\n",
      "685/1000, test_loss 17.593278594118498\n",
      "686/1000, test_loss 17.593266442051476\n",
      "687/1000, test_loss 17.593254434926585\n",
      "688/1000, test_loss 17.593242571784184\n",
      "689/1000, test_loss 17.59323085167126\n",
      "690/1000, test_loss 17.593219273641314\n",
      "691/1000, test_loss 17.593207836754353\n",
      "692/1000, test_loss 17.593196540076818\n",
      "693/1000, test_loss 17.593185382681582\n",
      "694/1000, test_loss 17.593174363647844\n",
      "695/1000, test_loss 17.59316348206113\n",
      "696/1000, test_loss 17.59315273701323\n",
      "697/1000, test_loss 17.59314212760217\n",
      "698/1000, test_loss 17.593131652932144\n",
      "699/1000, test_loss 17.593121312113478\n",
      "700/1000, test_loss 17.5931111042626\n",
      "701/1000, test_loss 17.59310102850199\n",
      "702/1000, test_loss 17.593091083960122\n",
      "703/1000, test_loss 17.59308126977144\n",
      "704/1000, test_loss 17.593071585076324\n",
      "705/1000, test_loss 17.59306202902101\n",
      "706/1000, test_loss 17.593052600757595\n",
      "707/1000, test_loss 17.593043299443963\n",
      "708/1000, test_loss 17.59303412424376\n",
      "709/1000, test_loss 17.59302507432635\n",
      "710/1000, test_loss 17.593016148866763\n",
      "711/1000, test_loss 17.593007347045674\n",
      "712/1000, test_loss 17.59299866804937\n",
      "713/1000, test_loss 17.592990111069657\n",
      "714/1000, test_loss 17.592981675303903\n",
      "715/1000, test_loss 17.59297335995493\n",
      "716/1000, test_loss 17.592965164231007\n",
      "717/1000, test_loss 17.592957087345802\n",
      "718/1000, test_loss 17.592949128518363\n",
      "719/1000, test_loss 17.592941286973055\n",
      "720/1000, test_loss 17.59293356193953\n",
      "721/1000, test_loss 17.5929259526527\n",
      "722/1000, test_loss 17.5929184583527\n",
      "723/1000, test_loss 17.59291107828482\n",
      "724/1000, test_loss 17.592903811699525\n",
      "725/1000, test_loss 17.592896657852368\n",
      "726/1000, test_loss 17.592889616003983\n",
      "727/1000, test_loss 17.592882685420033\n",
      "728/1000, test_loss 17.59287586537119\n",
      "729/1000, test_loss 17.592869155133084\n",
      "730/1000, test_loss 17.59286255398629\n",
      "731/1000, test_loss 17.59285606121626\n",
      "732/1000, test_loss 17.592849676113335\n",
      "733/1000, test_loss 17.59284339797266\n",
      "734/1000, test_loss 17.592837226094193\n",
      "735/1000, test_loss 17.592831159782648\n",
      "736/1000, test_loss 17.592825198347455\n",
      "737/1000, test_loss 17.592819341102768\n",
      "738/1000, test_loss 17.592813587367385\n",
      "739/1000, test_loss 17.592807936464727\n",
      "740/1000, test_loss 17.592802387722838\n",
      "741/1000, test_loss 17.5927969404743\n",
      "742/1000, test_loss 17.592791594056262\n",
      "743/1000, test_loss 17.592786347810346\n",
      "744/1000, test_loss 17.592781201082655\n",
      "745/1000, test_loss 17.59277615322374\n",
      "746/1000, test_loss 17.59277120358856\n",
      "747/1000, test_loss 17.592766351536447\n",
      "748/1000, test_loss 17.592761596431078\n",
      "749/1000, test_loss 17.592756937640466\n",
      "750/1000, test_loss 17.592752374536893\n",
      "751/1000, test_loss 17.592747906496914\n",
      "752/1000, test_loss 17.59274353290131\n",
      "753/1000, test_loss 17.592739253135043\n",
      "754/1000, test_loss 17.592735066587277\n",
      "755/1000, test_loss 17.592730972651303\n",
      "756/1000, test_loss 17.592726970724513\n",
      "757/1000, test_loss 17.5927230602084\n",
      "758/1000, test_loss 17.59271924050851\n",
      "759/1000, test_loss 17.592715511034413\n",
      "760/1000, test_loss 17.59271187119968\n",
      "761/1000, test_loss 17.59270832042186\n",
      "762/1000, test_loss 17.592704858122442\n",
      "763/1000, test_loss 17.59270148372683\n",
      "764/1000, test_loss 17.592698196664326\n",
      "765/1000, test_loss 17.592694996368085\n",
      "766/1000, test_loss 17.592691882275115\n",
      "767/1000, test_loss 17.592688853826225\n",
      "768/1000, test_loss 17.592685910466\n",
      "769/1000, test_loss 17.592683051642798\n",
      "770/1000, test_loss 17.5926802768087\n",
      "771/1000, test_loss 17.592677585419505\n",
      "772/1000, test_loss 17.592674976934674\n",
      "773/1000, test_loss 17.592672450817332\n",
      "774/1000, test_loss 17.59267000653424\n",
      "775/1000, test_loss 17.592667643555764\n",
      "776/1000, test_loss 17.592665361355845\n",
      "777/1000, test_loss 17.592663159411963\n",
      "778/1000, test_loss 17.592661037205165\n",
      "779/1000, test_loss 17.59265899421998\n",
      "780/1000, test_loss 17.592657029944426\n",
      "781/1000, test_loss 17.592655143869976\n",
      "782/1000, test_loss 17.592653335491548\n",
      "783/1000, test_loss 17.592651604307456\n",
      "784/1000, test_loss 17.59264994981941\n",
      "785/1000, test_loss 17.592648371532494\n",
      "786/1000, test_loss 17.592646868955107\n",
      "787/1000, test_loss 17.592645441599\n",
      "788/1000, test_loss 17.592644088979185\n",
      "789/1000, test_loss 17.59264281061398\n",
      "790/1000, test_loss 17.592641606024923\n",
      "791/1000, test_loss 17.592640474736804\n",
      "792/1000, test_loss 17.5926394162776\n",
      "793/1000, test_loss 17.592638430178482\n",
      "794/1000, test_loss 17.592637515973784\n",
      "795/1000, test_loss 17.59263667320098\n",
      "796/1000, test_loss 17.592635901400655\n",
      "797/1000, test_loss 17.592635200116494\n",
      "798/1000, test_loss 17.592634568895267\n",
      "799/1000, test_loss 17.592634007286787\n",
      "800/1000, test_loss 17.59263351484391\n",
      "801/1000, test_loss 17.592633091122497\n",
      "802/1000, test_loss 17.592632735681406\n",
      "803/1000, test_loss 17.59263244808247\n",
      "804/1000, test_loss 17.59263222789047\n",
      "805/1000, test_loss 17.592632074673116\n",
      "806/1000, test_loss 17.59263198800103\n",
      "807/1000, test_loss 17.592631967447733\n",
      "808/1000, test_loss 17.59263201258961\n",
      "809/1000, test_loss 17.5926321230059\n",
      "810/1000, test_loss 17.592632298278673\n",
      "811/1000, test_loss 17.592632537992813\n",
      "812/1000, test_loss 17.592632841736002\n",
      "813/1000, test_loss 17.592633209098693\n",
      "814/1000, test_loss 17.59263363967409\n",
      "815/1000, test_loss 17.59263413305815\n",
      "816/1000, test_loss 17.592634688849518\n",
      "817/1000, test_loss 17.592635306649576\n",
      "818/1000, test_loss 17.592635986062362\n",
      "819/1000, test_loss 17.59263672669458\n",
      "820/1000, test_loss 17.592637528155592\n",
      "821/1000, test_loss 17.592638390057367\n",
      "822/1000, test_loss 17.592639312014494\n",
      "823/1000, test_loss 17.592640293644163\n",
      "824/1000, test_loss 17.592641334566114\n",
      "825/1000, test_loss 17.59264243440266\n",
      "826/1000, test_loss 17.592643592778646\n",
      "827/1000, test_loss 17.592644809321445\n",
      "828/1000, test_loss 17.59264608366091\n",
      "829/1000, test_loss 17.59264741542942\n",
      "830/1000, test_loss 17.592648804261792\n",
      "831/1000, test_loss 17.59265024979531\n",
      "832/1000, test_loss 17.592651751669678\n",
      "833/1000, test_loss 17.592653309527048\n",
      "834/1000, test_loss 17.592654923011946\n",
      "835/1000, test_loss 17.592656591771313\n",
      "836/1000, test_loss 17.592658315454432\n",
      "837/1000, test_loss 17.59266009371296\n",
      "838/1000, test_loss 17.592661926200883\n",
      "839/1000, test_loss 17.592663812574518\n",
      "840/1000, test_loss 17.59266575249248\n",
      "841/1000, test_loss 17.59266774561567\n",
      "842/1000, test_loss 17.59266979160729\n",
      "843/1000, test_loss 17.59267189013277\n",
      "844/1000, test_loss 17.592674040859805\n",
      "845/1000, test_loss 17.592676243458307\n",
      "846/1000, test_loss 17.592678497600417\n",
      "847/1000, test_loss 17.592680802960462\n",
      "848/1000, test_loss 17.592683159214964\n",
      "849/1000, test_loss 17.592685566042594\n",
      "850/1000, test_loss 17.592688023124207\n",
      "851/1000, test_loss 17.59269053014277\n",
      "852/1000, test_loss 17.592693086783395\n",
      "853/1000, test_loss 17.592695692733297\n",
      "854/1000, test_loss 17.592698347681782\n",
      "855/1000, test_loss 17.592701051320258\n",
      "856/1000, test_loss 17.59270380334217\n",
      "857/1000, test_loss 17.59270660344305\n",
      "858/1000, test_loss 17.592709451320456\n",
      "859/1000, test_loss 17.592712346673967\n",
      "860/1000, test_loss 17.592715289205174\n",
      "861/1000, test_loss 17.59271827861768\n",
      "862/1000, test_loss 17.59272131461707\n",
      "863/1000, test_loss 17.59272439691089\n",
      "864/1000, test_loss 17.592727525208655\n",
      "865/1000, test_loss 17.592730699221825\n",
      "866/1000, test_loss 17.59273391866379\n",
      "867/1000, test_loss 17.592737183249856\n",
      "868/1000, test_loss 17.592740492697242\n",
      "869/1000, test_loss 17.59274384672505\n",
      "870/1000, test_loss 17.592747245054273\n",
      "871/1000, test_loss 17.59275068740777\n",
      "872/1000, test_loss 17.59275417351025\n",
      "873/1000, test_loss 17.592757703088267\n",
      "874/1000, test_loss 17.59276127587021\n",
      "875/1000, test_loss 17.592764891586274\n",
      "876/1000, test_loss 17.592768549968465\n",
      "877/1000, test_loss 17.592772250750592\n",
      "878/1000, test_loss 17.592775993668234\n",
      "879/1000, test_loss 17.592779778458734\n",
      "880/1000, test_loss 17.59278360486121\n",
      "881/1000, test_loss 17.592787472616507\n",
      "882/1000, test_loss 17.59279138146721\n",
      "883/1000, test_loss 17.592795331157635\n",
      "884/1000, test_loss 17.59279932143379\n",
      "885/1000, test_loss 17.5928033520434\n",
      "886/1000, test_loss 17.59280742273586\n",
      "887/1000, test_loss 17.59281153326225\n",
      "888/1000, test_loss 17.592815683375314\n",
      "889/1000, test_loss 17.592819872829445\n",
      "890/1000, test_loss 17.59282410138068\n",
      "891/1000, test_loss 17.592828368786698\n",
      "892/1000, test_loss 17.59283267480677\n",
      "893/1000, test_loss 17.5928370192018\n",
      "894/1000, test_loss 17.592841401734283\n",
      "895/1000, test_loss 17.592845822168304\n",
      "896/1000, test_loss 17.59285028026951\n",
      "897/1000, test_loss 17.59285477580514\n",
      "898/1000, test_loss 17.59285930854396\n",
      "899/1000, test_loss 17.592863878256303\n",
      "900/1000, test_loss 17.592868484714025\n",
      "901/1000, test_loss 17.5928731276905\n",
      "902/1000, test_loss 17.592877806960633\n",
      "903/1000, test_loss 17.59288252230082\n",
      "904/1000, test_loss 17.59288727348896\n",
      "905/1000, test_loss 17.59289206030441\n",
      "906/1000, test_loss 17.59289688252804\n",
      "907/1000, test_loss 17.592901739942153\n",
      "908/1000, test_loss 17.592906632330518\n",
      "909/1000, test_loss 17.592911559478345\n",
      "910/1000, test_loss 17.59291652117227\n",
      "911/1000, test_loss 17.592921517200377\n",
      "912/1000, test_loss 17.59292654735213\n",
      "913/1000, test_loss 17.592931611418425\n",
      "914/1000, test_loss 17.59293670919155\n",
      "915/1000, test_loss 17.592941840465173\n",
      "916/1000, test_loss 17.59294700503434\n",
      "917/1000, test_loss 17.59295220269546\n",
      "918/1000, test_loss 17.592957433246315\n",
      "919/1000, test_loss 17.592962696486026\n",
      "920/1000, test_loss 17.59296799221505\n",
      "921/1000, test_loss 17.592973320235192\n",
      "922/1000, test_loss 17.592978680349553\n",
      "923/1000, test_loss 17.592984072362572\n",
      "924/1000, test_loss 17.592989496079984\n",
      "925/1000, test_loss 17.59299495130882\n",
      "926/1000, test_loss 17.5930004378574\n",
      "927/1000, test_loss 17.593005955535308\n",
      "928/1000, test_loss 17.593011504153417\n",
      "929/1000, test_loss 17.593017083523858\n",
      "930/1000, test_loss 17.593022693460007\n",
      "931/1000, test_loss 17.593028333776488\n",
      "932/1000, test_loss 17.593034004289162\n",
      "933/1000, test_loss 17.593039704815112\n",
      "934/1000, test_loss 17.59304543517265\n",
      "935/1000, test_loss 17.5930511951813\n",
      "936/1000, test_loss 17.593056984661775\n",
      "937/1000, test_loss 17.593062803435995\n",
      "938/1000, test_loss 17.593068651327062\n",
      "939/1000, test_loss 17.59307452815926\n",
      "940/1000, test_loss 17.59308043375805\n",
      "941/1000, test_loss 17.59308636795004\n",
      "942/1000, test_loss 17.59309233056301\n",
      "943/1000, test_loss 17.59309832142587\n",
      "944/1000, test_loss 17.593104340368697\n",
      "945/1000, test_loss 17.593110387222666\n",
      "946/1000, test_loss 17.593116461820106\n",
      "947/1000, test_loss 17.593122563994445\n",
      "948/1000, test_loss 17.59312869358024\n",
      "949/1000, test_loss 17.593134850413122\n",
      "950/1000, test_loss 17.59314103432984\n",
      "951/1000, test_loss 17.593147245168222\n",
      "952/1000, test_loss 17.59315348276718\n",
      "953/1000, test_loss 17.593159746966695\n",
      "954/1000, test_loss 17.593166037607816\n",
      "955/1000, test_loss 17.593172354532637\n",
      "956/1000, test_loss 17.59317869758434\n",
      "957/1000, test_loss 17.593185066607102\n",
      "958/1000, test_loss 17.59319146144618\n",
      "959/1000, test_loss 17.593197881947848\n",
      "960/1000, test_loss 17.59320432795938\n",
      "961/1000, test_loss 17.593210799329103\n",
      "962/1000, test_loss 17.593217295906335\n",
      "963/1000, test_loss 17.593223817541393\n",
      "964/1000, test_loss 17.59323036408561\n",
      "965/1000, test_loss 17.593236935391285\n",
      "966/1000, test_loss 17.59324353131171\n",
      "967/1000, test_loss 17.59325015170116\n",
      "968/1000, test_loss 17.593256796414874\n",
      "969/1000, test_loss 17.593263465309054\n",
      "970/1000, test_loss 17.593270158240866\n",
      "971/1000, test_loss 17.59327687506841\n",
      "972/1000, test_loss 17.593283615650744\n",
      "973/1000, test_loss 17.59329037984786\n",
      "974/1000, test_loss 17.593297167520696\n",
      "975/1000, test_loss 17.593303978531083\n",
      "976/1000, test_loss 17.5933108127418\n",
      "977/1000, test_loss 17.59331767001653\n",
      "978/1000, test_loss 17.59332455021985\n",
      "979/1000, test_loss 17.59333145321726\n",
      "980/1000, test_loss 17.593338378875146\n",
      "981/1000, test_loss 17.593345327060778\n",
      "982/1000, test_loss 17.593352297642316\n",
      "983/1000, test_loss 17.593359290488777\n",
      "984/1000, test_loss 17.59336630547009\n",
      "985/1000, test_loss 17.59337334245701\n",
      "986/1000, test_loss 17.59338040132117\n",
      "987/1000, test_loss 17.59338748193505\n",
      "988/1000, test_loss 17.593394584171982\n",
      "989/1000, test_loss 17.593401707906136\n",
      "990/1000, test_loss 17.59340885301253\n",
      "991/1000, test_loss 17.593416019366995\n",
      "992/1000, test_loss 17.59342320684621\n",
      "993/1000, test_loss 17.593430415327653\n",
      "994/1000, test_loss 17.593437644689626\n",
      "995/1000, test_loss 17.59344489481124\n",
      "996/1000, test_loss 17.5934521655724\n",
      "997/1000, test_loss 17.593459456853836\n",
      "998/1000, test_loss 17.593466768537038\n",
      "999/1000, test_loss 17.59347410050431\n",
      "1000/1000, test_loss 17.593481452638716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "X = iris.data[:, 2:][50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "model_scratch = ScratchLogisticRegression(1000, 0.001, True, True)\n",
    "model_scratch.fit(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1]\n",
      "[0.48510111 0.57607411 0.4954316  0.55624094 0.59520261 0.56547638\n",
      " 0.53479391 0.49687176 0.57466659 0.60052449 0.54791799 0.62469687\n",
      " 0.6060396  0.49495156 0.55197082 0.52305884 0.4954316  0.51417688\n",
      " 0.49591164 0.48462146 0.55197082 0.44778928 0.54363349 0.55576688\n",
      " 0.53861386]\n",
      "--------------------------------------------------\n",
      "[[ 6  2]\n",
      " [ 7 10]]\n",
      "0.64\n",
      "0.8333333333333334\n",
      "0.5882352941176471\n",
      "0.6896551724137931\n",
      "log loss: 17.593488327358447\n"
     ]
    }
   ],
   "source": [
    "# 評価の確認\n",
    "pred = model_scratch.predict(X_test)\n",
    "pred_proba = model_scratch.predict_proba(X_test)\n",
    "\n",
    "print(pred.ravel())\n",
    "print(pred_proba)\n",
    "print(\"-\"*50)\n",
    "# 混合行列、正解率、適合率、再現率、F値\n",
    "print(confusion_matrix(pred, y_test))\n",
    "print(accuracy_score(pred, y_test))\n",
    "print(precision_score(pred, y_test))\n",
    "print(recall_score(pred, y_test))\n",
    "print(f1_score(pred, y_test))\n",
    "\n",
    "# スクラッチのクロスエントロピー\n",
    "_, cost = model_scratch.cost_function(X_test, y_test, pred_proba)\n",
    "print(\"log loss: {}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0]\n",
      "[0.52877735 0.88913456 0.96798053 0.05138881 0.06277319 0.03785298\n",
      " 0.39727709 0.97402401 0.00547457 0.23847379 0.18528484 0.01489268\n",
      " 0.55471013 0.58113101 0.96626881 0.22883272 0.48842459 0.55471013\n",
      " 0.02039217 0.0092844  0.99540772 0.48842459 0.29007187 0.03083935\n",
      " 0.46237292]\n",
      "[[13  3]\n",
      " [ 1  8]]\n",
      "0.84\n",
      "0.7272727272727273\n",
      "0.8888888888888888\n",
      "0.7999999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.526236207082917"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearnのロジスティック回帰と比較する\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris.data[:, 2:][50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "pred_sk = model.predict(X_test)\n",
    "pred_sk_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# predとprobaの確認\n",
    "print(pred_sk)\n",
    "print(pred_sk_proba)\n",
    "\n",
    "# 混合行列、正解率、適合率、再現率、F値\n",
    "print(confusion_matrix(pred_sk, y_test))\n",
    "print(accuracy_score(pred_sk, y_test))\n",
    "print(precision_score(pred_sk, y_test))\n",
    "print(recall_score(pred_sk, y_test))\n",
    "print(f1_score(pred_sk, y_test))\n",
    "\n",
    "# sklearnのクロスエントロピー\n",
    "\n",
    "log_loss(y_test, pred_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習曲線のプロット\n",
    "学習曲線を見て損失が適切に下がっているかどうか確認してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8ddnshBIAoSQQExYAoRENoMGaq/V6rXXIlqxVi1dvGj1Z91atWrFLra1em9761arwrV1u61VLEpr77W21trr7aMuJMi+hbAGKAlhDRCyfX5/ZMCIgZDMhJOZvJ+Pxzxm5nvmTN7fQd85OXPmjLk7IiISX0JBBxARkehTuYuIxCGVu4hIHFK5i4jEIZW7iEgcSgw6AMDAgQN9+PDhQccQEYkpZWVl2909q61l3aLchw8fTmlpadAxRERiipltONoy7ZYREYlDKncRkTikchcRiUPdYp+7iMSfhoYGKisrqaurCzpKzEtJSSEvL4+kpKTjXkflLiJdorKykvT0dIYPH46ZBR0nZrk7NTU1VFZWkp+ff9zrabeMiHSJuro6MjMzVewRMjMyMzM7/BeQyl1EuoyKPTo68zrGdLnX11dTXn4LDQ27go4iItKtxPQ+94MHN7F588+AJgoKfhZ0HBGRbiOmt9zT00/lpJOuZ/Pmx9m7d0HQcUSkm9m1axePP/54h9ebOnUqu3Z1fI/AlVdeydy5czu8XleI6XIHyM+/l6SkgaxefQPuzUHHEZFu5Gjl3tTUdMz1Xn31Vfr3799VsU6ImN4tA5CU1J+RI3/CypUz2Lr1KU466ZqgI4nIEcrLb6G2dmFUnzMtrZiCgoeP+ZiZM2dSUVFBcXExSUlJpKWlkZOTw8KFC1m+fDkXX3wxmzZtoq6ujptvvplrr70W+OB8V7W1tZx//vl84hOf4O9//zu5ubn87ne/o3fv3u3me+ONN7j99ttpbGxk0qRJzJo1i169ejFz5kxeeeUVEhMTOe+887j//vv5zW9+ww9+8AMSEhLo168fb731VsSvT8xvuQMMGnQF/fqdydq1M2loqAk6joh0Ez/60Y8YOXIkCxcu5Cc/+Qnvvfce9913H8uXLwfgqaeeoqysjNLSUh555BFqaj7aH+Xl5dx4440sW7aM/v3789JLL7X7c+vq6rjyyiuZM2cOS5YsobGxkVmzZrFjxw7mzZvHsmXLWLx4Md/5zncAuOeee/jjH//IokWLeOWVV6Iy95jfcoeWw4QKCh6jtHQia9feRWHhE0FHEpFW2tvCPlEmT578oQ8CPfLII8ybNw+ATZs2UV5eTmZm5ofWyc/Pp7i4GIDTTjuN9evXt/tzVq1aRX5+PqNHjwZgxowZPPbYY9x0002kpKRwzTXXcMEFF3DhhRcCcMYZZ3DllVdy+eWXc8kll0RjqvGx5Q6QljaevLyb2br1F+zZ827QcUSkG0pNTT18+69//St//vOfefvtt1m0aBETJ05s84NCvXr1Onw7ISGBxsbGdn+Ou7c5npiYyHvvvcfnPvc5fvvb3zJlyhQAZs+ezb333sumTZsoLi5u8y+IjoqbcgcYPvz7JCfnhN9cPfYbJiIS/9LT09m7d2+by3bv3k1GRgZ9+vRh5cqVvPPOO1H7uUVFRaxfv541a9YA8Mtf/pJPfvKT1NbWsnv3bqZOncrDDz/MwoUt70NUVFTwsY99jHvuuYeBAweyadOmiDPExW6ZQxIT0xk16kGWL5/Oli3/SW7uDUFHEpEAZWZmcsYZZzBu3Dh69+7NoEGDDi+bMmUKs2fPZsKECRQWFnL66adH7eempKTw9NNPc9lllx1+Q/W6665jx44dTJs2jbq6Otydhx56CIA77riD8vJy3J1zzz2XU045JeIMdrQ/Hw4/wOwp4EKgyt3HhcfmAIXhh/QHdrl7sZkNB1YAq8LL3nH369oLUVJS4tH6JiZ3Z9GiT1Fbu4DJk1eRnJwdlecVkY5ZsWIFJ598ctAx4kZbr6eZlbl7SVuPP57dMs8AU1oPuPvn3b3Y3YuBl4CXWy2uOLTseIo92g69udrUtI+Kim+e6B8vItIttFvu7v4WsKOtZdZyNpvLgeejnCsiqalFDBlyG9u2PcuuXX8LOo6IxJkbb7yR4uLiD12efvrpoGN9SKT73M8Etrl7eauxfDN7H9gDfMfd/6+tFc3sWuBagKFDh0YY46OGDfsO27b9mvLyGzjttAWEQnH19oKIBOixxx4LOkK7Ij1a5gt8eKt9KzDU3ScC3wB+bWZ921rR3Z9w9xJ3L8nKyoowxkclJKQyatTD7Nu3JHxyMRGRnqPT5W5micAlwJxDY+5+0N1rwrfLgApgdKQhO2vgwIsZMOB81q//HgcPbgkqhojICRfJlvungJXuXnlowMyyzCwhfHsEUACsjSxi57W8ufozmpvrqai4LagYIiInXLvlbmbPA28DhWZWaWZXhxdN56NvpJ4FLDazRcBc4Dp3b/PN2BOld++RDB06k6qqF9i5840go4iInDDHc7TMF9w9x92T3D3P3Z8Mj1/p7rOPeOxL7j7W3U9x91Pd/fddFbwjhg69k5SUEZSX30Rzc33QcUSkm0pLSzvqsvXr1zNu3LgTmCYycXX6gaNJSOhNQcHP2L9/JZs2PRh0HBGRLtdjjg/MzJzKwIEXs2HDDxk06IukpET/8EsRadstt8DC6J7OneJieLidk03eeeedDBs2jBtuaDkVyfe//33MjLfeeoudO3fS0NDAvffey7Rp0zr0s+vq6rj++uspLS0lMTGRBx98kHPOOYdly5Zx1VVXUV9fT3NzMy+99BInnXQSl19+OZWVlTQ1NfHd736Xz3/+852d9nHrEVvuh4wa9TDgrFlza9BRROQEmD59OnPmHD6gjxdffJGrrrqKefPmsWDBAt58801uu+22o57F8WgOHee+ZMkSnn/+eWbMmEFdXR2zZ8/m5ptvZuHChZSWlpKXl8drr73GSSedxKJFi1i6dOnhM0F2tR6z5Q6QkjKMYcO+w7p136am5jUyM0/MiyzS07W3hd1VJk6cSFVVFVu2bKG6upqMjAxycnK49dZbeeuttwiFQmzevJlt27YxePDg437ev/3tb3zta18DWs4AOWzYMFavXs3HP/5x7rvvPiorK7nkkksoKChg/Pjx3H777dx5551ceOGFnHnmmV013Q/pUVvuAEOG3Ebv3oWUl99IU9OBoOOISBe79NJLmTt3LnPmzGH69Ok899xzVFdXU1ZWxsKFCxk0aFCb53E/lqNt6X/xi1/klVdeoXfv3nz605/mL3/5C6NHj6asrIzx48dz1113cc8990RjWu3qceUeCvVi9OhZ1NWtZePGfws6joh0senTp/PCCy8wd+5cLr30Unbv3k12djZJSUm8+eabbNiwocPPedZZZ/Hcc88BsHr1ajZu3EhhYSFr165lxIgRfP3rX+eiiy5i8eLFbNmyhT59+vDlL3+Z22+/nQULFkR7im3qUbtlDsnIOIdBg65g48Yfk539RVJTdVpSkXg1duxY9u7dS25uLjk5OXzpS1/iM5/5DCUlJRQXF1NUVNTh57zhhhu47rrrGD9+PImJiTzzzDP06tWLOXPm8Ktf/YqkpCQGDx7M3Xffzfz587njjjsIhUIkJSUxa9asLpjlR7V7PvcTIZrncz9e9fVVvPdeEampEygufpOWE1yKSLTofO7R1RXnc49LycnZjBjxY3bv/l+2bfuvoOOIiERVj9wtc0hOztX84x/PUFFxO5mZF5KUlNn+SiIS15YsWcIVV1zxobFevXrx7rvvBpSoc3p0uZuFGD16NmVlp1JR8U2Kip4MOpJIXHH3mNvlOX78+MNfXN1ddGb3eY/dLXNIWtp48vK+wT/+8RS7drX5vSIi0gkpKSnU1NR0qpjkA+5OTU0NKSkpHVqvR2+5HzJ8+N1UVc1h9errKCl5n1AoOehIIjEvLy+PyspKqqurg44S81JSUsjLy+vQOip3Wr61qaDgUZYu/QybNj3IsGEzg44kEvOSkpLIz88POkaP1eN3yxwycOCFDBx4CRs23MOBA+uCjiMiEhGVeyujRv0UswTKy2/UfkIRiWkq91ZSUvIYPvyH7NjxB6qrXwo6johIp6ncj5CbexNpaRNZs+ZmGhv3BB1HRKRTVO5HCIUSGT16NvX1W1m37rtBxxER6RSVexv69p3MSSfdwObNj7J3b1nQcUREOqzdcjezp8ysysyWthr7vpltNrOF4cvUVsvuMrM1ZrbKzD7dVcG72ogR95GcnM2qVV/FvSnoOCIiHXI8W+7PAG19ZdFD7l4cvrwKYGZjgOnA2PA6j5tZQrTCnkiJif0YNephamvL2Lz50aDjiIh0SLvl7u5vATuO8/mmAS+4+0F3XwesASZHkC9QWVmXM2DA+axd+23q6jp+Qn8RkaBEss/9JjNbHN5tkxEeywU2tXpMZXjsI8zsWjMrNbPS7vrxZDNj9OiWE+uvXn2Djn0XkZjR2XKfBYwEioGtwAPh8bZO/9ZmI7r7E+5e4u4lWVlZnYzR9VJShpGffy87drxKVdULQccRETkunSp3d9/m7k3u3gz8nA92vVQCQ1o9NA/YElnE4OXlfY309EmsWXMzDQ01QccREWlXp8rdzHJa3f0scOhImleA6WbWy8zygQLgvcgiBs8sgcLCX9DYuJOKituDjiMi0q52zwppZs8DZwMDzawS+B5wtpkV07LLZT3wVQB3X2ZmLwLLgUbgRo+T4wjT0iYwZMgdbNz472Rnf4kBAz4VdCQRkaPqsV+Q3RlNTQcoLZ2AezOTJi0hIaFP0JFEpAfTF2RHSUJCb0aPfoK6urWsX/+DoOOIiByVyr2DMjLOYfDgq9m06QH27n0/6DgiIm1SuXfCyJE/ISlpIKtWXUNzc2PQcUREPkLl3glJSRkUFDxCbe0CNm/+adBxREQ+QuXeSVlZl5GZeSHr1t2tr+UTkW5H5d5JZkZBweOYhVi9+jqdmkBEuhWVewRSUoaQn//v7Nz5J7Zt+2XQcUREDlO5Ryg393r69v0n1qy5hYMHtwYdR0QEULlHrOXUBE/S1LSf8nKdOVJEugeVexSkphaRn38P27f/lurqF4OOIyKico+WvLxvkJ4+ifLym6iv757npxeRnkPlHiWhUCJFRU/T2Lib8vKbgo4jIj2cyj2KUlPHMmzY3VRXv0h19ctBxxGRHkzlHmVDh95JWtpEVq++Xl/sISKBUblHWSiUFN49s4Py8puDjiMiPZTKvQukpZ3C0KHfoqrqObZv/33QcUSkB1K5d5Fhw75Naup4Vq/+Kg0NO4OOIyI9jMq9i4RCyRQVPU19fRUVFbcFHUdEehiVexdKTz+NoUO/yT/+8TQ1NX8IOo6I9CAq9y42fPj36NNnLKtWXUNDw46g44hID6Fy72KhUC9OPvm/aGio0oebROSEabfczewpM6sys6Wtxn5iZivNbLGZzTOz/uHx4WZ2wMwWhi+zuzJ8rEhPP5Vhw+6mqup5qqp07hkR6XrHs+X+DDDliLHXgXHuPgFYDdzValmFuxeHL9dFJ2bsGzr0LtLTJ7N69fU6NbCIdLl2y93d3wJ2HDH2J3c/9M3Q7wB5XZAtrrSce+ZZmpv3s2rVNTo1sIh0qWjsc/8K0PpQkHwze9/M/tfMzjzaSmZ2rZmVmllpdXXPOItiamoRI0b8mB07XmXr1ieDjiMicSyicjezbwONwHPhoa3AUHefCHwD+LWZ9W1rXXd/wt1L3L0kKysrkhgxJTf3Jvr3/2cqKm7VF2uLSJfpdLmb2QzgQuBLHt7H4O4H3b0mfLsMqABGRyNovDALUVT0NBBi5coZuDcFHUlE4lCnyt3MpgB3Ahe5+/5W41lmlhC+PQIoANZGI2g8SUkZSkHBI+ze/X9UVj4cdBwRiUPHcyjk88DbQKGZVZrZ1cCjQDrw+hGHPJ4FLDazRcBc4Dp31yd32jBo0L+SmTmNtWu/RW3t0vZXEBHpAOsOR22UlJR4aWlp0DFOuPr6KubPH0dycg6nnvouCQkpQUcSkRhiZmXuXtLWMn1CNUDJydkUFT3Nvn2LWbt2ZtBxRCSOqNwDlpl5Abm5X2Pz5p9SU/Na0HFEJE6o3LuBESP+g9TUcaxcOYP6+m1BxxGROKBy7wYSElI4+eTnaWzczcqVX9GnV0UkYir3biItbRwjR97Pjh2vsnnzo0HHEZEYp3LvRnJzb2TAgAuoqLiD2tolQccRkRimcu9GzIyioqdITOzP8uVfoKnpQNCRRCRGqdy7mZbDI59h//5lVFTcEXQcEYlRKvduKDNzCnl5t7Jly2NUV78UdBwRiUEq925qxIgfkZ4+mZUrv8KBAxVBxxGRGKNy76ZCoWTGjJmDWYhlyy6nuflg0JFEJIao3Lux3r2HU1T0DLW1C1iz5rag44hIDFG5d3MDB04jL+8bbNnyGFVVvwk6jojECJV7DBgx4kf07Xs6q1Zdzf79a4KOIyIxQOUeA0KhJMaMeQGzRJYvv4ymprqgI4lIN6dyjxEpKcMoKnqW2tqFrFlzc9BxRKSbU7nHkIEDP8OQIXeydesTbN36ZNBxRKQbU7nHmBEj7iMj41OsXn0De/a8F3QcEemmVO4xxiyBMWNeIDk5h2XLPkd9fVXQkUSkG1K5x6CkpEzGjZtHQ8N2li//PM3NjUFHEpFupt1yN7OnzKzKzJa2GhtgZq+bWXn4OqPVsrvMbI2ZrTKzT3dV8J4uPX0io0c/wa5df2Xt2m8GHUdEupnj2XJ/BphyxNhM4A13LwDeCN/HzMYA04Gx4XUeN7OEqKWVDxk8+Apyc79GZeVDbNv266DjiEg30m65u/tbwI4jhqcBz4ZvPwtc3Gr8BXc/6O7rgDXA5ChllTaMHPkA/fqdyapVV7Nnz/yg44hIN9HZfe6D3H0rQPg6OzyeC2xq9bjK8Jh0kVAoibFj55KcPJilS6dRV1cZdCQR6Qai/YaqtTHW5rc9m9m1ZlZqZqXV1dVRjtGzJCdnM27c72lq2svSpdNoatoXdCQRCVhny32bmeUAhK8PHY9XCQxp9bg8YEtbT+DuT7h7ibuXZGVldTKGHJKWNo4xY16gtnYhK1b8K+7NQUcSkQB1ttxfAWaEb88AftdqfLqZ9TKzfKAA0CdtTpDMzAsYOfJ+tm9/mXXr7g46jogEKLG9B5jZ88DZwEAzqwS+B/wIeNHMrgY2ApcBuPsyM3sRWA40Aje6e1MXZZc25OXdwv79K9i48T769Cli8OAvBx1JRAJg7m3uEj+hSkpKvLS0NOgYcaO5uZ7Fi6ewe/ffmDDhD2RknBt0JBHpAmZW5u4lbS3TJ1TjUCiUzNixL9OnTyFLl36W2tpFQUcSkRNM5R6nkpL6M378H0hM7MfixedTV7ch6EgicgKp3ONYSkoeEya8RnPzARYvnkJDw5GfRROReKVyj3OpqWMZN+53HDiwliVLLqKp6UDQkUTkBFC59wD9+5/FySf/ij17/h4+i2RD0JFEpIup3HuI7OzLKCh4lJqa37NixRXoCFWR+Nbuce4SP3Jzb6CpaR9r136TVatSKSz8OWb6/S4Sj1TuPczQoXfQ1LSXDRt+SEJCGqNGPYxZW6cEEpFYpnLvgYYP/wFNTbVUVj5EQkIaI0bcF3QkEYkylXsPZGaMHPkATU372Ljx3zALMXz4PdqCF4kjKvceyswYPXoW0MyGDffS3NzAiBH/roIXiRMq9x7MLMTo0f+JWRKbNv0Y93pGjnxABS8SB1TuPZxZiIKCxzBLorLyIdwbGTXqpyp4kRinchfMLHzUTBKVlQ/Q1FTL6NFPEArpPw+RWKX/ewU49CbrT0hISGPDhh/Q0FDNmDFzSEjoE3Q0EekEfYJFDjMz8vO/T0HB49TU/A+LFp2nk42JxCiVu3xEbu71jBnzInv3zuf998+irq4y6Egi0kEqd2lTdvalTJjwGgcPbmTBgo+xd29Z0JFEpANU7nJUGRnnMHHi3zBL5P33z6Sqam7QkUTkOKnc5ZjS0iZw2mnvkZZWzPLll7F+/b10h+/dFZFjU7lLu5KTB3HKKX8hO/tLrF//XVas+BJNTfuCjiUix9DpQyHNrBCY02poBHA30B/4f0B1ePxb7v5qpxNKt5CQkMLJJ/+S1NQxrFv3HWprFzN27FxSU4uCjiYibej0lru7r3L3YncvBk4D9gPzwosfOrRMxR4/zIxhw77FhAl/pKFhGwsWTKKqak77K4rICRet3TLnAhXuviFKzyfd2IAB/8Jpp71Paup4li+fTnn512luPhh0LBFpJVrlPh14vtX9m8xssZk9ZWYZba1gZteaWamZlVZXV7f1EOnGUlLyKC7+X/LybmHz5p9RVjaJ2trFQccSkbCIy93MkoGLgN+Eh2YBI4FiYCvwQFvrufsT7l7i7iVZWVmRxpAAhEJJjBr1EOPH/w/19VWUlU1i48b7cW8OOppIjxeNLffzgQXuvg3A3be5e5O3/B/+c2ByFH6GdGOZmVOZNGkJmZlTWbv2DhYtOpcDB9YFHUukR4tGuX+BVrtkzCyn1bLPAkuj8DOkm0tOzmLs2JcpLHyKvXtLmT9/LBs33k9zc2PQ0UR6pIjK3cz6AP8CvNxq+D/MbImZLQbOAW6N5GdI7DAzcnKuYtKk5WRk/Atr197BggWT2LNnftDRRHqciMrd3fe7e6a77241doW7j3f3Ce5+kbtvjTymxJKUlCGMG/dbxo59ifr6KhYsOJ3Vq2+ioaEm6GgiPYY+oSpdwszIyrqEyZOXk5t7A1u2zObdd0exadNDNDfXBx1PJO6p3KVLJSb2o6DgZ0yatIi+fU+nouIbzJ8/lurqeTpHjUgXUrnLCZGaOpYJE/7A+PF/wCyZZcsuoayshO3bf6+SF+kCKnc5oTIzp1BSsoiiomdobNzF0qUXsWDBZGpqXlXJi0SRyl1OuFAokcGDZzB58koKC5+koWE7S5ZcQGnpKWzd+oxOZSASBSp3CUwolEROzleYPHkVhYVPA86qVVfxzjvD2bDh36iv12kpRDpL5S6BC4WSycm5kpKSxUyY8EdSUyewbt23efvtXJYt+zw7dvxZpzQQ6aBOn89dJNrMjAEDzmPAgPPYt28ZW7b8nG3bfkl19YukpIwgJ+crZGd/gd69RwQdVaTbs+7wJlZJSYmXlpYGHUO6oaamOrZvn8fWrT9n1643AUhPn0x29nSysi4jJSUv4IQiwTGzMncvaXOZyl1iRV3dBqqqXqSqag61tWUA9O17BpmZF5KZeSGpqWMxs4BTipw4KneJO/v3l1NV9QLbt8+jtvZ9AHr1Gkpm5gUMGHA+/fqdSVJS/4BTinQtlbvEtYMHt1BT8yo1Nf/Nzp1/prl5HxAiLW0i/fufTf/+Z9Ov3ydU9hJ3VO7SYzQ3H2TPnnfYteuv7Nz5Jnv2vI17PWD06VNIevpk+vadTHr6ZNLSJhAK9Qo6skinqdylx2pqOsCePe+ye/f/sXfvfPbseZeGhioAzJJJTR1Haup4UlPHkZbWcp2cfJL23UtMOFa561BIiWsJCb3JyDibjIyzAXB3Dh7cxJ4977F377vU1i5i584/sm3bs4fXSUzMoE+fMfTuPeqIy0iSktr8SmCRbkflLj2KmZGSMpSUlKFkZ196eLy+fjv79y9j376l1NYuYf/+Fezc+ecPlT5AYuIAUlLySUkZQq9eeR+5JCfnkpCQcqKnJfIRKncRIDl5IMnJn6R//09+aLypaT91des4cGBNq0vL/V27/kpj466PPFdiYn+SkrJISsoiOTk7fDub5OSW65b7GSQm9ichoR+Jif0IhZJO1FSlh1C5ixxDQkIfUlPHkpo6ts3ljY211Ndv5uDBSg4erKSubhMNDVXU11fR0FDNgQNr2L37bRoaqoGjn0IhFEolMbE/iYn9wtf9w+WfTkJC6uFLKNSn1e3UI5Yduk4hFOqFWZLeO+jBVO4iEUhMTCMxsZA+fQqP+Tj3Zhobd1JfX01DQxWNjbvCl92tbn8wVl+/jf37V9LUVEtT077w4Z0dZ9aLUKjX4cJvKf2jjaUQCiVjlnjEJemY90OhYy83SwBCmIWAhPB16EPXH37MkdedWefQabMs/Avug8sHv/COHDtyeWxTuYucAGYhkpIySUrKBIo6vL6709x8gKamfYfL/tDtI+83Nx/E/SDNzYcudccca2ioPTzm3oB7Y/jS+nYjzc0NQFPUX5vure1fAIeWdfQXx6F1W48NHPhZCgtnRz15ROVuZuuBvbT8ize6e4mZDQDmAMOB9cDl7r4zspgiPZuZkZDQh4SEPkBWYDncHfemNsv/6Pebgebweodut74+2njn1wEPf/nLB5cP3+cYj+EY6/nh16Htx3DU9Y723OnpbR7JGLFobLmf4+7bW92fCbzh7j8ys5nh+3dG4eeISMDMDLNEWqpDRwV1Z11xPvdpwKHjx54FLu6CnyEiIscQabk78CczKzOza8Njg9x9K0D4OjvCnyEiIh0U6W6ZM9x9i5llA6+b2crjXTH8y+BagKFDh0YYQ0REWotoy93dt4Svq4B5wGRgm5nlAISvq46y7hPuXuLuJVlZwb1BJCISjzpd7maWambph24D5wFLgVeAGeGHzQB+F2lIERHpmEh2ywwC5oWP60wEfu3ur5nZfOBFM7sa2AhcFnlMERHpiE6Xu7uvBU5pY7wGODeSUCIiEpmuOBRSREQCpnIXEYlDKncRkTikchcRiUMqdxGROKRyFxGJQyp3EZE4pHIXEYlDKncRkTikchcRiUMqdxGROKRyFxGJQyp3EZE4pHIXEYlDKncRkTikchcRiUMqdxGROKRyFxGJQyp3EZE4pHIXEYlDKncRkTjU6XI3syFm9qaZrTCzZWZ2c3j8+2a22cwWhi9ToxdXRESOR2IE6zYCt7n7AjNLB8rM7PXwsofc/f7I44mISGd0utzdfSuwNXx7r5mtAHKjFUxERDovKvvczWw4MBF4Nzx0k5ktNrOnzCzjKOtca2alZlZaXV0djRgiIhIWcbmbWRrwEnCLu+8BZgEjgWJatuwfaGs9d3/C3UvcvSQrKyvSGCIi0kpE5W5mSbQU+3Pu/htMB1QAAAUnSURBVDKAu29z9yZ3bwZ+DkyOPKaIiHREJEfLGPAksMLdH2w1ntPqYZ8FlnY+noiIdEYkR8ucAVwBLDGzheGxbwFfMLNiwIH1wFcjSigiIh0WydEyfwOsjUWvdj6OiIhEgz6hKiIShyLZLRO4HTvg9dchIQFCobavo7UsFAKz6F9ERLpCTJf7mjUwfXrQKSIX6S+I1r942nrujtyP1mN6wjoi0TB1KjzQ5gHjkYnpch8/HpYtg+ZmaGr66PXxjh3PsuZmcO++l+bmj74+7h27H63H9IR1RKJlyJCued6YLvfevWHMmKBTiIh0P3pDVUQkDqncRUTikMpdRCQOqdxFROKQyl1EJA6p3EVE4pDKXUQkDqncRUTikHk3+PidmVUDGyJ4ioHA9ijFiQU9bb6gOfcUmnPHDHP3Nr/KrluUe6TMrNTdS4LOcaL0tPmC5txTaM7Ro90yIiJxSOUuIhKH4qXcnwg6wAnW0+YLmnNPoTlHSVzscxcRkQ+Lly13ERFpReUuIhKHYrrczWyKma0yszVmNjPoPNFiZkPM7E0zW2Fmy8zs5vD4ADN73czKw9cZrda5K/w6rDKzTweXvvPMLMHM3jez/w7fj+v5AphZfzOba2Yrw//eH4/neZvZreH/ppea2fNmlhKP8zWzp8ysysyWthrr8DzN7DQzWxJe9ohZB77w0d1j8gIkABXACCAZWASMCTpXlOaWA5wavp0OrAbGAP8BzAyPzwR+HL49Jjz/XkB++HVJCHoenZj3N4BfA/8dvh/X8w3P5VngmvDtZKB/vM4byAXWAb3D918ErozH+QJnAacCS1uNdXiewHvAxwED/gCcf7wZYnnLfTKwxt3Xuns98AIwLeBMUeHuW919Qfj2XmAFLf9jTKOlDAhfXxy+PQ14wd0Puvs6YA0tr0/MMLM84ALgF62G43a+AGbWl5YSeBLA3evdfRfxPe9EoLeZJQJ9gC3E4Xzd/S1gxxHDHZqnmeUAfd39bW9p+v9qtU67Yrncc4FNre5XhsfiipkNByYC7wKD3H0rtPwCALLDD4uH1+Jh4JtA66/6juf5QstfndXA0+HdUb8ws1TidN7uvhm4H9gIbAV2u/ufiNP5tqGj88wN3z5y/LjEcrm3te8pro7rNLM04CXgFnffc6yHtjEWM6+FmV0IVLl72fGu0sZYzMy3lURa/nSf5e4TgX20/Ll+NDE97/A+5mm07Ho4CUg1sy8fa5U2xmJmvh1wtHlGNP9YLvdKYEir+3m0/IkXF8wsiZZif87dXw4Pbwv/qUb4uio8HuuvxRnARWa2npbda/9sZr8ifud7SCVQ6e7vhu/PpaXs43XenwLWuXu1uzcALwP/RPzO90gdnWdl+PaR48cllst9PlBgZvlmlgxMB14JOFNUhN8RfxJY4e4Ptlr0CjAjfHsG8LtW49PNrJeZ5QMFtLwRExPc/S53z3P34bT8O/7F3b9MnM73EHf/B7DJzArDQ+cCy4nfeW8ETjezPuH/xs+l5f2keJ3vkTo0z/Cum71mdnr49frXVuu0L+h3lSN8R3oqLUeSVADfDjpPFOf1CVr+/FoMLAxfpgKZwBtAefh6QKt1vh1+HVbRgXfUu9sFOJsPjpbpCfMtBkrD/9a/BTLied7AD4CVwFLgl7QcIRJ38wWep+V9hQZatsCv7sw8gZLwa1UBPEr4rALHc9HpB0RE4lAs75YREZGjULmLiMQhlbuISBxSuYuIxCGVu4hIHFK5i4jEIZW7iEgc+v9kVv9T7R2zYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1000), model_scratch.list_train_loss, c='y', label='train_loss')  \n",
    "plt.plot(range(1000), model_scratch.list_val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】決定領域の可視化\n",
    "決定領域を可視化してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data[50:150], columns=iris.feature_names)\n",
    "y = pd.DataFrame(iris.target[50:150], columns=['target'])\n",
    "X = X.loc[:, ['petal length (cm)','petal width (cm)']]\n",
    "df = pd.concat([X,y], axis=1)\n",
    "df[df['target'] == 1].iloc[:, 0]\n",
    "\n",
    "iris.feature_names\n",
    "#sepal length (cm)\", \"petal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98400, 2)\n",
      "(240, 410)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "def decision_region(X_train, y_train, model, step=0.01, title='decision region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor', 'virginica']):\n",
    "    \"\"\"\n",
    "    Draw the decision area of the model learned by binary classification with two dimensional \n",
    "    feature quantity.\n",
    "     The color of the background is drawn from the estimated value by the learned model.\n",
    "     The point of the scatter diagram is learning data.\n",
    "\n",
    "     Parameters\n",
    "     ------------------\n",
    "     X_train: ndarray, shape (n_samples, 2)\n",
    "         Characteristic quantity of learning data\n",
    "     y_train: ndarray, shape (n_samples,)\n",
    "         Correct value of learning data\n",
    "     model: object\n",
    "         **** Insert Instantus of the learned model\n",
    "     step: float, (default: 0.1)\n",
    "         Set interval to calculate estimated value\n",
    "     title: str\n",
    "         Give the text of the title of the graph\n",
    "     xlabel, ylabel: str\n",
    "         Give sentences of axis labels\n",
    "     target_names =: list of str\n",
    "         Give a list of legends\n",
    "    \"\"\"\n",
    "    # define Initial setting \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    fig.patch.set_facecolor('skyblue')\n",
    "    fig.patch.set_alpha(1)\n",
    "    ax.patch.set_facecolor('white')\n",
    "    ax.patch.set_alpha(1)\n",
    "    # ax.set_yscale('log') \n",
    "    plt.grid()\n",
    "    \n",
    "    # setting\n",
    "    scatter_color = ['red', 'green']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "\n",
    "    # pred\n",
    "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X_train[:,0])-0.5, np.max(X_train[:,0])+0.5, step), np.arange(np.min(X_train[:,1])-0.5, np.max(X_train[:,1])+0.5, step))\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    print(mesh.shape)\n",
    "    print(mesh_f0.shape)\n",
    "    pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
    "\n",
    "    # plot setting title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    fig.patch.set_facecolor('blue')\n",
    "    fig.patch.set_alpha(0.1)\n",
    "    ax.patch.set_facecolor('white')\n",
    "    ax.patch.set_alpha(1)\n",
    "    plt.contourf(mesh_f0, mesh_f1, pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(mesh_f0, mesh_f1, pred, n_class-1, colors='g', linewidths=5, alpha=0.5)\n",
    "    \n",
    "    for i, target in enumerate(set(y_train)):\n",
    "        plt.scatter(X_train[y_train==target][:, 0], X_train[y_train==target][:, 1], s=150, color=scatter_color[i], label=target_names[i], \n",
    "                    marker='x', alpha=0.7)\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    plt.legend(handles=patches)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "decision_region(X_test,y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
