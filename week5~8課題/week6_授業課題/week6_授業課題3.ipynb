{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_hypothesis(X):\n",
    "    np.random.seed(0)\n",
    "    theta = np.random.rand(1,X.shape[1])\n",
    "    hypothesis = 1/(1 + np.exp(-1 * (np.dot(X, theta.T))))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "logistic_hypothesis(X)\n",
    "\n",
    "ones = np.ones((1,1))\n",
    "theta = np.random.rand(1,X.shape[1])\n",
    "theta = np.insert(theta, 0, ones).reshape(-1,1)\n",
    "\n",
    "(logistic_hypothesis(X) - y).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題2\n",
    "    def _gradient_descent(self, X, error, y):\n",
    "        error = (error - y).reshape(-1,1) # shape(100,1) - shape(100,1)\n",
    "        sum_error = np.sum(error * X, axis=0) #shape(100,5) * (100,1)をsumで縦方向に計算 (1,5)\n",
    "        self.theta[0] = self.theta[0] - self.lr*(sum_error[0]/X.shape[0]) \n",
    "        for i in range(1, X.shape[1]):\n",
    "             self.theta[i] = self.theta[i] - self.lr*(sum_error[i]/X.shape[0] + self.lamda*self.theta[i]/X.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef predict(X):\\n    pred = logistic_hypothesis(X)\\n    pred = np.where(pred < 0.5, 0, 1)\\n    return pred\\n\\npredict(X_train).ravel()\\n'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def predict(X):\n",
    "    pred = logistic_hypothesis(X)\n",
    "    pred = np.where(pred < 0.5, 0, 1)\n",
    "    return pred\n",
    "\n",
    "predict(X_train).ravel()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef predict_proba(X):\\n    pred_0 = 1- logistic_hypothesis(X)\\n    pred_1 = logistic_hypothesis(X)\\n    pred_proba = np.hstack([pred_0, pred_1])\\n    return pred_proba\\n\\npredict_proba(X_test)\\n'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def predict_proba(X):\n",
    "    pred_0 = 1- logistic_hypothesis(X)\n",
    "    pred_1 = logistic_hypothesis(X)\n",
    "    pred_proba = np.hstack([pred_0, pred_1])\n",
    "    return pred_proba\n",
    "\n",
    "predict_proba(X_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】目的関数\n",
    "以下の数式で表されるロジスティック回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "\n",
    "なお、この数式には正則化項が含まれています。\n",
    "\n",
    "\n",
    "＊数式が見切れる場合、DIVERを全画面にして御覧ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.983508244147409"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(X, y,lamda=0.01):\n",
    "    theta = np.random.rand(1,X.shape[1])\n",
    "    y = y.reshape(-1,1)\n",
    "    cross_entropy = (1/X.shape[0]) * np.sum((-y * np.log(logistic_hypothesis(X))) - ((1- y) * np.log(1-logistic_hypothesis(X))))\n",
    "    regularization_term = lamda/2*X.shape[0] * np.sum(theta**2)\n",
    "    cost_loss =  (cross_entropy + regularization_term).tolist()\n",
    "    return cost_loss\n",
    "\n",
    "cost_function(X_test, y_test)\n",
    "#(-y * np.log(logistic_hypothesis(X))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7631021115928553"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証用に調べたが、あまり間違っていなさそう。\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris.data[50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "pred_sklearn = model.predict(X_test)\n",
    "pred_sklearn\n",
    "\n",
    "log_loss(y_test, pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したirisデータセットのvirgicolorとvirginicaの2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合体版\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.lamda = 0.001\n",
    "        self.list_train_loss = []\n",
    "        self.list_val_loss = []\n",
    "        \n",
    "    # 問題１    \n",
    "    def _logistic_hypothesis(self, X):\n",
    "        z = np.dot(X, self.theta)\n",
    "        y_hot = 1/(1 + np.exp(-z))\n",
    "        return y_hot\n",
    "\n",
    "    # 問題2\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        \n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "        \n",
    "        # バイアスをthetaとX, X_valに追加\n",
    "        if self.bias:\n",
    "            # ブロードキャストで特徴量の先頭に１を追加\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            ones = np.ones((1,1))\n",
    "            self.theta = np.insert(self.theta, 0, ones) ###########\n",
    "        \n",
    "        # 訓練データを勾配計算からpredict〜cost_functionを計算\n",
    "        for i in range(self.iter):\n",
    "            error = self._logistic_hypothesis(X) #(75,1) \n",
    "            self._gradient_descent(X, error, y)\n",
    "            #y_pred = self.predict_proba(X)[:,1]\n",
    "            loss, _ = self.cost_function(X, y, error)\n",
    "            self.list_train_loss.append(loss)\n",
    "            \n",
    "            if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "                print(\"{}/{}, train_loss {}\".format(i+1, self.iter, loss))\n",
    "        \n",
    "\n",
    "        # テストデータを勾配計算からpredict〜cost_functionを計算\n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.val_theta = np.random.rand(X_val.shape[1], 1)\n",
    "            \n",
    "            if self.bias:\n",
    "                X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "                val_ones = np.ones((1,1))\n",
    "                self.val_theta = np.insert(self.val_theta, 0, val_ones)\n",
    "            \n",
    "            for i in range(self.iter):\n",
    "                error = self._logistic_hypothesis(X_val) # (25,1)\n",
    "                self._gradient_descent(X_val, error, y_val)\n",
    "                #y_val_pred = self.predict_proba(X_val)[:, 1]\n",
    "                _, val_loss = self.cost_function(X_val, y_val, error)\n",
    "                self.list_val_loss.append(val_loss)\n",
    "        \n",
    "                if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程を出力\n",
    "                    print(\"{}/{}, test_loss {}\".format(i+1, self.iter, val_loss))\n",
    "             \n",
    "            \n",
    "    # 問題3\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\" \n",
    "        if self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)    \n",
    "            \n",
    "        pred = self._logistic_hypothesis(X)\n",
    "        pred = np.where(pred < 0.5, 0, 1)\n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        pred_proba = self._logistic_hypothesis(X)\n",
    "        return pred_proba\n",
    "    \n",
    "    # 問題2\n",
    "    def _gradient_descent(self, X, error, y):\n",
    "        error = (error - y).reshape(-1,1) # shape(100,1) - shape(100,1)\n",
    "        sum_error = np.sum(error * X, axis=0) #shape(100,5) * (100,1)をsumで縦方向に計算 (1,5)\n",
    "        self.theta[0] = self.theta[0] - self.lr*(sum_error[0]/X.shape[0]) \n",
    "        for i in range(1, X.shape[1]):\n",
    "             self.theta[i] = self.theta[i] - self.lr*(sum_error[i]/X.shape[0] + self.lamda*self.theta[i]/X.shape[0]) \n",
    "    \n",
    "    \n",
    "    # 問題4\n",
    "    def cost_function(self, X,y, y_pred):\n",
    "        y = y.reshape(-1,1)\n",
    "        cross_entropy = 1/X.shape[0] * np.sum(-y * np.log(y_pred) - (1- y) * np.log(1-y_pred))\n",
    "        regularization_term = self.lamda * np.sum(self.theta**2) / 2*X.shape[0]\n",
    "        cost_loss =  cross_entropy + regularization_term\n",
    "        self.loss = cost_loss\n",
    "        self.val_loss = cost_loss\n",
    "        return self.loss, self.val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(0,5).reshape(1,5)\n",
    "a[0] * 0.01\n",
    "theta[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "[[ 15  18  21]\n",
      " [ 42  54  66]\n",
      " [ 69  90 111]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.rand(1,X.shape[1])\n",
    "theta\n",
    "\n",
    "a = np.arange(0,9).reshape(3,3)\n",
    "b = np.arange(0,9).reshape(3,3)\n",
    "\n",
    "print(np.sum(a*b))\n",
    "print(a@b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000, train_loss 491.05383923947323\n",
      "2/1000, train_loss 489.7140161339745\n",
      "3/1000, train_loss 488.37419529285614\n",
      "4/1000, train_loss 487.0343767389592\n",
      "5/1000, train_loss 485.6945605143414\n",
      "6/1000, train_loss 484.3547466488428\n",
      "7/1000, train_loss 483.0149351744716\n",
      "8/1000, train_loss 481.67512612822475\n",
      "9/1000, train_loss 480.3353195484127\n",
      "10/1000, train_loss 478.99551547145376\n",
      "11/1000, train_loss 477.65571393786513\n",
      "12/1000, train_loss 476.3159149879967\n",
      "13/1000, train_loss 474.9761186638255\n",
      "14/1000, train_loss 473.63632500786474\n",
      "15/1000, train_loss 472.296534063985\n",
      "16/1000, train_loss 470.95674587906535\n",
      "17/1000, train_loss 469.6169604997485\n",
      "18/1000, train_loss 468.27717797569215\n",
      "19/1000, train_loss 466.937398356697\n",
      "20/1000, train_loss 465.5976216944822\n",
      "21/1000, train_loss 464.2578480434335\n",
      "22/1000, train_loss 462.91807745903714\n",
      "23/1000, train_loss 461.57830999905644\n",
      "24/1000, train_loss 460.2385457194327\n",
      "25/1000, train_loss 458.8987846853881\n",
      "26/1000, train_loss 457.55902695535633\n",
      "27/1000, train_loss 456.2192725981169\n",
      "28/1000, train_loss 454.8795216782599\n",
      "29/1000, train_loss 453.53977426532373\n",
      "30/1000, train_loss 452.20003043098416\n",
      "31/1000, train_loss 450.86029024869595\n",
      "32/1000, train_loss 449.5205537935957\n",
      "33/1000, train_loss 448.18082114523577\n",
      "34/1000, train_loss 446.84109238331115\n",
      "35/1000, train_loss 445.5013675924939\n",
      "36/1000, train_loss 444.1616468577766\n",
      "37/1000, train_loss 442.8219302691279\n",
      "38/1000, train_loss 441.48221791743623\n",
      "39/1000, train_loss 440.14250989748734\n",
      "40/1000, train_loss 438.8028063074257\n",
      "41/1000, train_loss 437.46310724816345\n",
      "42/1000, train_loss 436.12341282398484\n",
      "43/1000, train_loss 434.7837231418721\n",
      "44/1000, train_loss 433.44403831305675\n",
      "45/1000, train_loss 432.1043584520208\n",
      "46/1000, train_loss 430.764683677187\n",
      "47/1000, train_loss 429.4250141106555\n",
      "48/1000, train_loss 428.08534987794303\n",
      "49/1000, train_loss 426.7456911095927\n",
      "50/1000, train_loss 425.40603793988197\n",
      "51/1000, train_loss 424.0663905072547\n",
      "52/1000, train_loss 422.72674895446056\n",
      "53/1000, train_loss 421.38711342967343\n",
      "54/1000, train_loss 420.04748408517395\n",
      "55/1000, train_loss 418.7078610784779\n",
      "56/1000, train_loss 417.368244571906\n",
      "57/1000, train_loss 416.0286347334273\n",
      "58/1000, train_loss 414.68903173632657\n",
      "59/1000, train_loss 413.349435759076\n",
      "60/1000, train_loss 412.0098469866195\n",
      "61/1000, train_loss 410.67026560917003\n",
      "62/1000, train_loss 409.3306918239627\n",
      "63/1000, train_loss 407.991125833803\n",
      "64/1000, train_loss 406.65156784875916\n",
      "65/1000, train_loss 405.31201808530227\n",
      "66/1000, train_loss 403.97247676680036\n",
      "67/1000, train_loss 402.63294412454485\n",
      "68/1000, train_loss 401.293420396771\n",
      "69/1000, train_loss 399.9539058295724\n",
      "70/1000, train_loss 398.61440067713835\n",
      "71/1000, train_loss 397.27490520201854\n",
      "72/1000, train_loss 395.93541967518604\n",
      "73/1000, train_loss 394.5959443761988\n",
      "74/1000, train_loss 393.256479594308\n",
      "75/1000, train_loss 391.9170256277074\n",
      "76/1000, train_loss 390.5775827846132\n",
      "77/1000, train_loss 389.2381513831249\n",
      "78/1000, train_loss 387.89873175168634\n",
      "79/1000, train_loss 386.5593242297252\n",
      "80/1000, train_loss 385.21992916751185\n",
      "81/1000, train_loss 383.88054692674945\n",
      "82/1000, train_loss 382.54117788117105\n",
      "83/1000, train_loss 381.20182241669477\n",
      "84/1000, train_loss 379.86248093170644\n",
      "85/1000, train_loss 378.523153837707\n",
      "86/1000, train_loss 377.18384155980675\n",
      "87/1000, train_loss 375.84454453681496\n",
      "88/1000, train_loss 374.50526322210186\n",
      "89/1000, train_loss 373.16599808376793\n",
      "90/1000, train_loss 371.8267496053733\n",
      "91/1000, train_loss 370.4875182862775\n",
      "92/1000, train_loss 369.14830464237264\n",
      "93/1000, train_loss 367.8091092061996\n",
      "94/1000, train_loss 366.4699325280132\n",
      "95/1000, train_loss 365.13077517608815\n",
      "96/1000, train_loss 363.79163773741413\n",
      "97/1000, train_loss 362.45252081820297\n",
      "98/1000, train_loss 361.11342504465995\n",
      "99/1000, train_loss 359.7743510636092\n",
      "100/1000, train_loss 358.4352995431018\n",
      "101/1000, train_loss 357.09627117322606\n",
      "102/1000, train_loss 355.75726666684886\n",
      "103/1000, train_loss 354.41828676019315\n",
      "104/1000, train_loss 353.07933221390795\n",
      "105/1000, train_loss 351.74040381358355\n",
      "106/1000, train_loss 350.4015023707981\n",
      "107/1000, train_loss 349.06262872392836\n",
      "108/1000, train_loss 347.72378373891706\n",
      "109/1000, train_loss 346.3849683104165\n",
      "110/1000, train_loss 345.04618336259\n",
      "111/1000, train_loss 343.70742985011805\n",
      "112/1000, train_loss 342.3687087591895\n",
      "113/1000, train_loss 341.03002110873024\n",
      "114/1000, train_loss 339.6913679512251\n",
      "115/1000, train_loss 338.35275037403534\n",
      "116/1000, train_loss 337.0141695005265\n",
      "117/1000, train_loss 335.6756264912196\n",
      "118/1000, train_loss 334.33712254507714\n",
      "119/1000, train_loss 332.99865890077217\n",
      "120/1000, train_loss 331.66023683803996\n",
      "121/1000, train_loss 330.32185767897784\n",
      "122/1000, train_loss 328.98352278955696\n",
      "123/1000, train_loss 327.645233581068\n",
      "124/1000, train_loss 326.30699151155596\n",
      "125/1000, train_loss 324.9687980875061\n",
      "126/1000, train_loss 323.63065486536317\n",
      "127/1000, train_loss 322.29256345327434\n",
      "128/1000, train_loss 320.9545255127688\n",
      "129/1000, train_loss 319.6165427606033\n",
      "130/1000, train_loss 318.2786169705261\n",
      "131/1000, train_loss 316.94074997528054\n",
      "132/1000, train_loss 315.6029436685182\n",
      "133/1000, train_loss 314.26520000685315\n",
      "134/1000, train_loss 312.9275210119823\n",
      "135/1000, train_loss 311.5899087728498\n",
      "136/1000, train_loss 310.25236544791505\n",
      "137/1000, train_loss 308.9148932674744\n",
      "138/1000, train_loss 307.5774945360602\n",
      "139/1000, train_loss 306.2401716349444\n",
      "140/1000, train_loss 304.9029270246856\n",
      "141/1000, train_loss 303.56576324781446\n",
      "142/1000, train_loss 302.2286829315453\n",
      "143/1000, train_loss 300.89168879065903\n",
      "144/1000, train_loss 299.5547836303724\n",
      "145/1000, train_loss 298.2179703494338\n",
      "146/1000, train_loss 296.88125194319736\n",
      "147/1000, train_loss 295.54463150690384\n",
      "148/1000, train_loss 294.20811223898534\n",
      "149/1000, train_loss 292.8716974445535\n",
      "150/1000, train_loss 291.5353905389377\n",
      "151/1000, train_loss 290.1991950513937\n",
      "152/1000, train_loss 288.8631146289118\n",
      "153/1000, train_loss 287.52715304014276\n",
      "154/1000, train_loss 286.1913141794726\n",
      "155/1000, train_loss 284.85560207123035\n",
      "156/1000, train_loss 283.5200208740183\n",
      "157/1000, train_loss 282.1845748851964\n",
      "158/1000, train_loss 280.8492685455244\n",
      "159/1000, train_loss 279.5141064439427\n",
      "160/1000, train_loss 278.17909332250764\n",
      "161/1000, train_loss 276.8442340815121\n",
      "162/1000, train_loss 275.50953378473963\n",
      "163/1000, train_loss 274.1749976649283\n",
      "164/1000, train_loss 272.8406311293854\n",
      "165/1000, train_loss 271.50643976578624\n",
      "166/1000, train_loss 270.1724293482014\n",
      "167/1000, train_loss 268.8386058432463\n",
      "168/1000, train_loss 267.5049754165206\n",
      "169/1000, train_loss 266.1715444391734\n",
      "170/1000, train_loss 264.8383194947367\n",
      "171/1000, train_loss 263.50530738615686\n",
      "172/1000, train_loss 262.1725151430542\n",
      "173/1000, train_loss 260.83995002922927\n",
      "174/1000, train_loss 259.50761955038075\n",
      "175/1000, train_loss 258.17553146210525\n",
      "176/1000, train_loss 256.8436937781275\n",
      "177/1000, train_loss 255.51211477880997\n",
      "178/1000, train_loss 254.1808030199134\n",
      "179/1000, train_loss 252.84976734164863\n",
      "180/1000, train_loss 251.51901687801708\n",
      "181/1000, train_loss 250.18856106643116\n",
      "182/1000, train_loss 248.85840965764186\n",
      "183/1000, train_loss 247.52857272598607\n",
      "184/1000, train_loss 246.19906067993645\n",
      "185/1000, train_loss 244.86988427298917\n",
      "186/1000, train_loss 243.5410546148867\n",
      "187/1000, train_loss 242.21258318317956\n",
      "188/1000, train_loss 240.88448183514504\n",
      "189/1000, train_loss 239.55676282007693\n",
      "190/1000, train_loss 238.22943879193147\n",
      "191/1000, train_loss 236.90252282236338\n",
      "192/1000, train_loss 235.57602841415897\n",
      "193/1000, train_loss 234.24996951505793\n",
      "194/1000, train_loss 232.92436053199145\n",
      "195/1000, train_loss 231.59921634573652\n",
      "196/1000, train_loss 230.27455232600636\n",
      "197/1000, train_loss 228.95038434696636\n",
      "198/1000, train_loss 227.62672880321225\n",
      "199/1000, train_loss 226.3036026261968\n",
      "200/1000, train_loss 224.98102330112542\n",
      "201/1000, train_loss 223.65900888433166\n",
      "202/1000, train_loss 222.3375780211324\n",
      "203/1000, train_loss 221.01674996418242\n",
      "204/1000, train_loss 219.69654459233297\n",
      "205/1000, train_loss 218.37698242999707\n",
      "206/1000, train_loss 217.05808466704175\n",
      "207/1000, train_loss 215.73987317920518\n",
      "208/1000, train_loss 214.42237054905098\n",
      "209/1000, train_loss 213.1056000874662\n",
      "210/1000, train_loss 211.78958585570976\n",
      "211/1000, train_loss 210.47435268801894\n",
      "212/1000, train_loss 209.1599262147744\n",
      "213/1000, train_loss 207.84633288623624\n",
      "214/1000, train_loss 206.53359999685173\n",
      "215/1000, train_loss 205.22175571013437\n",
      "216/1000, train_loss 203.91082908412614\n",
      "217/1000, train_loss 202.60085009743418\n",
      "218/1000, train_loss 201.2918496758522\n",
      "219/1000, train_loss 199.98385971955597\n",
      "220/1000, train_loss 198.6769131308819\n",
      "221/1000, train_loss 197.3710438426748\n",
      "222/1000, train_loss 196.06628684720962\n",
      "223/1000, train_loss 194.7626782256729\n",
      "224/1000, train_loss 193.4602551782026\n",
      "225/1000, train_loss 192.15905605446926\n",
      "226/1000, train_loss 190.85912038479387\n",
      "227/1000, train_loss 189.56048891178344\n",
      "228/1000, train_loss 188.2632036224688\n",
      "229/1000, train_loss 186.96730778092623\n",
      "230/1000, train_loss 185.6728459613619\n",
      "231/1000, train_loss 184.3798640816335\n",
      "232/1000, train_loss 183.08840943718113\n",
      "233/1000, train_loss 181.79853073533968\n",
      "234/1000, train_loss 180.51027812999547\n",
      "235/1000, train_loss 179.22370325655018\n",
      "236/1000, train_loss 177.938859267153\n",
      "237/1000, train_loss 176.6558008661531\n",
      "238/1000, train_loss 175.37458434572207\n",
      "239/1000, train_loss 174.09526762159553\n",
      "240/1000, train_loss 172.81791026887137\n",
      "241/1000, train_loss 171.54257355780388\n",
      "242/1000, train_loss 170.26932048952196\n",
      "243/1000, train_loss 168.99821583159942\n",
      "244/1000, train_loss 167.72932615339678\n",
      "245/1000, train_loss 166.46271986108786\n",
      "246/1000, train_loss 165.19846723228036\n",
      "247/1000, train_loss 163.93664045013236\n",
      "248/1000, train_loss 162.67731363685897\n",
      "249/1000, train_loss 161.4205628865188\n",
      "250/1000, train_loss 160.16646629696197\n",
      "251/1000, train_loss 158.9151040008136\n",
      "252/1000, train_loss 157.66655819536112\n",
      "253/1000, train_loss 156.42091317120398\n",
      "254/1000, train_loss 155.17825533952066\n",
      "255/1000, train_loss 153.93867325779516\n",
      "256/1000, train_loss 152.70225765384276\n",
      "257/1000, train_loss 151.46910144796252\n",
      "258/1000, train_loss 150.23929977304036\n",
      "259/1000, train_loss 149.01294999241554\n",
      "260/1000, train_loss 147.79015171531836\n",
      "261/1000, train_loss 146.57100680967827\n",
      "262/1000, train_loss 145.35561941209434\n",
      "263/1000, train_loss 144.14409593475457\n",
      "264/1000, train_loss 142.93654506908192\n",
      "265/1000, train_loss 141.73307778588025\n",
      "266/1000, train_loss 140.53380733174856\n",
      "267/1000, train_loss 139.33884922152356\n",
      "268/1000, train_loss 138.14832122651083\n",
      "269/1000, train_loss 136.9623433582567\n",
      "270/1000, train_loss 135.7810378476142\n",
      "271/1000, train_loss 134.60452911885136\n",
      "272/1000, train_loss 133.4329437585513\n",
      "273/1000, train_loss 132.26641047905383\n",
      "274/1000, train_loss 131.10506007618923\n",
      "275/1000, train_loss 129.94902538105805\n",
      "276/1000, train_loss 128.7984412056167\n",
      "277/1000, train_loss 127.65344428183255\n",
      "278/1000, train_loss 126.51417319418235\n",
      "279/1000, train_loss 125.38076830527655\n",
      "280/1000, train_loss 124.25337167440311\n",
      "281/1000, train_loss 123.13212696880014\n",
      "282/1000, train_loss 122.01717936748052\n",
      "283/1000, train_loss 120.90867545745103\n",
      "284/1000, train_loss 119.80676312218834\n",
      "285/1000, train_loss 118.71159142225744\n",
      "286/1000, train_loss 117.62331046798246\n",
      "287/1000, train_loss 116.54207128410812\n",
      "288/1000, train_loss 115.46802566641942\n",
      "289/1000, train_loss 114.40132603032008\n",
      "290/1000, train_loss 113.34212525140424\n",
      "291/1000, train_loss 112.29057649809356\n",
      "292/1000, train_loss 111.24683305645114\n",
      "293/1000, train_loss 110.21104814732387\n",
      "294/1000, train_loss 109.18337473600977\n",
      "295/1000, train_loss 108.16396533469042\n",
      "296/1000, train_loss 107.1529717979159\n",
      "297/1000, train_loss 106.15054511147693\n",
      "298/1000, train_loss 105.15683517504854\n",
      "299/1000, train_loss 104.17199057903768\n",
      "300/1000, train_loss 103.19615837611903\n",
      "301/1000, train_loss 102.22948384799072\n",
      "302/1000, train_loss 101.27211026793323\n",
      "303/1000, train_loss 100.32417865980166\n",
      "304/1000, train_loss 99.38582755412988\n",
      "305/1000, train_loss 98.45719274206907\n",
      "306/1000, train_loss 97.53840702792813\n",
      "307/1000, train_loss 96.62959998112207\n",
      "308/1000, train_loss 95.7308976883723\n",
      "309/1000, train_loss 94.8424225070364\n",
      "310/1000, train_loss 93.96429282047315\n",
      "311/1000, train_loss 93.09662279637388\n",
      "312/1000, train_loss 92.23952214900963\n",
      "313/1000, train_loss 91.39309590635796\n",
      "314/1000, train_loss 90.55744418308039\n",
      "315/1000, train_loss 89.73266196032351\n",
      "316/1000, train_loss 88.91883887331119\n",
      "317/1000, train_loss 88.116059007684\n",
      "318/1000, train_loss 87.32440070552329\n",
      "319/1000, train_loss 86.54393638197077\n",
      "320/1000, train_loss 85.77473235332366\n",
      "321/1000, train_loss 85.01684867744386\n",
      "322/1000, train_loss 84.27033900727496\n",
      "323/1000, train_loss 83.53525045820678\n",
      "324/1000, train_loss 82.81162348996932\n",
      "325/1000, train_loss 82.09949180367178\n",
      "326/1000, train_loss 81.39888225453316\n",
      "327/1000, train_loss 80.70981478077482\n",
      "328/1000, train_loss 80.03230234906715\n",
      "329/1000, train_loss 79.36635091683776\n",
      "330/1000, train_loss 78.71195941166509\n",
      "331/1000, train_loss 78.06911972789024\n",
      "332/1000, train_loss 77.43781674049342\n",
      "333/1000, train_loss 76.81802833618903\n",
      "334/1000, train_loss 76.20972546160459\n",
      "335/1000, train_loss 75.61287218831986\n",
      "336/1000, train_loss 75.02742579445489\n",
      "337/1000, train_loss 74.45333686241219\n",
      "338/1000, train_loss 73.89054939229577\n",
      "339/1000, train_loss 73.33900093045408\n",
      "340/1000, train_loss 72.79862271252033\n",
      "341/1000, train_loss 72.2693398202567\n",
      "342/1000, train_loss 71.75107135144772\n",
      "343/1000, train_loss 71.24373060203253\n",
      "344/1000, train_loss 70.74722525961664\n",
      "345/1000, train_loss 70.26145760746294\n",
      "346/1000, train_loss 69.78632473802527\n",
      "347/1000, train_loss 69.32171877506207\n",
      "348/1000, train_loss 68.86752710334524\n",
      "349/1000, train_loss 68.42363260496772\n",
      "350/1000, train_loss 67.98991390124567\n",
      "351/1000, train_loss 67.5662455992131\n",
      "352/1000, train_loss 67.15249854171303\n",
      "353/1000, train_loss 66.74854006010325\n",
      "354/1000, train_loss 66.35423422861456\n",
      "355/1000, train_loss 65.96944211942406\n",
      "356/1000, train_loss 65.59402205753628\n",
      "357/1000, train_loss 65.2278298746001\n",
      "358/1000, train_loss 64.87071916082789\n",
      "359/1000, train_loss 64.52254151422571\n",
      "360/1000, train_loss 64.18314678638964\n",
      "361/1000, train_loss 63.852383324170475\n",
      "362/1000, train_loss 63.53009820655952\n",
      "363/1000, train_loss 63.21613747620008\n",
      "364/1000, train_loss 62.910346364981045\n",
      "365/1000, train_loss 62.61256951322281\n",
      "366/1000, train_loss 62.32265118201857\n",
      "367/1000, train_loss 62.040435458346764\n",
      "368/1000, train_loss 61.76576645262277\n",
      "369/1000, train_loss 61.49848848840908\n",
      "370/1000, train_loss 61.23844628405237\n",
      "371/1000, train_loss 60.98548512606445\n",
      "372/1000, train_loss 60.73945103410948\n",
      "373/1000, train_loss 60.50019091750451\n",
      "374/1000, train_loss 60.267552723181325\n",
      "375/1000, train_loss 60.041385575097394\n",
      "376/1000, train_loss 59.82153990511989\n",
      "377/1000, train_loss 59.60786757544135\n",
      "378/1000, train_loss 59.40022199261633\n",
      "379/1000, train_loss 59.19845821333804\n",
      "380/1000, train_loss 59.002433042099014\n",
      "381/1000, train_loss 58.812005120904146\n",
      "382/1000, train_loss 58.627035011225104\n",
      "383/1000, train_loss 58.44738526840325\n",
      "384/1000, train_loss 58.272920508724454\n",
      "385/1000, train_loss 58.10350746940265\n",
      "386/1000, train_loss 57.93901506172053\n",
      "387/1000, train_loss 57.77931441758466\n",
      "388/1000, train_loss 57.62427892976021\n",
      "389/1000, train_loss 57.473784286055235\n",
      "390/1000, train_loss 57.32770849772838\n",
      "391/1000, train_loss 57.18593192239612\n",
      "392/1000, train_loss 57.048337281715945\n",
      "393/1000, train_loss 56.91480967412098\n",
      "394/1000, train_loss 56.78523658287998\n",
      "395/1000, train_loss 56.65950787975293\n",
      "396/1000, train_loss 56.53751582450871\n",
      "397/1000, train_loss 56.419155060566254\n",
      "398/1000, train_loss 56.30432260701457\n",
      "399/1000, train_loss 56.192917847260794\n",
      "400/1000, train_loss 56.08484251454854\n",
      "401/1000, train_loss 55.98000067458077\n",
      "402/1000, train_loss 55.878298705474265\n",
      "403/1000, train_loss 55.77964527526399\n",
      "404/1000, train_loss 55.68395131716743\n",
      "405/1000, train_loss 55.59113000281055\n",
      "406/1000, train_loss 55.50109671360801\n",
      "407/1000, train_loss 55.41376901048153\n",
      "408/1000, train_loss 55.32906660209192\n",
      "409/1000, train_loss 55.24691131175116\n",
      "410/1000, train_loss 55.16722704317238\n",
      "411/1000, train_loss 55.08993974520707\n",
      "412/1000, train_loss 55.01497737571076\n",
      "413/1000, train_loss 54.94226986466975\n",
      "414/1000, train_loss 54.87174907671373\n",
      "415/1000, train_loss 54.803348773131574\n",
      "416/1000, train_loss 54.737004573499384\n",
      "417/1000, train_loss 54.67265391702365\n",
      "418/1000, train_loss 54.61023602369394\n",
      "419/1000, train_loss 54.549691855334395\n",
      "420/1000, train_loss 54.49096407663519\n",
      "421/1000, train_loss 54.43399701624029\n",
      "422/1000, train_loss 54.37873662796086\n",
      "423/1000, train_loss 54.32513045217874\n",
      "424/1000, train_loss 54.27312757749838\n",
      "425/1000, train_loss 54.222678602701095\n",
      "426/1000, train_loss 54.17373559905023\n",
      "427/1000, train_loss 54.126252072991356\n",
      "428/1000, train_loss 54.0801829292871\n",
      "429/1000, train_loss 54.03548443462269\n",
      "430/1000, train_loss 53.992114181713575\n",
      "431/1000, train_loss 53.950031053943476\n",
      "432/1000, train_loss 53.909195190557746\n",
      "433/1000, train_loss 53.86956795243341\n",
      "434/1000, train_loss 53.83111188844472\n",
      "435/1000, train_loss 53.79379070243982\n",
      "436/1000, train_loss 53.75756922084206\n",
      "437/1000, train_loss 53.722413360886485\n",
      "438/1000, train_loss 53.688290099500385\n",
      "439/1000, train_loss 53.65516744283436\n",
      "440/1000, train_loss 53.62301439644846\n",
      "441/1000, train_loss 53.59180093615646\n",
      "442/1000, train_loss 53.56149797952947\n",
      "443/1000, train_loss 53.532077358058615\n",
      "444/1000, train_loss 53.50351178997537\n",
      "445/1000, train_loss 53.475774853726634\n",
      "446/1000, train_loss 53.448840962100746\n",
      "447/1000, train_loss 53.422685336999365\n",
      "448/1000, train_loss 53.39728398484947\n",
      "449/1000, train_loss 53.37261367264887\n",
      "450/1000, train_loss 53.34865190463754\n",
      "451/1000, train_loss 53.32537689958707\n",
      "452/1000, train_loss 53.30276756869923\n",
      "453/1000, train_loss 53.280803494104724\n",
      "454/1000, train_loss 53.259464907952314\n",
      "455/1000, train_loss 53.238732672078534\n",
      "456/1000, train_loss 53.218588258247394\n",
      "457/1000, train_loss 53.19901372894962\n",
      "458/1000, train_loss 53.17999171875065\n",
      "459/1000, train_loss 53.161505416176006\n",
      "460/1000, train_loss 53.143538546123374\n",
      "461/1000, train_loss 53.12607535278961\n",
      "462/1000, train_loss 53.10910058310163\n",
      "463/1000, train_loss 53.09259947063966\n",
      "464/1000, train_loss 53.07655772004137\n",
      "465/1000, train_loss 53.060961491875716\n",
      "466/1000, train_loss 53.045797387974766\n",
      "467/1000, train_loss 53.03105243721257\n",
      "468/1000, train_loss 53.016714081719606\n",
      "469/1000, train_loss 53.0027701635217\n",
      "470/1000, train_loss 52.98920891159258\n",
      "471/1000, train_loss 52.976018929308836\n",
      "472/1000, train_loss 52.963189182296844\n",
      "473/1000, train_loss 52.950708986660935\n",
      "474/1000, train_loss 52.93856799758223\n",
      "475/1000, train_loss 52.92675619827806\n",
      "476/1000, train_loss 52.91526388931182\n",
      "477/1000, train_loss 52.90408167824307\n",
      "478/1000, train_loss 52.89320046960865\n",
      "479/1000, train_loss 52.88261145522475\n",
      "480/1000, train_loss 52.872306104800856\n",
      "481/1000, train_loss 52.862276156856325\n",
      "482/1000, train_loss 52.85251360993074\n",
      "483/1000, train_loss 52.84301071407902\n",
      "484/1000, train_loss 52.83375996264301\n",
      "485/1000, train_loss 52.82475408429108\n",
      "486/1000, train_loss 52.81598603531745\n",
      "487/1000, train_loss 52.80744899219349\n",
      "488/1000, train_loss 52.79913634436307\n",
      "489/1000, train_loss 52.79104168727447\n",
      "490/1000, train_loss 52.78315881564149\n",
      "491/1000, train_loss 52.77548171692651\n",
      "492/1000, train_loss 52.768004565038495\n",
      "493/1000, train_loss 52.76072171423945\n",
      "494/1000, train_loss 52.75362769325216\n",
      "495/1000, train_loss 52.74671719956334\n",
      "496/1000, train_loss 52.73998509391562\n",
      "497/1000, train_loss 52.73342639498231\n",
      "498/1000, train_loss 52.72703627421933\n",
      "499/1000, train_loss 52.72081005088814\n",
      "500/1000, train_loss 52.71474318724447\n",
      "501/1000, train_loss 52.70883128388738\n",
      "502/1000, train_loss 52.70307007526316\n",
      "503/1000, train_loss 52.697455425319404\n",
      "504/1000, train_loss 52.69198332330409\n",
      "505/1000, train_loss 52.686649879704746\n",
      "506/1000, train_loss 52.68145132232353\n",
      "507/1000, train_loss 52.67638399248325\n",
      "508/1000, train_loss 52.67144434136023\n",
      "509/1000, train_loss 52.66662892643988\n",
      "510/1000, train_loss 52.661934408090566\n",
      "511/1000, train_loss 52.65735754625229\n",
      "512/1000, train_loss 52.6528951972359\n",
      "513/1000, train_loss 52.64854431062937\n",
      "514/1000, train_loss 52.64430192630761\n",
      "515/1000, train_loss 52.640165171542144\n",
      "516/1000, train_loss 52.63613125820745\n",
      "517/1000, train_loss 52.63219748008062\n",
      "518/1000, train_loss 52.62836121023129\n",
      "519/1000, train_loss 52.62461989849861\n",
      "520/1000, train_loss 52.62097106905264\n",
      "521/1000, train_loss 52.61741231803696\n",
      "522/1000, train_loss 52.613941311289935\n",
      "523/1000, train_loss 52.610555782141965\n",
      "524/1000, train_loss 52.607253529286155\n",
      "525/1000, train_loss 52.604032414719846\n",
      "526/1000, train_loss 52.60089036175457\n",
      "527/1000, train_loss 52.597825353092354\n",
      "528/1000, train_loss 52.59483542896572\n",
      "529/1000, train_loss 52.591918685339635\n",
      "530/1000, train_loss 52.58907327217293\n",
      "531/1000, train_loss 52.586297391737496\n",
      "532/1000, train_loss 52.583589296993\n",
      "533/1000, train_loss 52.580947290015466\n",
      "534/1000, train_loss 52.578369720477696\n",
      "535/1000, train_loss 52.57585498417998\n",
      "536/1000, train_loss 52.573401521629094\n",
      "537/1000, train_loss 52.57100781666426\n",
      "538/1000, train_loss 52.568672395128225\n",
      "539/1000, train_loss 52.56639382358202\n",
      "540/1000, train_loss 52.564170708061965\n",
      "541/1000, train_loss 52.56200169287731\n",
      "542/1000, train_loss 52.55988545944738\n",
      "543/1000, train_loss 52.557820725176605\n",
      "544/1000, train_loss 52.55580624236634\n",
      "545/1000, train_loss 52.5538407971622\n",
      "546/1000, train_loss 52.55192320853562\n",
      "547/1000, train_loss 52.550052327298445\n",
      "548/1000, train_loss 52.548227035149694\n",
      "549/1000, train_loss 52.54644624375307\n",
      "550/1000, train_loss 52.54470889384433\n",
      "551/1000, train_loss 52.54301395436756\n",
      "552/1000, train_loss 52.54136042163924\n",
      "553/1000, train_loss 52.539747318539256\n",
      "554/1000, train_loss 52.5381736937279\n",
      "555/1000, train_loss 52.536638620887985\n",
      "556/1000, train_loss 52.535141197991265\n",
      "557/1000, train_loss 52.53368054658829\n",
      "558/1000, train_loss 52.53225581112089\n",
      "559/1000, train_loss 52.53086615825659\n",
      "560/1000, train_loss 52.52951077624419\n",
      "561/1000, train_loss 52.528188874289775\n",
      "562/1000, train_loss 52.52689968195247\n",
      "563/1000, train_loss 52.5256424485593\n",
      "564/1000, train_loss 52.524416442638554\n",
      "565/1000, train_loss 52.523220951370845\n",
      "566/1000, train_loss 52.52205528005759\n",
      "567/1000, train_loss 52.52091875160601\n",
      "568/1000, train_loss 52.51981070603025\n",
      "569/1000, train_loss 52.51873049996816\n",
      "570/1000, train_loss 52.517677506213005\n",
      "571/1000, train_loss 52.5166511132598\n",
      "572/1000, train_loss 52.515650724865615\n",
      "573/1000, train_loss 52.514675759623636\n",
      "574/1000, train_loss 52.51372565055019\n",
      "575/1000, train_loss 52.51279984468455\n",
      "576/1000, train_loss 52.51189780270109\n",
      "577/1000, train_loss 52.511018998533196\n",
      "578/1000, train_loss 52.510162919008806\n",
      "579/1000, train_loss 52.509329063496914\n",
      "580/1000, train_loss 52.50851694356495\n",
      "581/1000, train_loss 52.507726082646435\n",
      "582/1000, train_loss 52.50695601571884\n",
      "583/1000, train_loss 52.50620628899099\n",
      "584/1000, train_loss 52.50547645959996\n",
      "585/1000, train_loss 52.50476609531717\n",
      "586/1000, train_loss 52.504074774263046\n",
      "587/1000, train_loss 52.503402084630416\n",
      "588/1000, train_loss 52.50274762441604\n",
      "589/1000, train_loss 52.5021110011601\n",
      "590/1000, train_loss 52.50149183169349\n",
      "591/1000, train_loss 52.50088974189249\n",
      "592/1000, train_loss 52.500304366440695\n",
      "593/1000, train_loss 52.4997353485979\n",
      "594/1000, train_loss 52.49918233997586\n",
      "595/1000, train_loss 52.498645000320465\n",
      "596/1000, train_loss 52.498122997300385\n",
      "597/1000, train_loss 52.497616006301705\n",
      "598/1000, train_loss 52.497123710228685\n",
      "599/1000, train_loss 52.496645799310095\n",
      "600/1000, train_loss 52.49618197091127\n",
      "601/1000, train_loss 52.49573192935147\n",
      "602/1000, train_loss 52.495295385726536\n",
      "603/1000, train_loss 52.4948720577365\n",
      "604/1000, train_loss 52.494461669518216\n",
      "605/1000, train_loss 52.494063951482694\n",
      "606/1000, train_loss 52.49367864015703\n",
      "607/1000, train_loss 52.49330547803072\n",
      "608/1000, train_loss 52.49294421340651\n",
      "609/1000, train_loss 52.492594600255146\n",
      "610/1000, train_loss 52.49225639807439\n",
      "611/1000, train_loss 52.49192937175189\n",
      "612/1000, train_loss 52.49161329143182\n",
      "613/1000, train_loss 52.49130793238532\n",
      "614/1000, train_loss 52.49101307488435\n",
      "615/1000, train_loss 52.490728504079215\n",
      "616/1000, train_loss 52.49045400987925\n",
      "617/1000, train_loss 52.49018938683693\n",
      "618/1000, train_loss 52.48993443403499\n",
      "619/1000, train_loss 52.48968895497678\n",
      "620/1000, train_loss 52.48945275747941\n",
      "621/1000, train_loss 52.489225653569946\n",
      "622/1000, train_loss 52.48900745938426\n",
      "623/1000, train_loss 52.4887979950687\n",
      "624/1000, train_loss 52.48859708468432\n",
      "625/1000, train_loss 52.488404556113714\n",
      "626/1000, train_loss 52.488220240970264\n",
      "627/1000, train_loss 52.48804397450986\n",
      "628/1000, train_loss 52.487875595544935\n",
      "629/1000, train_loss 52.48771494636068\n",
      "630/1000, train_loss 52.48756187263365\n",
      "631/1000, train_loss 52.487416223352206\n",
      "632/1000, train_loss 52.48727785073937\n",
      "633/1000, train_loss 52.48714661017739\n",
      "634/1000, train_loss 52.48702236013448\n",
      "635/1000, train_loss 52.48690496209329\n",
      "636/1000, train_loss 52.48679428048133\n",
      "637/1000, train_loss 52.48669018260309\n",
      "638/1000, train_loss 52.486592538574016\n",
      "639/1000, train_loss 52.486501221256034\n",
      "640/1000, train_loss 52.48641610619479\n",
      "641/1000, train_loss 52.486337071558474\n",
      "642/1000, train_loss 52.48626399807821\n",
      "643/1000, train_loss 52.48619676898987\n",
      "644/1000, train_loss 52.48613526997744\n",
      "645/1000, train_loss 52.48607938911775\n",
      "646/1000, train_loss 52.486029016826606\n",
      "647/1000, train_loss 52.48598404580625\n",
      "648/1000, train_loss 52.4859443709941\n",
      "649/1000, train_loss 52.48590988951285\n",
      "650/1000, train_loss 52.48588050062165\n",
      "651/1000, train_loss 52.48585610566869\n",
      "652/1000, train_loss 52.485836608044686\n",
      "653/1000, train_loss 52.485821913137755\n",
      "654/1000, train_loss 52.48581192828923\n",
      "655/1000, train_loss 52.485806562750604\n",
      "656/1000, train_loss 52.485805727641505\n",
      "657/1000, train_loss 52.485809335908726\n",
      "658/1000, train_loss 52.485817302286144\n",
      "659/1000, train_loss 52.48582954325571\n",
      "660/1000, train_loss 52.48584597700932\n",
      "661/1000, train_loss 52.48586652341156\n",
      "662/1000, train_loss 52.485891103963446\n",
      "663/1000, train_loss 52.48591964176693\n",
      "664/1000, train_loss 52.4859520614902\n",
      "665/1000, train_loss 52.485988289334045\n",
      "666/1000, train_loss 52.48602825299864\n",
      "667/1000, train_loss 52.48607188165144\n",
      "668/1000, train_loss 52.486119105895675\n",
      "669/1000, train_loss 52.48616985773954\n",
      "670/1000, train_loss 52.486224070566244\n",
      "671/1000, train_loss 52.4862816791046\n",
      "672/1000, train_loss 52.48634261940041\n",
      "673/1000, train_loss 52.48640682878847\n",
      "674/1000, train_loss 52.4864742458652\n",
      "675/1000, train_loss 52.486544810461915\n",
      "676/1000, train_loss 52.48661846361874\n",
      "677/1000, train_loss 52.486695147559075\n",
      "678/1000, train_loss 52.48677480566468\n",
      "679/1000, train_loss 52.48685738245126\n",
      "680/1000, train_loss 52.48694282354476\n",
      "681/1000, train_loss 52.487031075657924\n",
      "682/1000, train_loss 52.487122086567716\n",
      "683/1000, train_loss 52.48721580509297\n",
      "684/1000, train_loss 52.48731218107269\n",
      "685/1000, train_loss 52.48741116534481\n",
      "686/1000, train_loss 52.48751270972535\n",
      "687/1000, train_loss 52.487616766988225\n",
      "688/1000, train_loss 52.487723290845274\n",
      "689/1000, train_loss 52.48783223592686\n",
      "690/1000, train_loss 52.48794355776295\n",
      "691/1000, train_loss 52.4880572127644\n",
      "692/1000, train_loss 52.48817315820499\n",
      "693/1000, train_loss 52.48829135220347\n",
      "694/1000, train_loss 52.488411753706295\n",
      "695/1000, train_loss 52.48853432247063\n",
      "696/1000, train_loss 52.48865901904765\n",
      "697/1000, train_loss 52.488785804766394\n",
      "698/1000, train_loss 52.48891464171776\n",
      "699/1000, train_loss 52.489045492739024\n",
      "700/1000, train_loss 52.48917832139855\n",
      "701/1000, train_loss 52.48931309198092\n",
      "702/1000, train_loss 52.489449769472394\n",
      "703/1000, train_loss 52.489588319546606\n",
      "704/1000, train_loss 52.48972870855063\n",
      "705/1000, train_loss 52.48987090349134\n",
      "706/1000, train_loss 52.49001487202203\n",
      "707/1000, train_loss 52.49016058242934\n",
      "708/1000, train_loss 52.49030800362048\n",
      "709/1000, train_loss 52.49045710511068\n",
      "710/1000, train_loss 52.49060785701096\n",
      "711/1000, train_loss 52.49076023001607\n",
      "712/1000, train_loss 52.49091419539286\n",
      "713/1000, train_loss 52.491069724968696\n",
      "714/1000, train_loss 52.4912267911202\n",
      "715/1000, train_loss 52.49138536676229\n",
      "716/1000, train_loss 52.49154542533738\n",
      "717/1000, train_loss 52.49170694080478\n",
      "718/1000, train_loss 52.491869887630436\n",
      "719/1000, train_loss 52.49203424077673\n",
      "720/1000, train_loss 52.49219997569263\n",
      "721/1000, train_loss 52.49236706830395\n",
      "722/1000, train_loss 52.49253549500388\n",
      "723/1000, train_loss 52.49270523264365\n",
      "724/1000, train_loss 52.49287625852348\n",
      "725/1000, train_loss 52.49304855038359\n",
      "726/1000, train_loss 52.4932220863955\n",
      "727/1000, train_loss 52.493396845153484\n",
      "728/1000, train_loss 52.493572805666155\n",
      "729/1000, train_loss 52.49374994734831\n",
      "730/1000, train_loss 52.49392825001284\n",
      "731/1000, train_loss 52.49410769386291\n",
      "732/1000, train_loss 52.49428825948419\n",
      "733/1000, train_loss 52.49446992783737\n",
      "734/1000, train_loss 52.4946526802507\n",
      "735/1000, train_loss 52.49483649841281\n",
      "736/1000, train_loss 52.495021364365535\n",
      "737/1000, train_loss 52.495207260497\n",
      "738/1000, train_loss 52.49539416953485\n",
      "739/1000, train_loss 52.495582074539485\n",
      "740/1000, train_loss 52.49577095889763\n",
      "741/1000, train_loss 52.49596080631582\n",
      "742/1000, train_loss 52.4961516008142\n",
      "743/1000, train_loss 52.49634332672037\n",
      "744/1000, train_loss 52.49653596866331\n",
      "745/1000, train_loss 52.496729511567565\n",
      "746/1000, train_loss 52.496923940647335\n",
      "747/1000, train_loss 52.49711924140096\n",
      "748/1000, train_loss 52.497315399605256\n",
      "749/1000, train_loss 52.49751240131012\n",
      "750/1000, train_loss 52.497710232833185\n",
      "751/1000, train_loss 52.49790888075459\n",
      "752/1000, train_loss 52.4981083319119\n",
      "753/1000, train_loss 52.49830857339502\n",
      "754/1000, train_loss 52.49850959254136\n",
      "755/1000, train_loss 52.49871137693093\n",
      "756/1000, train_loss 52.498913914381696\n",
      "757/1000, train_loss 52.49911719294491\n",
      "758/1000, train_loss 52.49932120090057\n",
      "759/1000, train_loss 52.499525926753\n",
      "760/1000, train_loss 52.49973135922649\n",
      "761/1000, train_loss 52.49993748726099\n",
      "762/1000, train_loss 52.50014430000799\n",
      "763/1000, train_loss 52.50035178682633\n",
      "764/1000, train_loss 52.50055993727826\n",
      "765/1000, train_loss 52.500768741125476\n",
      "766/1000, train_loss 52.500978188325234\n",
      "767/1000, train_loss 52.501188269026564\n",
      "768/1000, train_loss 52.501398973566594\n",
      "769/1000, train_loss 52.50161029246687\n",
      "770/1000, train_loss 52.50182221642984\n",
      "771/1000, train_loss 52.50203473633529\n",
      "772/1000, train_loss 52.50224784323695\n",
      "773/1000, train_loss 52.502461528359206\n",
      "774/1000, train_loss 52.50267578309365\n",
      "775/1000, train_loss 52.50289059899597\n",
      "776/1000, train_loss 52.50310596778279\n",
      "777/1000, train_loss 52.50332188132846\n",
      "778/1000, train_loss 52.503538331662135\n",
      "779/1000, train_loss 52.503755310964735\n",
      "780/1000, train_loss 52.50397281156601\n",
      "781/1000, train_loss 52.5041908259417\n",
      "782/1000, train_loss 52.504409346710695\n",
      "783/1000, train_loss 52.504628366632325\n",
      "784/1000, train_loss 52.504847878603606\n",
      "785/1000, train_loss 52.50506787565664\n",
      "786/1000, train_loss 52.50528835095597\n",
      "787/1000, train_loss 52.50550929779608\n",
      "788/1000, train_loss 52.50573070959886\n",
      "789/1000, train_loss 52.50595257991115\n",
      "790/1000, train_loss 52.50617490240243\n",
      "791/1000, train_loss 52.506397670862306\n",
      "792/1000, train_loss 52.50662087919837\n",
      "793/1000, train_loss 52.50684452143384\n",
      "794/1000, train_loss 52.50706859170534\n",
      "795/1000, train_loss 52.50729308426078\n",
      "796/1000, train_loss 52.50751799345715\n",
      "797/1000, train_loss 52.5077433137585\n",
      "798/1000, train_loss 52.50796903973385\n",
      "799/1000, train_loss 52.50819516605516\n",
      "800/1000, train_loss 52.50842168749541\n",
      "801/1000, train_loss 52.50864859892662\n",
      "802/1000, train_loss 52.50887589531797\n",
      "803/1000, train_loss 52.50910357173395\n",
      "804/1000, train_loss 52.509331623332514\n",
      "805/1000, train_loss 52.5095600453633\n",
      "806/1000, train_loss 52.50978883316589\n",
      "807/1000, train_loss 52.510017982168065\n",
      "808/1000, train_loss 52.51024748788414\n",
      "809/1000, train_loss 52.510477345913294\n",
      "810/1000, train_loss 52.510707551937955\n",
      "811/1000, train_loss 52.510938101722196\n",
      "812/1000, train_loss 52.51116899111022\n",
      "813/1000, train_loss 52.511400216024754\n",
      "814/1000, train_loss 52.511631772465606\n",
      "815/1000, train_loss 52.51186365650817\n",
      "816/1000, train_loss 52.512095864302026\n",
      "817/1000, train_loss 52.51232839206942\n",
      "818/1000, train_loss 52.512561236104006\n",
      "819/1000, train_loss 52.512794392769386\n",
      "820/1000, train_loss 52.51302785849781\n",
      "821/1000, train_loss 52.513261629788886\n",
      "822/1000, train_loss 52.51349570320825\n",
      "823/1000, train_loss 52.513730075386356\n",
      "824/1000, train_loss 52.513964743017176\n",
      "825/1000, train_loss 52.514199702857084\n",
      "826/1000, train_loss 52.514434951723565\n",
      "827/1000, train_loss 52.51467048649411\n",
      "828/1000, train_loss 52.514906304105054\n",
      "829/1000, train_loss 52.51514240155048\n",
      "830/1000, train_loss 52.51537877588105\n",
      "831/1000, train_loss 52.51561542420302\n",
      "832/1000, train_loss 52.51585234367711\n",
      "833/1000, train_loss 52.516089531517494\n",
      "834/1000, train_loss 52.51632698499079\n",
      "835/1000, train_loss 52.516564701415035\n",
      "836/1000, train_loss 52.516802678158726\n",
      "837/1000, train_loss 52.51704091263987\n",
      "838/1000, train_loss 52.517279402325\n",
      "839/1000, train_loss 52.517518144728314\n",
      "840/1000, train_loss 52.51775713741068\n",
      "841/1000, train_loss 52.51799637797886\n",
      "842/1000, train_loss 52.51823586408452\n",
      "843/1000, train_loss 52.51847559342347\n",
      "844/1000, train_loss 52.51871556373478\n",
      "845/1000, train_loss 52.51895577279995\n",
      "846/1000, train_loss 52.51919621844212\n",
      "847/1000, train_loss 52.519436898525285\n",
      "848/1000, train_loss 52.51967781095352\n",
      "849/1000, train_loss 52.51991895367017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850/1000, train_loss 52.52016032465716\n",
      "851/1000, train_loss 52.52040192193424\n",
      "852/1000, train_loss 52.52064374355827\n",
      "853/1000, train_loss 52.52088578762252\n",
      "854/1000, train_loss 52.52112805225594\n",
      "855/1000, train_loss 52.52137053562253\n",
      "856/1000, train_loss 52.52161323592069\n",
      "857/1000, train_loss 52.52185615138245\n",
      "858/1000, train_loss 52.52209928027301\n",
      "859/1000, train_loss 52.52234262088994\n",
      "860/1000, train_loss 52.522586171562665\n",
      "861/1000, train_loss 52.52282993065186\n",
      "862/1000, train_loss 52.52307389654878\n",
      "863/1000, train_loss 52.52331806767475\n",
      "864/1000, train_loss 52.52356244248062\n",
      "865/1000, train_loss 52.52380701944607\n",
      "866/1000, train_loss 52.524051797079224\n",
      "867/1000, train_loss 52.52429677391601\n",
      "868/1000, train_loss 52.52454194851964\n",
      "869/1000, train_loss 52.524787319480154\n",
      "870/1000, train_loss 52.52503288541384\n",
      "871/1000, train_loss 52.525278644962775\n",
      "872/1000, train_loss 52.525524596794305\n",
      "873/1000, train_loss 52.52577073960062\n",
      "874/1000, train_loss 52.5260170720982\n",
      "875/1000, train_loss 52.52626359302745\n",
      "876/1000, train_loss 52.52651030115217\n",
      "877/1000, train_loss 52.52675719525915\n",
      "878/1000, train_loss 52.52700427415773\n",
      "879/1000, train_loss 52.527251536679344\n",
      "880/1000, train_loss 52.52749898167712\n",
      "881/1000, train_loss 52.52774660802551\n",
      "882/1000, train_loss 52.52799441461981\n",
      "883/1000, train_loss 52.52824240037581\n",
      "884/1000, train_loss 52.528490564229415\n",
      "885/1000, train_loss 52.52873890513621\n",
      "886/1000, train_loss 52.528987422071154\n",
      "887/1000, train_loss 52.52923611402816\n",
      "888/1000, train_loss 52.52948498001977\n",
      "889/1000, train_loss 52.529734019076784\n",
      "890/1000, train_loss 52.52998323024793\n",
      "891/1000, train_loss 52.530232612599484\n",
      "892/1000, train_loss 52.53048216521498\n",
      "893/1000, train_loss 52.53073188719488\n",
      "894/1000, train_loss 52.53098177765624\n",
      "895/1000, train_loss 52.53123183573237\n",
      "896/1000, train_loss 52.53148206057259\n",
      "897/1000, train_loss 52.531732451341874\n",
      "898/1000, train_loss 52.53198300722056\n",
      "899/1000, train_loss 52.53223372740409\n",
      "900/1000, train_loss 52.53248461110268\n",
      "901/1000, train_loss 52.53273565754107\n",
      "902/1000, train_loss 52.53298686595824\n",
      "903/1000, train_loss 52.533238235607165\n",
      "904/1000, train_loss 52.53348976575449\n",
      "905/1000, train_loss 52.53374145568032\n",
      "906/1000, train_loss 52.53399330467796\n",
      "907/1000, train_loss 52.53424531205367\n",
      "908/1000, train_loss 52.53449747712635\n",
      "909/1000, train_loss 52.534749799227434\n",
      "910/1000, train_loss 52.53500227770053\n",
      "911/1000, train_loss 52.53525491190122\n",
      "912/1000, train_loss 52.535507701196906\n",
      "913/1000, train_loss 52.535760644966466\n",
      "914/1000, train_loss 52.53601374260016\n",
      "915/1000, train_loss 52.53626699349931\n",
      "916/1000, train_loss 52.53652039707615\n",
      "917/1000, train_loss 52.5367739527536\n",
      "918/1000, train_loss 52.53702765996508\n",
      "919/1000, train_loss 52.537281518154266\n",
      "920/1000, train_loss 52.53753552677498\n",
      "921/1000, train_loss 52.53778968529092\n",
      "922/1000, train_loss 52.538043993175485\n",
      "923/1000, train_loss 52.538298449911615\n",
      "924/1000, train_loss 52.53855305499161\n",
      "925/1000, train_loss 52.538807807916946\n",
      "926/1000, train_loss 52.53906270819812\n",
      "927/1000, train_loss 52.53931775535439\n",
      "928/1000, train_loss 52.539572948913744\n",
      "929/1000, train_loss 52.53982828841265\n",
      "930/1000, train_loss 52.5400837733959\n",
      "931/1000, train_loss 52.540339403416496\n",
      "932/1000, train_loss 52.54059517803543\n",
      "933/1000, train_loss 52.5408510968216\n",
      "934/1000, train_loss 52.541107159351604\n",
      "935/1000, train_loss 52.541363365209634\n",
      "936/1000, train_loss 52.541619713987316\n",
      "937/1000, train_loss 52.54187620528357\n",
      "938/1000, train_loss 52.54213283870446\n",
      "939/1000, train_loss 52.5423896138631\n",
      "940/1000, train_loss 52.54264653037947\n",
      "941/1000, train_loss 52.5429035878803\n",
      "942/1000, train_loss 52.54316078599897\n",
      "943/1000, train_loss 52.543418124375364\n",
      "944/1000, train_loss 52.543675602655746\n",
      "945/1000, train_loss 52.54393322049263\n",
      "946/1000, train_loss 52.54419097754467\n",
      "947/1000, train_loss 52.544448873476576\n",
      "948/1000, train_loss 52.54470690795894\n",
      "949/1000, train_loss 52.54496508066818\n",
      "950/1000, train_loss 52.54522339128636\n",
      "951/1000, train_loss 52.54548183950118\n",
      "952/1000, train_loss 52.5457404250058\n",
      "953/1000, train_loss 52.54599914749873\n",
      "954/1000, train_loss 52.54625800668381\n",
      "955/1000, train_loss 52.546517002269965\n",
      "956/1000, train_loss 52.546776133971285\n",
      "957/1000, train_loss 52.54703540150679\n",
      "958/1000, train_loss 52.547294804600384\n",
      "959/1000, train_loss 52.547554342980774\n",
      "960/1000, train_loss 52.54781401638139\n",
      "961/1000, train_loss 52.548073824540225\n",
      "962/1000, train_loss 52.54833376719984\n",
      "963/1000, train_loss 52.548593844107224\n",
      "964/1000, train_loss 52.548854055013734\n",
      "965/1000, train_loss 52.54911439967497\n",
      "966/1000, train_loss 52.54937487785073\n",
      "967/1000, train_loss 52.54963548930497\n",
      "968/1000, train_loss 52.54989623380564\n",
      "969/1000, train_loss 52.55015711112465\n",
      "970/1000, train_loss 52.550418121037815\n",
      "971/1000, train_loss 52.55067926332476\n",
      "972/1000, train_loss 52.55094053776882\n",
      "973/1000, train_loss 52.55120194415704\n",
      "974/1000, train_loss 52.551463482280035\n",
      "975/1000, train_loss 52.551725151931976\n",
      "976/1000, train_loss 52.55198695291047\n",
      "977/1000, train_loss 52.55224888501654\n",
      "978/1000, train_loss 52.55251094805455\n",
      "979/1000, train_loss 52.55277314183211\n",
      "980/1000, train_loss 52.55303546616007\n",
      "981/1000, train_loss 52.55329792085241\n",
      "982/1000, train_loss 52.553560505726225\n",
      "983/1000, train_loss 52.5538232206016\n",
      "984/1000, train_loss 52.55408606530164\n",
      "985/1000, train_loss 52.55434903965232\n",
      "986/1000, train_loss 52.55461214348253\n",
      "987/1000, train_loss 52.55487537662393\n",
      "988/1000, train_loss 52.55513873891094\n",
      "989/1000, train_loss 52.55540223018071\n",
      "990/1000, train_loss 52.55566585027302\n",
      "991/1000, train_loss 52.55592959903026\n",
      "992/1000, train_loss 52.55619347629735\n",
      "993/1000, train_loss 52.55645748192177\n",
      "994/1000, train_loss 52.556721615753396\n",
      "995/1000, train_loss 52.556985877644564\n",
      "996/1000, train_loss 52.55725026744994\n",
      "997/1000, train_loss 52.55751478502653\n",
      "998/1000, train_loss 52.55777943023362\n",
      "999/1000, train_loss 52.55804420293272\n",
      "1000/1000, train_loss 52.55830910298754\n",
      "1/1000, test_loss 17.433291404076268\n",
      "2/1000, test_loss 17.43381270306553\n",
      "3/1000, test_loss 17.434370878582698\n",
      "4/1000, test_loss 17.434963562292403\n",
      "5/1000, test_loss 17.435588493487472\n",
      "6/1000, test_loss 17.436243514684556\n",
      "7/1000, test_loss 17.436926567391186\n",
      "8/1000, test_loss 17.43763568803805\n",
      "9/1000, test_loss 17.438369004070196\n",
      "10/1000, test_loss 17.439124730191267\n",
      "11/1000, test_loss 17.43990116475505\n",
      "12/1000, test_loss 17.440696686298732\n",
      "13/1000, test_loss 17.44150975021257\n",
      "14/1000, test_loss 17.44233888554081\n",
      "15/1000, test_loss 17.44318269190882\n",
      "16/1000, test_loss 17.444039836571744\n",
      "17/1000, test_loss 17.444909051579902\n",
      "18/1000, test_loss 17.445789131056646\n",
      "19/1000, test_loss 17.44667892858419\n",
      "20/1000, test_loss 17.447577354693465\n",
      "21/1000, test_loss 17.448483374453804\n",
      "22/1000, test_loss 17.449396005158757\n",
      "23/1000, test_loss 17.450314314104332\n",
      "24/1000, test_loss 17.451237416455964\n",
      "25/1000, test_loss 17.452164473200895\n",
      "26/1000, test_loss 17.453094689182684\n",
      "27/1000, test_loss 17.454027311214524\n",
      "28/1000, test_loss 17.454961626268396\n",
      "29/1000, test_loss 17.455896959737174\n",
      "30/1000, test_loss 17.456832673766655\n",
      "31/1000, test_loss 17.45776816565501\n",
      "32/1000, test_loss 17.458702866316816\n",
      "33/1000, test_loss 17.459636238809303\n",
      "34/1000, test_loss 17.4605677769183\n",
      "35/1000, test_loss 17.46149700380158\n",
      "36/1000, test_loss 17.46242347068728\n",
      "37/1000, test_loss 17.463346755625373\n",
      "38/1000, test_loss 17.464266462289977\n",
      "39/1000, test_loss 17.46518221883054\n",
      "40/1000, test_loss 17.466093676770026\n",
      "41/1000, test_loss 17.46700050994818\n",
      "42/1000, test_loss 17.46790241350814\n",
      "43/1000, test_loss 17.46879910292468\n",
      "44/1000, test_loss 17.469690313072412\n",
      "45/1000, test_loss 17.47057579733241\n",
      "46/1000, test_loss 17.47145532673566\n",
      "47/1000, test_loss 17.47232868914204\n",
      "48/1000, test_loss 17.473195688453156\n",
      "49/1000, test_loss 17.474056143857993\n",
      "50/1000, test_loss 17.47490988910984\n",
      "51/1000, test_loss 17.475756771833353\n",
      "52/1000, test_loss 17.4765966528606\n",
      "53/1000, test_loss 17.47742940559478\n",
      "54/1000, test_loss 17.478254915400683\n",
      "55/1000, test_loss 17.479073079020733\n",
      "56/1000, test_loss 17.479883804015607\n",
      "57/1000, test_loss 17.48068700822842\n",
      "58/1000, test_loss 17.481482619271652\n",
      "59/1000, test_loss 17.482270574035738\n",
      "60/1000, test_loss 17.48305081821856\n",
      "61/1000, test_loss 17.483823305875035\n",
      "62/1000, test_loss 17.484587998985816\n",
      "63/1000, test_loss 17.485344867044613\n",
      "64/1000, test_loss 17.48609388666311\n",
      "65/1000, test_loss 17.486835041192947\n",
      "66/1000, test_loss 17.487568320364012\n",
      "67/1000, test_loss 17.488293719938437\n",
      "68/1000, test_loss 17.489011241379533\n",
      "69/1000, test_loss 17.48972089153526\n",
      "70/1000, test_loss 17.490422682335456\n",
      "71/1000, test_loss 17.491116630502397\n",
      "72/1000, test_loss 17.491802757274076\n",
      "73/1000, test_loss 17.49248108813972\n",
      "74/1000, test_loss 17.493151652587006\n",
      "75/1000, test_loss 17.493814483860582\n",
      "76/1000, test_loss 17.49446961873132\n",
      "77/1000, test_loss 17.495117097275966\n",
      "78/1000, test_loss 17.495756962666697\n",
      "79/1000, test_loss 17.4963892609702\n",
      "80/1000, test_loss 17.49701404095591\n",
      "81/1000, test_loss 17.497631353912947\n",
      "82/1000, test_loss 17.498241253475516\n",
      "83/1000, test_loss 17.49884379545635\n",
      "84/1000, test_loss 17.499439037687782\n",
      "85/1000, test_loss 17.50002703987037\n",
      "86/1000, test_loss 17.5006078634285\n",
      "87/1000, test_loss 17.501181571372793\n",
      "88/1000, test_loss 17.501748228169138\n",
      "89/1000, test_loss 17.50230789961382\n",
      "90/1000, test_loss 17.502860652714762\n",
      "91/1000, test_loss 17.50340655557843\n",
      "92/1000, test_loss 17.50394567730225\n",
      "93/1000, test_loss 17.50447808787233\n",
      "94/1000, test_loss 17.505003858066146\n",
      "95/1000, test_loss 17.505523059360158\n",
      "96/1000, test_loss 17.506035763841943\n",
      "97/1000, test_loss 17.5065420441269\n",
      "98/1000, test_loss 17.507041973279037\n",
      "99/1000, test_loss 17.507535624735954\n",
      "100/1000, test_loss 17.508023072237666\n",
      "101/1000, test_loss 17.50850438975911\n",
      "102/1000, test_loss 17.508979651446314\n",
      "103/1000, test_loss 17.509448931555845\n",
      "104/1000, test_loss 17.509912304397673\n",
      "105/1000, test_loss 17.510369844281012\n",
      "106/1000, test_loss 17.51082162546326\n",
      "107/1000, test_loss 17.511267722101717\n",
      "108/1000, test_loss 17.51170820820809\n",
      "109/1000, test_loss 17.51214315760558\n",
      "110/1000, test_loss 17.51257264388846\n",
      "111/1000, test_loss 17.51299674038409\n",
      "112/1000, test_loss 17.513415520117142\n",
      "113/1000, test_loss 17.51382905577606\n",
      "114/1000, test_loss 17.514237419681578\n",
      "115/1000, test_loss 17.514640683757232\n",
      "116/1000, test_loss 17.51503891950177\n",
      "117/1000, test_loss 17.51543219796337\n",
      "118/1000, test_loss 17.515820589715617\n",
      "119/1000, test_loss 17.516204164835084\n",
      "120/1000, test_loss 17.51658299288054\n",
      "121/1000, test_loss 17.51695714287362\n",
      "122/1000, test_loss 17.517326683280942\n",
      "123/1000, test_loss 17.517691681997615\n",
      "124/1000, test_loss 17.518052206332015\n",
      "125/1000, test_loss 17.518408322991814\n",
      "126/1000, test_loss 17.518760098071205\n",
      "127/1000, test_loss 17.519107597039227\n",
      "128/1000, test_loss 17.519450884729192\n",
      "129/1000, test_loss 17.519790025329108\n",
      "130/1000, test_loss 17.520125082373085\n",
      "131/1000, test_loss 17.520456118733634\n",
      "132/1000, test_loss 17.520783196614932\n",
      "133/1000, test_loss 17.521106377546776\n",
      "134/1000, test_loss 17.521425722379476\n",
      "135/1000, test_loss 17.521741291279398\n",
      "136/1000, test_loss 17.522053143725262\n",
      "137/1000, test_loss 17.522361338505117\n",
      "138/1000, test_loss 17.522665933713938\n",
      "139/1000, test_loss 17.52296698675185\n",
      "140/1000, test_loss 17.523264554322925\n",
      "141/1000, test_loss 17.5235586924345\n",
      "142/1000, test_loss 17.523849456397038\n",
      "143/1000, test_loss 17.524136900824477\n",
      "144/1000, test_loss 17.524421079635008\n",
      "145/1000, test_loss 17.524702046052305\n",
      "146/1000, test_loss 17.524979852607178\n",
      "147/1000, test_loss 17.525254551139593\n",
      "148/1000, test_loss 17.525526192801056\n",
      "149/1000, test_loss 17.525794828057347\n",
      "150/1000, test_loss 17.526060506691554\n",
      "151/1000, test_loss 17.526323277807446\n",
      "152/1000, test_loss 17.52658318983309\n",
      "153/1000, test_loss 17.526840290524742\n",
      "154/1000, test_loss 17.527094626971007\n",
      "155/1000, test_loss 17.5273462455972\n",
      "156/1000, test_loss 17.527595192169933\n",
      "157/1000, test_loss 17.527841511801896\n",
      "158/1000, test_loss 17.528085248956856\n",
      "159/1000, test_loss 17.528326447454763\n",
      "160/1000, test_loss 17.52856515047709\n",
      "161/1000, test_loss 17.528801400572238\n",
      "162/1000, test_loss 17.529035239661184\n",
      "163/1000, test_loss 17.529266709043124\n",
      "164/1000, test_loss 17.52949584940133\n",
      "165/1000, test_loss 17.529722700809057\n",
      "166/1000, test_loss 17.529947302735575\n",
      "167/1000, test_loss 17.53016969405226\n",
      "168/1000, test_loss 17.530389913038753\n",
      "169/1000, test_loss 17.530607997389232\n",
      "170/1000, test_loss 17.53082398421868\n",
      "171/1000, test_loss 17.531037910069255\n",
      "172/1000, test_loss 17.531249810916673\n",
      "173/1000, test_loss 17.531459722176663\n",
      "174/1000, test_loss 17.531667678711383\n",
      "175/1000, test_loss 17.531873714835953\n",
      "176/1000, test_loss 17.53207786432494\n",
      "177/1000, test_loss 17.53228016041887\n",
      "178/1000, test_loss 17.53248063583077\n",
      "179/1000, test_loss 17.53267932275268\n",
      "180/1000, test_loss 17.532876252862206\n",
      "181/1000, test_loss 17.533071457329008\n",
      "182/1000, test_loss 17.53326496682135\n",
      "183/1000, test_loss 17.533456811512572\n",
      "184/1000, test_loss 17.533647021087578\n",
      "185/1000, test_loss 17.53383562474929\n",
      "186/1000, test_loss 17.53402265122511\n",
      "187/1000, test_loss 17.534208128773287\n",
      "188/1000, test_loss 17.53439208518932\n",
      "189/1000, test_loss 17.5345745478123\n",
      "190/1000, test_loss 17.534755543531222\n",
      "191/1000, test_loss 17.534935098791234\n",
      "192/1000, test_loss 17.53511323959989\n",
      "193/1000, test_loss 17.535289991533357\n",
      "194/1000, test_loss 17.5354653797425\n",
      "195/1000, test_loss 17.535639428959033\n",
      "196/1000, test_loss 17.535812163501554\n",
      "197/1000, test_loss 17.535983607281544\n",
      "198/1000, test_loss 17.53615378380932\n",
      "199/1000, test_loss 17.53632271619994\n",
      "200/1000, test_loss 17.536490427179043\n",
      "201/1000, test_loss 17.536656939088637\n",
      "202/1000, test_loss 17.536822273892852\n",
      "203/1000, test_loss 17.53698645318362\n",
      "204/1000, test_loss 17.53714949818627\n",
      "205/1000, test_loss 17.537311429765147\n",
      "206/1000, test_loss 17.537472268429063\n",
      "207/1000, test_loss 17.5376320343368\n",
      "208/1000, test_loss 17.53779074730247\n",
      "209/1000, test_loss 17.537948426800845\n",
      "210/1000, test_loss 17.53810509197265\n",
      "211/1000, test_loss 17.538260761629754\n",
      "212/1000, test_loss 17.538415454260317\n",
      "213/1000, test_loss 17.538569188033915\n",
      "214/1000, test_loss 17.538721980806507\n",
      "215/1000, test_loss 17.538873850125466\n",
      "216/1000, test_loss 17.53902481323444\n",
      "217/1000, test_loss 17.53917488707823\n",
      "218/1000, test_loss 17.53932408830753\n",
      "219/1000, test_loss 17.5394724332837\n",
      "220/1000, test_loss 17.53961993808339\n",
      "221/1000, test_loss 17.539766618503165\n",
      "222/1000, test_loss 17.53991249006403\n",
      "223/1000, test_loss 17.54005756801591\n",
      "224/1000, test_loss 17.5402018673421\n",
      "225/1000, test_loss 17.540345402763556\n",
      "226/1000, test_loss 17.540488188743286\n",
      "227/1000, test_loss 17.540630239490522\n",
      "228/1000, test_loss 17.54077156896493\n",
      "229/1000, test_loss 17.540912190880732\n",
      "230/1000, test_loss 17.54105211871077\n",
      "231/1000, test_loss 17.541191365690505\n",
      "232/1000, test_loss 17.54132994482199\n",
      "233/1000, test_loss 17.541467868877742\n",
      "234/1000, test_loss 17.541605150404592\n",
      "235/1000, test_loss 17.54174180172746\n",
      "236/1000, test_loss 17.541877834953088\n",
      "237/1000, test_loss 17.542013261973704\n",
      "238/1000, test_loss 17.54214809447064\n",
      "239/1000, test_loss 17.542282343917893\n",
      "240/1000, test_loss 17.54241602158565\n",
      "241/1000, test_loss 17.542549138543727\n",
      "242/1000, test_loss 17.54268170566497\n",
      "243/1000, test_loss 17.542813733628627\n",
      "244/1000, test_loss 17.542945232923625\n",
      "245/1000, test_loss 17.543076213851844\n",
      "246/1000, test_loss 17.543206686531303\n",
      "247/1000, test_loss 17.543336660899303\n",
      "248/1000, test_loss 17.543466146715545\n",
      "249/1000, test_loss 17.54359515356519\n",
      "250/1000, test_loss 17.543723690861825\n",
      "251/1000, test_loss 17.54385176785046\n",
      "252/1000, test_loss 17.54397939361042\n",
      "253/1000, test_loss 17.544106577058223\n",
      "254/1000, test_loss 17.544233326950383\n",
      "255/1000, test_loss 17.544359651886186\n",
      "256/1000, test_loss 17.54448556031043\n",
      "257/1000, test_loss 17.54461106051612\n",
      "258/1000, test_loss 17.544736160647073\n",
      "259/1000, test_loss 17.544860868700543\n",
      "260/1000, test_loss 17.54498519252979\n",
      "261/1000, test_loss 17.545109139846573\n",
      "262/1000, test_loss 17.545232718223627\n",
      "263/1000, test_loss 17.545355935097124\n",
      "264/1000, test_loss 17.545478797769015\n",
      "265/1000, test_loss 17.54560131340944\n",
      "266/1000, test_loss 17.545723489059007\n",
      "267/1000, test_loss 17.545845331631092\n",
      "268/1000, test_loss 17.545966847914062\n",
      "269/1000, test_loss 17.54608804457348\n",
      "270/1000, test_loss 17.546208928154282\n",
      "271/1000, test_loss 17.5463295050829\n",
      "272/1000, test_loss 17.54644978166936\n",
      "273/1000, test_loss 17.54656976410932\n",
      "274/1000, test_loss 17.546689458486124\n",
      "275/1000, test_loss 17.546808870772782\n",
      "276/1000, test_loss 17.54692800683391\n",
      "277/1000, test_loss 17.54704687242768\n",
      "278/1000, test_loss 17.547165473207688\n",
      "279/1000, test_loss 17.54728381472483\n",
      "280/1000, test_loss 17.54740190242912\n",
      "281/1000, test_loss 17.547519741671483\n",
      "282/1000, test_loss 17.54763733770553\n",
      "283/1000, test_loss 17.547754695689296\n",
      "284/1000, test_loss 17.547871820686918\n",
      "285/1000, test_loss 17.54798871767035\n",
      "286/1000, test_loss 17.548105391520977\n",
      "287/1000, test_loss 17.54822184703127\n",
      "288/1000, test_loss 17.54833808890633\n",
      "289/1000, test_loss 17.5484541217655\n",
      "290/1000, test_loss 17.548569950143868\n",
      "291/1000, test_loss 17.54868557849378\n",
      "292/1000, test_loss 17.548801011186367\n",
      "293/1000, test_loss 17.548916252512925\n",
      "294/1000, test_loss 17.549031306686423\n",
      "295/1000, test_loss 17.549146177842868\n",
      "296/1000, test_loss 17.54926087004271\n",
      "297/1000, test_loss 17.549375387272175\n",
      "298/1000, test_loss 17.54948973344463\n",
      "299/1000, test_loss 17.549603912401885\n",
      "300/1000, test_loss 17.549717927915474\n",
      "301/1000, test_loss 17.54983178368792\n",
      "302/1000, test_loss 17.549945483354005\n",
      "303/1000, test_loss 17.550059030481965\n",
      "304/1000, test_loss 17.550172428574697\n",
      "305/1000, test_loss 17.550285681070957\n",
      "306/1000, test_loss 17.55039879134649\n",
      "307/1000, test_loss 17.550511762715185\n",
      "308/1000, test_loss 17.550624598430215\n",
      "309/1000, test_loss 17.550737301685082\n",
      "310/1000, test_loss 17.55084987561475\n",
      "311/1000, test_loss 17.550962323296673\n",
      "312/1000, test_loss 17.55107464775186\n",
      "313/1000, test_loss 17.55118685194587\n",
      "314/1000, test_loss 17.551298938789838\n",
      "315/1000, test_loss 17.551410911141467\n",
      "316/1000, test_loss 17.551522771805963\n",
      "317/1000, test_loss 17.551634523537047\n",
      "318/1000, test_loss 17.551746169037823\n",
      "319/1000, test_loss 17.551857710961745\n",
      "320/1000, test_loss 17.551969151913493\n",
      "321/1000, test_loss 17.55208049444989\n",
      "322/1000, test_loss 17.552191741080723\n",
      "323/1000, test_loss 17.552302894269655\n",
      "324/1000, test_loss 17.55241395643502\n",
      "325/1000, test_loss 17.55252492995067\n",
      "326/1000, test_loss 17.552635817146772\n",
      "327/1000, test_loss 17.55274662031062\n",
      "328/1000, test_loss 17.55285734168741\n",
      "329/1000, test_loss 17.55296798348098\n",
      "330/1000, test_loss 17.55307854785462\n",
      "331/1000, test_loss 17.55318903693175\n",
      "332/1000, test_loss 17.5532994527967\n",
      "333/1000, test_loss 17.55340979749538\n",
      "334/1000, test_loss 17.553520073036008\n",
      "335/1000, test_loss 17.55363028138979\n",
      "336/1000, test_loss 17.553740424491586\n",
      "337/1000, test_loss 17.5538505042406\n",
      "338/1000, test_loss 17.553960522500994\n",
      "339/1000, test_loss 17.55407048110256\n",
      "340/1000, test_loss 17.55418038184134\n",
      "341/1000, test_loss 17.554290226480234\n",
      "342/1000, test_loss 17.5544000167496\n",
      "343/1000, test_loss 17.554509754347873\n",
      "344/1000, test_loss 17.554619440942126\n",
      "345/1000, test_loss 17.554729078168673\n",
      "346/1000, test_loss 17.554838667633586\n",
      "347/1000, test_loss 17.554948210913285\n",
      "348/1000, test_loss 17.555057709555083\n",
      "349/1000, test_loss 17.555167165077677\n",
      "350/1000, test_loss 17.55527657897173\n",
      "351/1000, test_loss 17.555385952700334\n",
      "352/1000, test_loss 17.555495287699546\n",
      "353/1000, test_loss 17.555604585378873\n",
      "354/1000, test_loss 17.555713847121762\n",
      "355/1000, test_loss 17.55582307428606\n",
      "356/1000, test_loss 17.555932268204515\n",
      "357/1000, test_loss 17.556041430185207\n",
      "358/1000, test_loss 17.55615056151202\n",
      "359/1000, test_loss 17.55625966344507\n",
      "360/1000, test_loss 17.556368737221145\n",
      "361/1000, test_loss 17.55647778405416\n",
      "362/1000, test_loss 17.556586805135524\n",
      "363/1000, test_loss 17.556695801634604\n",
      "364/1000, test_loss 17.55680477469908\n",
      "365/1000, test_loss 17.556913725455413\n",
      "366/1000, test_loss 17.557022655009153\n",
      "367/1000, test_loss 17.55713156444538\n",
      "368/1000, test_loss 17.55724045482906\n",
      "369/1000, test_loss 17.55734932720542\n",
      "370/1000, test_loss 17.557458182600286\n",
      "371/1000, test_loss 17.55756702202049\n",
      "372/1000, test_loss 17.55767584645416\n",
      "373/1000, test_loss 17.557784656871124\n",
      "374/1000, test_loss 17.55789345422319\n",
      "375/1000, test_loss 17.558002239444505\n",
      "376/1000, test_loss 17.558111013451892\n",
      "377/1000, test_loss 17.558219777145137\n",
      "378/1000, test_loss 17.558328531407323\n",
      "379/1000, test_loss 17.558437277105128\n",
      "380/1000, test_loss 17.55854601508915\n",
      "381/1000, test_loss 17.558654746194154\n",
      "382/1000, test_loss 17.558763471239423\n",
      "383/1000, test_loss 17.558872191028986\n",
      "384/1000, test_loss 17.558980906351938\n",
      "385/1000, test_loss 17.55908961798271\n",
      "386/1000, test_loss 17.55919832668131\n",
      "387/1000, test_loss 17.559307033193633\n",
      "388/1000, test_loss 17.55941573825167\n",
      "389/1000, test_loss 17.559524442573807\n",
      "390/1000, test_loss 17.559633146865067\n",
      "391/1000, test_loss 17.559741851817325\n",
      "392/1000, test_loss 17.55985055810959\n",
      "393/1000, test_loss 17.559959266408217\n",
      "394/1000, test_loss 17.560067977367154\n",
      "395/1000, test_loss 17.560176691628158\n",
      "396/1000, test_loss 17.560285409821024\n",
      "397/1000, test_loss 17.560394132563815\n",
      "398/1000, test_loss 17.56050286046306\n",
      "399/1000, test_loss 17.560611594113993\n",
      "400/1000, test_loss 17.56072033410072\n",
      "401/1000, test_loss 17.560829080996463\n",
      "402/1000, test_loss 17.56093783536375\n",
      "403/1000, test_loss 17.56104659775459\n",
      "404/1000, test_loss 17.561155368710708\n",
      "405/1000, test_loss 17.56126414876369\n",
      "406/1000, test_loss 17.5613729384352\n",
      "407/1000, test_loss 17.561481738237156\n",
      "408/1000, test_loss 17.5615905486719\n",
      "409/1000, test_loss 17.561699370232386\n",
      "410/1000, test_loss 17.561808203402354\n",
      "411/1000, test_loss 17.56191704865648\n",
      "412/1000, test_loss 17.56202590646057\n",
      "413/1000, test_loss 17.562134777271705\n",
      "414/1000, test_loss 17.562243661538414\n",
      "415/1000, test_loss 17.562352559700813\n",
      "416/1000, test_loss 17.562461472190783\n",
      "417/1000, test_loss 17.56257039943212\n",
      "418/1000, test_loss 17.562679341840646\n",
      "419/1000, test_loss 17.562788299824415\n",
      "420/1000, test_loss 17.562897273783808\n",
      "421/1000, test_loss 17.563006264111714\n",
      "422/1000, test_loss 17.563115271193613\n",
      "423/1000, test_loss 17.56322429540778\n",
      "424/1000, test_loss 17.563333337125375\n",
      "425/1000, test_loss 17.563442396710574\n",
      "426/1000, test_loss 17.563551474520715\n",
      "427/1000, test_loss 17.563660570906425\n",
      "428/1000, test_loss 17.563769686211714\n",
      "429/1000, test_loss 17.563878820774146\n",
      "430/1000, test_loss 17.5639879749249\n",
      "431/1000, test_loss 17.564097148988946\n",
      "432/1000, test_loss 17.56420634328512\n",
      "433/1000, test_loss 17.56431555812625\n",
      "434/1000, test_loss 17.564424793819267\n",
      "435/1000, test_loss 17.564534050665305\n",
      "436/1000, test_loss 17.564643328959825\n",
      "437/1000, test_loss 17.56475262899271\n",
      "438/1000, test_loss 17.56486195104836\n",
      "439/1000, test_loss 17.56497129540582\n",
      "440/1000, test_loss 17.56508066233883\n",
      "441/1000, test_loss 17.565190052115977\n",
      "442/1000, test_loss 17.565299465000763\n",
      "443/1000, test_loss 17.565408901251697\n",
      "444/1000, test_loss 17.56551836112239\n",
      "445/1000, test_loss 17.565627844861652\n",
      "446/1000, test_loss 17.565737352713583\n",
      "447/1000, test_loss 17.56584688491763\n",
      "448/1000, test_loss 17.565956441708725\n",
      "449/1000, test_loss 17.566066023317315\n",
      "450/1000, test_loss 17.566175629969482\n",
      "451/1000, test_loss 17.566285261887018\n",
      "452/1000, test_loss 17.56639491928747\n",
      "453/1000, test_loss 17.566504602384278\n",
      "454/1000, test_loss 17.56661431138679\n",
      "455/1000, test_loss 17.566724046500383\n",
      "456/1000, test_loss 17.56683380792651\n",
      "457/1000, test_loss 17.56694359586279\n",
      "458/1000, test_loss 17.567053410503057\n",
      "459/1000, test_loss 17.56716325203745\n",
      "460/1000, test_loss 17.567273120652473\n",
      "461/1000, test_loss 17.56738301653108\n",
      "462/1000, test_loss 17.567492939852684\n",
      "463/1000, test_loss 17.5676028907933\n",
      "464/1000, test_loss 17.567712869525558\n",
      "465/1000, test_loss 17.567822876218766\n",
      "466/1000, test_loss 17.567932911038994\n",
      "467/1000, test_loss 17.568042974149122\n",
      "468/1000, test_loss 17.568153065708902\n",
      "469/1000, test_loss 17.568263185874997\n",
      "470/1000, test_loss 17.568373334801077\n",
      "471/1000, test_loss 17.568483512637833\n",
      "472/1000, test_loss 17.56859371953306\n",
      "473/1000, test_loss 17.56870395563169\n",
      "474/1000, test_loss 17.56881422107588\n",
      "475/1000, test_loss 17.56892451600501\n",
      "476/1000, test_loss 17.56903484055578\n",
      "477/1000, test_loss 17.569145194862234\n",
      "478/1000, test_loss 17.569255579055827\n",
      "479/1000, test_loss 17.56936599326546\n",
      "480/1000, test_loss 17.569476437617524\n",
      "481/1000, test_loss 17.56958691223597\n",
      "482/1000, test_loss 17.56969741724231\n",
      "483/1000, test_loss 17.569807952755713\n",
      "484/1000, test_loss 17.56991851889302\n",
      "485/1000, test_loss 17.570029115768786\n",
      "486/1000, test_loss 17.570139743495325\n",
      "487/1000, test_loss 17.57025040218276\n",
      "488/1000, test_loss 17.570361091939063\n",
      "489/1000, test_loss 17.57047181287007\n",
      "490/1000, test_loss 17.570582565079572\n",
      "491/1000, test_loss 17.570693348669277\n",
      "492/1000, test_loss 17.57080416373893\n",
      "493/1000, test_loss 17.57091501038629\n",
      "494/1000, test_loss 17.5710258887072\n",
      "495/1000, test_loss 17.571136798795607\n",
      "496/1000, test_loss 17.571247740743587\n",
      "497/1000, test_loss 17.571358714641413\n",
      "498/1000, test_loss 17.571469720577554\n",
      "499/1000, test_loss 17.571580758638728\n",
      "500/1000, test_loss 17.571691828909938\n",
      "501/1000, test_loss 17.57180293147447\n",
      "502/1000, test_loss 17.57191406641397\n",
      "503/1000, test_loss 17.57202523380845\n",
      "504/1000, test_loss 17.572136433736315\n",
      "505/1000, test_loss 17.57224766627441\n",
      "506/1000, test_loss 17.572358931498027\n",
      "507/1000, test_loss 17.57247022948095\n",
      "508/1000, test_loss 17.572581560295475\n",
      "509/1000, test_loss 17.572692924012447\n",
      "510/1000, test_loss 17.57280432070127\n",
      "511/1000, test_loss 17.572915750429953\n",
      "512/1000, test_loss 17.573027213265128\n",
      "513/1000, test_loss 17.573138709272058\n",
      "514/1000, test_loss 17.573250238514692\n",
      "515/1000, test_loss 17.573361801055675\n",
      "516/1000, test_loss 17.573473396956366\n",
      "517/1000, test_loss 17.57358502627687\n",
      "518/1000, test_loss 17.573696689076073\n",
      "519/1000, test_loss 17.573808385411624\n",
      "520/1000, test_loss 17.57392011534\n",
      "521/1000, test_loss 17.574031878916532\n",
      "522/1000, test_loss 17.574143676195366\n",
      "523/1000, test_loss 17.57425550722956\n",
      "524/1000, test_loss 17.574367372071045\n",
      "525/1000, test_loss 17.574479270770684\n",
      "526/1000, test_loss 17.574591203378287\n",
      "527/1000, test_loss 17.574703169942595\n",
      "528/1000, test_loss 17.574815170511357\n",
      "529/1000, test_loss 17.57492720513129\n",
      "530/1000, test_loss 17.57503927384814\n",
      "531/1000, test_loss 17.575151376706692\n",
      "532/1000, test_loss 17.57526351375077\n",
      "533/1000, test_loss 17.575375685023264\n",
      "534/1000, test_loss 17.57548789056616\n",
      "535/1000, test_loss 17.575600130420547\n",
      "536/1000, test_loss 17.57571240462662\n",
      "537/1000, test_loss 17.57582471322372\n",
      "538/1000, test_loss 17.575937056250332\n",
      "539/1000, test_loss 17.576049433744117\n",
      "540/1000, test_loss 17.57616184574191\n",
      "541/1000, test_loss 17.57627429227975\n",
      "542/1000, test_loss 17.576386773392876\n",
      "543/1000, test_loss 17.57649928911577\n",
      "544/1000, test_loss 17.576611839482148\n",
      "545/1000, test_loss 17.576724424524972\n",
      "546/1000, test_loss 17.57683704427649\n",
      "547/1000, test_loss 17.576949698768217\n",
      "548/1000, test_loss 17.577062388030974\n",
      "549/1000, test_loss 17.57717511209489\n",
      "550/1000, test_loss 17.57728787098941\n",
      "551/1000, test_loss 17.5774006647433\n",
      "552/1000, test_loss 17.577513493384718\n",
      "553/1000, test_loss 17.577626356941124\n",
      "554/1000, test_loss 17.57773925543938\n",
      "555/1000, test_loss 17.57785218890573\n",
      "556/1000, test_loss 17.577965157365803\n",
      "557/1000, test_loss 17.578078160844637\n",
      "558/1000, test_loss 17.57819119936667\n",
      "559/1000, test_loss 17.5783042729558\n",
      "560/1000, test_loss 17.578417381635322\n",
      "561/1000, test_loss 17.57853052542801\n",
      "562/1000, test_loss 17.578643704356082\n",
      "563/1000, test_loss 17.578756918441222\n",
      "564/1000, test_loss 17.57887016770459\n",
      "565/1000, test_loss 17.578983452166845\n",
      "566/1000, test_loss 17.579096771848132\n",
      "567/1000, test_loss 17.579210126768103\n",
      "568/1000, test_loss 17.579323516945934\n",
      "569/1000, test_loss 17.57943694240031\n",
      "570/1000, test_loss 17.579550403149458\n",
      "571/1000, test_loss 17.57966389921115\n",
      "572/1000, test_loss 17.57977743060269\n",
      "573/1000, test_loss 17.579890997340975\n",
      "574/1000, test_loss 17.580004599442425\n",
      "575/1000, test_loss 17.58011823692307\n",
      "576/1000, test_loss 17.5802319097985\n",
      "577/1000, test_loss 17.58034561808391\n",
      "578/1000, test_loss 17.580459361794077\n",
      "579/1000, test_loss 17.5805731409434\n",
      "580/1000, test_loss 17.58068695554588\n",
      "581/1000, test_loss 17.580800805615137\n",
      "582/1000, test_loss 17.580914691164427\n",
      "583/1000, test_loss 17.581028612206627\n",
      "584/1000, test_loss 17.581142568754263\n",
      "585/1000, test_loss 17.5812565608195\n",
      "586/1000, test_loss 17.581370588414167\n",
      "587/1000, test_loss 17.581484651549747\n",
      "588/1000, test_loss 17.581598750237383\n",
      "589/1000, test_loss 17.58171288448791\n",
      "590/1000, test_loss 17.581827054311812\n",
      "591/1000, test_loss 17.581941259719283\n",
      "592/1000, test_loss 17.582055500720198\n",
      "593/1000, test_loss 17.582169777324122\n",
      "594/1000, test_loss 17.582284089540337\n",
      "595/1000, test_loss 17.58239843737781\n",
      "596/1000, test_loss 17.58251282084524\n",
      "597/1000, test_loss 17.58262723995103\n",
      "598/1000, test_loss 17.582741694703326\n",
      "599/1000, test_loss 17.582856185109975\n",
      "600/1000, test_loss 17.582970711178575\n",
      "601/1000, test_loss 17.58308527291646\n",
      "602/1000, test_loss 17.5831998703307\n",
      "603/1000, test_loss 17.583314503428127\n",
      "604/1000, test_loss 17.583429172215304\n",
      "605/1000, test_loss 17.583543876698574\n",
      "606/1000, test_loss 17.583658616884026\n",
      "607/1000, test_loss 17.58377339277752\n",
      "608/1000, test_loss 17.58388820438468\n",
      "609/1000, test_loss 17.584003051710926\n",
      "610/1000, test_loss 17.58411793476142\n",
      "611/1000, test_loss 17.584232853541142\n",
      "612/1000, test_loss 17.58434780805484\n",
      "613/1000, test_loss 17.58446279830706\n",
      "614/1000, test_loss 17.584577824302137\n",
      "615/1000, test_loss 17.58469288604421\n",
      "616/1000, test_loss 17.584807983537214\n",
      "617/1000, test_loss 17.584923116784896\n",
      "618/1000, test_loss 17.585038285790805\n",
      "619/1000, test_loss 17.58515349055832\n",
      "620/1000, test_loss 17.58526873109061\n",
      "621/1000, test_loss 17.585384007390687\n",
      "622/1000, test_loss 17.58549931946137\n",
      "623/1000, test_loss 17.585614667305318\n",
      "624/1000, test_loss 17.58573005092501\n",
      "625/1000, test_loss 17.58584547032277\n",
      "626/1000, test_loss 17.58596092550074\n",
      "627/1000, test_loss 17.58607641646091\n",
      "628/1000, test_loss 17.586191943205108\n",
      "629/1000, test_loss 17.58630750573502\n",
      "630/1000, test_loss 17.586423104052173\n",
      "631/1000, test_loss 17.586538738157934\n",
      "632/1000, test_loss 17.586654408053537\n",
      "633/1000, test_loss 17.586770113740062\n",
      "634/1000, test_loss 17.58688585521845\n",
      "635/1000, test_loss 17.58700163248952\n",
      "636/1000, test_loss 17.587117445553933\n",
      "637/1000, test_loss 17.58723329441222\n",
      "638/1000, test_loss 17.587349179064788\n",
      "639/1000, test_loss 17.587465099511917\n",
      "640/1000, test_loss 17.587581055753752\n",
      "641/1000, test_loss 17.587697047790318\n",
      "642/1000, test_loss 17.587813075621522\n",
      "643/1000, test_loss 17.58792913924715\n",
      "644/1000, test_loss 17.588045238666865\n",
      "645/1000, test_loss 17.588161373880222\n",
      "646/1000, test_loss 17.588277544886665\n",
      "647/1000, test_loss 17.588393751685523\n",
      "648/1000, test_loss 17.58850999427601\n",
      "649/1000, test_loss 17.58862627265724\n",
      "650/1000, test_loss 17.588742586828232\n",
      "651/1000, test_loss 17.588858936787883\n",
      "652/1000, test_loss 17.588975322535003\n",
      "653/1000, test_loss 17.589091744068302\n",
      "654/1000, test_loss 17.589208201386388\n",
      "655/1000, test_loss 17.589324694487768\n",
      "656/1000, test_loss 17.58944122337088\n",
      "657/1000, test_loss 17.589557788034043\n",
      "658/1000, test_loss 17.589674388475494\n",
      "659/1000, test_loss 17.589791024693383\n",
      "660/1000, test_loss 17.58990769668579\n",
      "661/1000, test_loss 17.59002440445068\n",
      "662/1000, test_loss 17.59014114798595\n",
      "663/1000, test_loss 17.590257927289418\n",
      "664/1000, test_loss 17.590374742358815\n",
      "665/1000, test_loss 17.5904915931918\n",
      "666/1000, test_loss 17.59060847978595\n",
      "667/1000, test_loss 17.59072540213876\n",
      "668/1000, test_loss 17.59084236024766\n",
      "669/1000, test_loss 17.590959354110005\n",
      "670/1000, test_loss 17.59107638372307\n",
      "671/1000, test_loss 17.591193449084077\n",
      "672/1000, test_loss 17.591310550190155\n",
      "673/1000, test_loss 17.591427687038394\n",
      "674/1000, test_loss 17.591544859625785\n",
      "675/1000, test_loss 17.59166206794928\n",
      "676/1000, test_loss 17.591779312005755\n",
      "677/1000, test_loss 17.591896591792025\n",
      "678/1000, test_loss 17.592013907304842\n",
      "679/1000, test_loss 17.5921312585409\n",
      "680/1000, test_loss 17.59224864549683\n",
      "681/1000, test_loss 17.59236606816921\n",
      "682/1000, test_loss 17.59248352655455\n",
      "683/1000, test_loss 17.59260102064932\n",
      "684/1000, test_loss 17.592718550449916\n",
      "685/1000, test_loss 17.592836115952704\n",
      "686/1000, test_loss 17.59295371715397\n",
      "687/1000, test_loss 17.593071354049968\n",
      "688/1000, test_loss 17.593189026636892\n",
      "689/1000, test_loss 17.593306734910886\n",
      "690/1000, test_loss 17.593424478868048\n",
      "691/1000, test_loss 17.59354225850442\n",
      "692/1000, test_loss 17.593660073816007\n",
      "693/1000, test_loss 17.593777924798772\n",
      "694/1000, test_loss 17.593895811448608\n",
      "695/1000, test_loss 17.594013733761383\n",
      "696/1000, test_loss 17.59413169173292\n",
      "697/1000, test_loss 17.594249685358992\n",
      "698/1000, test_loss 17.594367714635336\n",
      "699/1000, test_loss 17.594485779557644\n",
      "700/1000, test_loss 17.594603880121564\n",
      "701/1000, test_loss 17.594722016322716\n",
      "702/1000, test_loss 17.594840188156667\n",
      "703/1000, test_loss 17.59495839561895\n",
      "704/1000, test_loss 17.595076638705063\n",
      "705/1000, test_loss 17.595194917410467\n",
      "706/1000, test_loss 17.595313231730582\n",
      "707/1000, test_loss 17.595431581660797\n",
      "708/1000, test_loss 17.59554996719647\n",
      "709/1000, test_loss 17.59566838833291\n",
      "710/1000, test_loss 17.59578684506541\n",
      "711/1000, test_loss 17.59590533738921\n",
      "712/1000, test_loss 17.59602386529954\n",
      "713/1000, test_loss 17.59614242879158\n",
      "714/1000, test_loss 17.59626102786049\n",
      "715/1000, test_loss 17.5963796625014\n",
      "716/1000, test_loss 17.596498332709395\n",
      "717/1000, test_loss 17.596617038479547\n",
      "718/1000, test_loss 17.59673577980689\n",
      "719/1000, test_loss 17.59685455668644\n",
      "720/1000, test_loss 17.59697336911317\n",
      "721/1000, test_loss 17.59709221708204\n",
      "722/1000, test_loss 17.597211100587966\n",
      "723/1000, test_loss 17.597330019625865\n",
      "724/1000, test_loss 17.5974489741906\n",
      "725/1000, test_loss 17.59756796427702\n",
      "726/1000, test_loss 17.597686989879957\n",
      "727/1000, test_loss 17.59780605099421\n",
      "728/1000, test_loss 17.597925147614554\n",
      "729/1000, test_loss 17.59804427973574\n",
      "730/1000, test_loss 17.598163447352494\n",
      "731/1000, test_loss 17.59828265045954\n",
      "732/1000, test_loss 17.59840188905155\n",
      "733/1000, test_loss 17.598521163123195\n",
      "734/1000, test_loss 17.59864047266911\n",
      "735/1000, test_loss 17.59875981768393\n",
      "736/1000, test_loss 17.598879198162244\n",
      "737/1000, test_loss 17.59899861409864\n",
      "738/1000, test_loss 17.59911806548768\n",
      "739/1000, test_loss 17.599237552323906\n",
      "740/1000, test_loss 17.599357074601837\n",
      "741/1000, test_loss 17.599476632315987\n",
      "742/1000, test_loss 17.599596225460836\n",
      "743/1000, test_loss 17.59971585403086\n",
      "744/1000, test_loss 17.5998355180205\n",
      "745/1000, test_loss 17.599955217424206\n",
      "746/1000, test_loss 17.600074952236383\n",
      "747/1000, test_loss 17.600194722451434\n",
      "748/1000, test_loss 17.600314528063752\n",
      "749/1000, test_loss 17.600434369067692\n",
      "750/1000, test_loss 17.600554245457626\n",
      "751/1000, test_loss 17.600674157227882\n",
      "752/1000, test_loss 17.60079410437278\n",
      "753/1000, test_loss 17.60091408688664\n",
      "754/1000, test_loss 17.601034104763745\n",
      "755/1000, test_loss 17.601154157998383\n",
      "756/1000, test_loss 17.60127424658482\n",
      "757/1000, test_loss 17.60139437051731\n",
      "758/1000, test_loss 17.601514529790084\n",
      "759/1000, test_loss 17.601634724397382\n",
      "760/1000, test_loss 17.60175495433341\n",
      "761/1000, test_loss 17.601875219592365\n",
      "762/1000, test_loss 17.601995520168444\n",
      "763/1000, test_loss 17.60211585605582\n",
      "764/1000, test_loss 17.60223622724867\n",
      "765/1000, test_loss 17.602356633741135\n",
      "766/1000, test_loss 17.602477075527357\n",
      "767/1000, test_loss 17.602597552601477\n",
      "768/1000, test_loss 17.602718064957603\n",
      "769/1000, test_loss 17.60283861258986\n",
      "770/1000, test_loss 17.602959195492332\n",
      "771/1000, test_loss 17.60307981365912\n",
      "772/1000, test_loss 17.6032004670843\n",
      "773/1000, test_loss 17.60332115576194\n",
      "774/1000, test_loss 17.6034418796861\n",
      "775/1000, test_loss 17.60356263885083\n",
      "776/1000, test_loss 17.60368343325017\n",
      "777/1000, test_loss 17.603804262878153\n",
      "778/1000, test_loss 17.6039251277288\n",
      "779/1000, test_loss 17.604046027796137\n",
      "780/1000, test_loss 17.604166963074146\n",
      "781/1000, test_loss 17.604287933556854\n",
      "782/1000, test_loss 17.604408939238223\n",
      "783/1000, test_loss 17.60452998011225\n",
      "784/1000, test_loss 17.60465105617291\n",
      "785/1000, test_loss 17.604772167414158\n",
      "786/1000, test_loss 17.604893313829958\n",
      "787/1000, test_loss 17.60501449541426\n",
      "788/1000, test_loss 17.605135712161008\n",
      "789/1000, test_loss 17.605256964064136\n",
      "790/1000, test_loss 17.60537825111758\n",
      "791/1000, test_loss 17.60549957331525\n",
      "792/1000, test_loss 17.605620930651078\n",
      "793/1000, test_loss 17.605742323118964\n",
      "794/1000, test_loss 17.605863750712807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795/1000, test_loss 17.605985213426514\n",
      "796/1000, test_loss 17.60610671125397\n",
      "797/1000, test_loss 17.606228244189058\n",
      "798/1000, test_loss 17.606349812225666\n",
      "799/1000, test_loss 17.606471415357657\n",
      "800/1000, test_loss 17.606593053578905\n",
      "801/1000, test_loss 17.60671472688327\n",
      "802/1000, test_loss 17.606836435264604\n",
      "803/1000, test_loss 17.606958178716766\n",
      "804/1000, test_loss 17.607079957233598\n",
      "805/1000, test_loss 17.60720177080894\n",
      "806/1000, test_loss 17.607323619436627\n",
      "807/1000, test_loss 17.6074455031105\n",
      "808/1000, test_loss 17.607567421824378\n",
      "809/1000, test_loss 17.607689375572082\n",
      "810/1000, test_loss 17.60781136434743\n",
      "811/1000, test_loss 17.60793338814424\n",
      "812/1000, test_loss 17.608055446956318\n",
      "813/1000, test_loss 17.60817754077747\n",
      "814/1000, test_loss 17.60829966960149\n",
      "815/1000, test_loss 17.608421833422177\n",
      "816/1000, test_loss 17.608544032233333\n",
      "817/1000, test_loss 17.608666266028735\n",
      "818/1000, test_loss 17.608788534802173\n",
      "819/1000, test_loss 17.608910838547427\n",
      "820/1000, test_loss 17.609033177258272\n",
      "821/1000, test_loss 17.609155550928485\n",
      "822/1000, test_loss 17.609277959551836\n",
      "823/1000, test_loss 17.609400403122095\n",
      "824/1000, test_loss 17.60952288163302\n",
      "825/1000, test_loss 17.609645395078374\n",
      "826/1000, test_loss 17.609767943451917\n",
      "827/1000, test_loss 17.609890526747403\n",
      "828/1000, test_loss 17.61001314495858\n",
      "829/1000, test_loss 17.61013579807919\n",
      "830/1000, test_loss 17.610258486103003\n",
      "831/1000, test_loss 17.610381209023735\n",
      "832/1000, test_loss 17.61050396683514\n",
      "833/1000, test_loss 17.610626759530952\n",
      "834/1000, test_loss 17.61074958710491\n",
      "835/1000, test_loss 17.610872449550733\n",
      "836/1000, test_loss 17.610995346862172\n",
      "837/1000, test_loss 17.611118279032937\n",
      "838/1000, test_loss 17.61124124605676\n",
      "839/1000, test_loss 17.61136424792737\n",
      "840/1000, test_loss 17.611487284638475\n",
      "841/1000, test_loss 17.611610356183803\n",
      "842/1000, test_loss 17.611733462557066\n",
      "843/1000, test_loss 17.611856603751978\n",
      "844/1000, test_loss 17.611979779762255\n",
      "845/1000, test_loss 17.612102990581608\n",
      "846/1000, test_loss 17.612226236203735\n",
      "847/1000, test_loss 17.612349516622352\n",
      "848/1000, test_loss 17.612472831831166\n",
      "849/1000, test_loss 17.612596181823875\n",
      "850/1000, test_loss 17.612719566594183\n",
      "851/1000, test_loss 17.61284298613579\n",
      "852/1000, test_loss 17.61296644044239\n",
      "853/1000, test_loss 17.613089929507684\n",
      "854/1000, test_loss 17.613213453325365\n",
      "855/1000, test_loss 17.613337011889126\n",
      "856/1000, test_loss 17.613460605192657\n",
      "857/1000, test_loss 17.61358423322966\n",
      "858/1000, test_loss 17.61370789599381\n",
      "859/1000, test_loss 17.613831593478803\n",
      "860/1000, test_loss 17.613955325678322\n",
      "861/1000, test_loss 17.61407909258606\n",
      "862/1000, test_loss 17.61420289419569\n",
      "863/1000, test_loss 17.614326730500903\n",
      "864/1000, test_loss 17.61445060149538\n",
      "865/1000, test_loss 17.614574507172804\n",
      "866/1000, test_loss 17.614698447526848\n",
      "867/1000, test_loss 17.614822422551203\n",
      "868/1000, test_loss 17.614946432239535\n",
      "869/1000, test_loss 17.615070476585522\n",
      "870/1000, test_loss 17.61519455558285\n",
      "871/1000, test_loss 17.615318669225186\n",
      "872/1000, test_loss 17.615442817506207\n",
      "873/1000, test_loss 17.615567000419585\n",
      "874/1000, test_loss 17.615691217958997\n",
      "875/1000, test_loss 17.615815470118104\n",
      "876/1000, test_loss 17.61593975689059\n",
      "877/1000, test_loss 17.616064078270124\n",
      "878/1000, test_loss 17.61618843425037\n",
      "879/1000, test_loss 17.616312824825\n",
      "880/1000, test_loss 17.616437249987683\n",
      "881/1000, test_loss 17.616561709732093\n",
      "882/1000, test_loss 17.616686204051888\n",
      "883/1000, test_loss 17.61681073294074\n",
      "884/1000, test_loss 17.616935296392313\n",
      "885/1000, test_loss 17.617059894400274\n",
      "886/1000, test_loss 17.61718452695829\n",
      "887/1000, test_loss 17.617309194060027\n",
      "888/1000, test_loss 17.617433895699147\n",
      "889/1000, test_loss 17.61755863186932\n",
      "890/1000, test_loss 17.617683402564197\n",
      "891/1000, test_loss 17.617808207777458\n",
      "892/1000, test_loss 17.617933047502756\n",
      "893/1000, test_loss 17.618057921733758\n",
      "894/1000, test_loss 17.618182830464125\n",
      "895/1000, test_loss 17.61830777368752\n",
      "896/1000, test_loss 17.618432751397606\n",
      "897/1000, test_loss 17.618557763588043\n",
      "898/1000, test_loss 17.618682810252494\n",
      "899/1000, test_loss 17.61880789138463\n",
      "900/1000, test_loss 17.618933006978093\n",
      "901/1000, test_loss 17.61905815702656\n",
      "902/1000, test_loss 17.61918334152369\n",
      "903/1000, test_loss 17.619308560463136\n",
      "904/1000, test_loss 17.61943381383857\n",
      "905/1000, test_loss 17.61955910164365\n",
      "906/1000, test_loss 17.61968442387203\n",
      "907/1000, test_loss 17.619809780517375\n",
      "908/1000, test_loss 17.61993517157335\n",
      "909/1000, test_loss 17.62006059703361\n",
      "910/1000, test_loss 17.62018605689182\n",
      "911/1000, test_loss 17.62031155114164\n",
      "912/1000, test_loss 17.620437079776732\n",
      "913/1000, test_loss 17.620562642790755\n",
      "914/1000, test_loss 17.62068824017737\n",
      "915/1000, test_loss 17.620813871930242\n",
      "916/1000, test_loss 17.620939538043025\n",
      "917/1000, test_loss 17.62106523850939\n",
      "918/1000, test_loss 17.62119097332299\n",
      "919/1000, test_loss 17.621316742477493\n",
      "920/1000, test_loss 17.621442545966552\n",
      "921/1000, test_loss 17.621568383783845\n",
      "922/1000, test_loss 17.621694255923018\n",
      "923/1000, test_loss 17.62182016237774\n",
      "924/1000, test_loss 17.621946103141678\n",
      "925/1000, test_loss 17.622072078208486\n",
      "926/1000, test_loss 17.62219808757184\n",
      "927/1000, test_loss 17.622324131225383\n",
      "928/1000, test_loss 17.622450209162796\n",
      "929/1000, test_loss 17.622576321377736\n",
      "930/1000, test_loss 17.622702467863874\n",
      "931/1000, test_loss 17.62282864861486\n",
      "932/1000, test_loss 17.622954863624372\n",
      "933/1000, test_loss 17.62308111288607\n",
      "934/1000, test_loss 17.62320739639362\n",
      "935/1000, test_loss 17.623333714140685\n",
      "936/1000, test_loss 17.623460066120938\n",
      "937/1000, test_loss 17.623586452328034\n",
      "938/1000, test_loss 17.623712872755654\n",
      "939/1000, test_loss 17.623839327397455\n",
      "940/1000, test_loss 17.6239658162471\n",
      "941/1000, test_loss 17.62409233929827\n",
      "942/1000, test_loss 17.624218896544622\n",
      "943/1000, test_loss 17.62434548797983\n",
      "944/1000, test_loss 17.624472113597566\n",
      "945/1000, test_loss 17.624598773391494\n",
      "946/1000, test_loss 17.624725467355283\n",
      "947/1000, test_loss 17.624852195482603\n",
      "948/1000, test_loss 17.624978957767134\n",
      "949/1000, test_loss 17.625105754202536\n",
      "950/1000, test_loss 17.625232584782484\n",
      "951/1000, test_loss 17.625359449500653\n",
      "952/1000, test_loss 17.625486348350712\n",
      "953/1000, test_loss 17.62561328132633\n",
      "954/1000, test_loss 17.62574024842119\n",
      "955/1000, test_loss 17.625867249628964\n",
      "956/1000, test_loss 17.625994284943324\n",
      "957/1000, test_loss 17.626121354357938\n",
      "958/1000, test_loss 17.62624845786649\n",
      "959/1000, test_loss 17.626375595462658\n",
      "960/1000, test_loss 17.626502767140114\n",
      "961/1000, test_loss 17.626629972892534\n",
      "962/1000, test_loss 17.626757212713596\n",
      "963/1000, test_loss 17.62688448659698\n",
      "964/1000, test_loss 17.627011794536358\n",
      "965/1000, test_loss 17.627139136525425\n",
      "966/1000, test_loss 17.627266512557842\n",
      "967/1000, test_loss 17.627393922627302\n",
      "968/1000, test_loss 17.62752136672748\n",
      "969/1000, test_loss 17.627648844852054\n",
      "970/1000, test_loss 17.62777635699472\n",
      "971/1000, test_loss 17.627903903149143\n",
      "972/1000, test_loss 17.628031483309016\n",
      "973/1000, test_loss 17.62815909746802\n",
      "974/1000, test_loss 17.628286745619846\n",
      "975/1000, test_loss 17.628414427758166\n",
      "976/1000, test_loss 17.62854214387668\n",
      "977/1000, test_loss 17.628669893969057\n",
      "978/1000, test_loss 17.628797678029002\n",
      "979/1000, test_loss 17.628925496050186\n",
      "980/1000, test_loss 17.62905334802631\n",
      "981/1000, test_loss 17.629181233951055\n",
      "982/1000, test_loss 17.629309153818113\n",
      "983/1000, test_loss 17.629437107621182\n",
      "984/1000, test_loss 17.629565095353932\n",
      "985/1000, test_loss 17.629693117010067\n",
      "986/1000, test_loss 17.629821172583284\n",
      "987/1000, test_loss 17.629949262067267\n",
      "988/1000, test_loss 17.630077385455706\n",
      "989/1000, test_loss 17.630205542742306\n",
      "990/1000, test_loss 17.630333733920754\n",
      "991/1000, test_loss 17.630461958984743\n",
      "992/1000, test_loss 17.630590217927974\n",
      "993/1000, test_loss 17.630718510744146\n",
      "994/1000, test_loss 17.630846837426944\n",
      "995/1000, test_loss 17.630975197970077\n",
      "996/1000, test_loss 17.631103592367236\n",
      "997/1000, test_loss 17.631232020612128\n",
      "998/1000, test_loss 17.631360482698444\n",
      "999/1000, test_loss 17.631488978619892\n",
      "1000/1000, test_loss 17.63161750837017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "X = iris.data[50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "model_scratch = ScratchLogisticRegression(1000, 0.001, True, True)\n",
    "model_scratch.fit(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1]\n",
      "[0.66599294 0.662864   0.61469487 0.6154344  0.62124002 0.50996591\n",
      " 0.50417035 0.51199084 0.49717478 0.52164983 0.58623628 0.57862637\n",
      " 0.46258604 0.53349048 0.44209164 0.50265607 0.53083809 0.66671261\n",
      " 0.46576207 0.65823374 0.61058535 0.55281494 0.5533495  0.50751661\n",
      " 0.59426495]\n",
      "--------------------------------------------------\n",
      "[[ 4  0]\n",
      " [ 8 13]]\n",
      "0.68\n",
      "1.0\n",
      "0.6190476190476191\n",
      "0.7647058823529412\n",
      "log loss: 17.631743915049086\n"
     ]
    }
   ],
   "source": [
    "# 評価の確認\n",
    "pred = model_scratch.predict(X_test)\n",
    "pred_proba = model_scratch.predict_proba(X_test)\n",
    "\n",
    "print(pred.ravel())\n",
    "print(pred_proba)\n",
    "print(\"-\"*50)\n",
    "# 混合行列、正解率、適合率、再現率、F値\n",
    "print(confusion_matrix(pred, y_test))\n",
    "print(accuracy_score(pred, y_test))\n",
    "print(precision_score(pred, y_test))\n",
    "print(recall_score(pred, y_test))\n",
    "print(f1_score(pred, y_test))\n",
    "\n",
    "# スクラッチのクロスエントロピー\n",
    "_, cost = model_scratch.cost_function(X_test, y_test, pred_proba)\n",
    "print(\"log loss: {}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1]\n",
      "[0.54645183 0.89594754 0.95960762 0.05054654 0.07560458 0.05182233\n",
      " 0.45569541 0.97205303 0.01100832 0.17350702 0.25509376 0.0213071\n",
      " 0.58417041 0.63124837 0.96149306 0.22305241 0.5059302  0.54943092\n",
      " 0.02981109 0.01313685 0.99445254 0.48639945 0.272587   0.03704218\n",
      " 0.58968991]\n",
      "[[ 2  2]\n",
      " [12  9]]\n",
      "0.44\n",
      "0.8181818181818182\n",
      "0.42857142857142855\n",
      "0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.342098587916475"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearnのロジスティック回帰と比較する\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n",
    "\n",
    "X = iris.data[50:150]\n",
    "y = pd.DataFrame(iris.target[50:150])\n",
    "\n",
    "y = y.replace({1:0, 2:1})\n",
    "y = y.to_numpy().flatten()\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "pred_sk = model.predict(X_test)\n",
    "pred_sk_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# predとprobaの確認\n",
    "print(pred_sk)\n",
    "print(pred_sk_proba)\n",
    "\n",
    "# 混合行列、正解率、適合率、再現率、F値\n",
    "print(confusion_matrix(pred, y_test))\n",
    "print(accuracy_score(pred, y_test))\n",
    "print(precision_score(pred, y_test))\n",
    "print(recall_score(pred, y_test))\n",
    "print(f1_score(pred, y_test))\n",
    "\n",
    "# sklearnのクロスエントロピー\n",
    "\n",
    "log_loss(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習曲線のプロット\n",
    "学習曲線を見て損失が適切に下がっているかどうか確認してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n考察\\nvalはほとんど誤差に変化がないが、、、\\n計算が正しいという前提の元でいうと\\n訓練データと比べてテストデータが低いので、汎化性が高いと言える\\n（※計算があっているのか？）\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1000), model_scratch.list_train_loss, c='y', label='train_loss')  \n",
    "plt.plot(range(1000), model_scratch.list_val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "考察\n",
    "valはほとんど誤差に変化がないが、、、\n",
    "計算が正しいという前提の元でいうと\n",
    "訓練データと比べてテストデータが低いので、汎化性が高いと言える\n",
    "（※計算があっているのか？）\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】決定領域の可視化\n",
    "決定領域を可視化してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data[50:150], columns=iris.feature_names)\n",
    "y = pd.DataFrame(iris.target[50:150], columns=['target'])\n",
    "X = X.loc[:, ['petal length (cm)','petal width (cm)']]\n",
    "df = pd.concat([X,y], axis=1)\n",
    "df[df['target'] == 1].iloc[:, 0]\n",
    "\n",
    "iris.feature_names\n",
    "#sepal length (cm)\", \"petal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n",
      "(50, 50)\n",
      "(2500, 2)\n",
      "[[0.60489515 0.59847922 0.59202937 ... 0.30314746 0.297522   0.29195719]\n",
      " [0.60911001 0.60271757 0.59628988 ... 0.3068929  0.30122798 0.29562303]\n",
      " [0.61330866 0.60694057 0.60053592 ... 0.31066399 0.30496009 0.29931543]\n",
      " ...\n",
      " [0.7783948  0.77374242 0.76902136 ... 0.49951949 0.49282712 0.48613732]\n",
      " [0.7814276  0.77682064 0.77214489 ... 0.50393661 0.49724385 0.49055208]\n",
      " [0.78443039 0.77986889 0.77523853 ... 0.5083531  0.50166101 0.49496832]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df[df['target'] == 1].iloc[:, 0], df[df['target'] == 1].iloc[:, 1], color='b', label='1')\n",
    "plt.scatter(df[df['target'] == 2].iloc[:, 0], df[df['target'] == 2].iloc[:, 1], color='r', label='2')\n",
    "plt.legend()\n",
    "\n",
    "x1_min, x1_max = X.iloc[:,0].min(), X.iloc[:,0].max(),\n",
    "x2_min, x2_max = X.iloc[:,1].min(), X.iloc[:,1].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "print(xx1.shape)\n",
    "print(xx2.shape)\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "print(grid.shape)\n",
    "probs = model_scratch.predict_proba(grid).reshape(xx1.shape)\n",
    "print(probs)\n",
    "plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='blue')\n",
    "plt.show()\n",
    "\n",
    "# これが限界っぽい\n",
    "# 30hほど格闘してsethaが正しく訂正されないので、こちらで提出します。\n",
    "# エラーの原因が判明できず下記にエラー現象を添付するので是非とも教えてください。\n",
    "\n",
    "# 詳細\n",
    "# thetaの次元が入力されたXに対し対応しないので起きているエラーです\n",
    "# __init__、fit、predict、predict_probaにthetaを入れましたがエラーに対応しないです。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n",
      "(50, 50)\n",
      "(2500, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2500,3) and (5,) not aligned: 3 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-688ed0b7439f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_scratch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-304-3cfec2e55cbb>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mpred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logistic_hypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-304-3cfec2e55cbb>\u001b[0m in \u001b[0;36m_logistic_hypothesis\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# 問題１\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_logistic_hypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0my_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2500,3) and (5,) not aligned: 3 (dim 1) != 5 (dim 0)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df[df['target'] == 1].iloc[:, 0], df[df['target'] == 1].iloc[:, 1], color='b', label='1')\n",
    "plt.scatter(df[df['target'] == 2].iloc[:, 0], df[df['target'] == 2].iloc[:, 1], color='r', label='2')\n",
    "plt.legend()\n",
    "\n",
    "x1_min, x1_max = X.iloc[:,0].min(), X.iloc[:,0].max(),\n",
    "x2_min, x2_max = X.iloc[:,1].min(), X.iloc[:,1].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "print(xx1.shape)\n",
    "print(xx2.shape)\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "print(grid.shape)\n",
    "probs = model_scratch.predict_proba(grid).reshape(xx1.shape)\n",
    "print(probs)\n",
    "plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='blue')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# エラー詳細\n",
    "# thetaの次元が入力されたXに対し対応しないので起きているエラーです\n",
    "# __init__、fit、predict、predict_probaにthetaを入れましたがエラーに対応しないです。。。\n",
    "# 更新式や、fitのthetaの位置を変更してもエラーが修正されないので教えてください。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
