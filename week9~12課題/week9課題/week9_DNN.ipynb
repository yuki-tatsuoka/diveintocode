{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.B = SimpleInitializer.B\n",
    "        self.W = SimpleInitializer.W\n",
    "\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        pass\n",
    "    def forward(self, X, batch_size=20):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        #self.batch_size = batch_size \n",
    "        A = X@self.w + self.b #(20, 784) (784, 400) (20, 400)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA, y=None):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = dA@self.w.T # (20, 784) (784, 400)  (20, 400)\n",
    "        dW = dZ.T@dA\n",
    "        dB = np.sum(dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([4, 8, 4, 1, 3, 4, 9, 1, 3, 1, 6, 4, 9, 1, 8, 8, 8, 4, 3, 6],\n",
      "      dtype=uint8))\n"
     ]
    }
   ],
   "source": [
    "get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n",
    "print(len(get_mini_batch)) # 2400\n",
    "print(get_mini_batch[5]) # 5番目のミニバッチが取得できる\n",
    "for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    --------------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(n_nodes1, n_nodes2) #例１層目 (784, 400)\n",
    "        return self.W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        self.B = np.random.randn(n_nodes2)\n",
    "        return self.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.B = SimpleInitializer.B\n",
    "        self.W = SimpleInitializer.W\n",
    "        # ❻lrは親ノードに記載するもの？？\n",
    "        # →感覚的にはそんな気がする\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        self.B -= lr * np.sum(layer, axis=0) # (400,)\n",
    "        self.W -= lr * X.T@layer # (784, 20) (20,400) (784, 400)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイド関数\n",
    "class Sigmoid:\n",
    "    def sigmoid(self, X):\n",
    "        s = 1/(1 + np.exp(-X))\n",
    "        return s\n",
    "    \n",
    "    def backward(self, X):\n",
    "        s_f = (1 - self.sigmoid(X)) * self.sigmoid(X) \n",
    "        return s_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソフトマックス    \n",
    "class SoftMax:\n",
    "    def soft_max(self, X):\n",
    "        # オーバーフロー対策 \n",
    "        X_max = np.max(X) # (20,10)\n",
    "        sft = np.exp(X - X_max)/ np.sum(np.exp(X - X_max), axis=1, keepdims=True)\n",
    "        return sft\n",
    "    \n",
    "    def backward(self, X):\n",
    "        stf_f = self.soft_max(X)*(1-self.soft_max(X))\n",
    "        return sft_f\n",
    "    \n",
    "    def log_loss(self, X, y):\n",
    "        self.loss = -(1/self.batch_size) * np.sum(y*np.log(self.forward(X)))\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパポリックタンジェント\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tanh(self, X):\n",
    "        t = np.tanh(X)\n",
    "        return t\n",
    "    \n",
    "    def backward(self, X):\n",
    "        t_f = 1 - self.tanh(X)**2\n",
    "        return t_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def relu(self, X):\n",
    "        f = np.maximum(0, X)\n",
    "        return f\n",
    "    \n",
    "    def backward(self, X):\n",
    "        f_x = np.where(X>0, 1, 0)\n",
    "        return f_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgmoid、tanhの初期の重み、バイアスはxavierが飽和しにくい\n",
    "import numpy as np\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def Xavier(nodes):\n",
    "        sigma = 1/np.sqrt(nodes)\n",
    "        return sigma\n",
    "\n",
    "# reluの初期の重み、バイアスはheが飽和しにくい\n",
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def he(self, nodes):\n",
    "        sigma = np.sqrt(2/nodes)\n",
    "        return sigma\n",
    "\n",
    "a = XavierInitializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGlad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        pass\n",
    "    \n",
    "    def adaglad(self, dA):\n",
    "        dZ = dA@self.w.T # (20, 784) (784, 400)  (20, 400)\n",
    "        h_W +=  (dZ.T@dA)**2\n",
    "        h_B += np.sum(dA, axis=0)**2\n",
    "        self.W -= lr * 1/(np.sqrt(h_W)) * FC.backward(X)\n",
    "        self.B -= lr * 1/(np.sqrt(h_B)) * FC.backward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 親クラス\n",
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    def __init__(self,n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, batch_size=20, sigma=0.01, lr=0.01, epoch=20,optimizer=\"ada\",activation = \"tanh\",inital = \"xavier\",verbose=False):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.inital = inital\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \n",
    "        if self.optimizer == \"ada\":\n",
    "            optimizer1 = AdaGlad(self.lr)\n",
    "            optimizer2 = AdaGlad(self.lr)\n",
    "            optimizer3 = AdaGlad(self.lr)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            optimizer1 = SGD(self.lr)\n",
    "            optimizer2 = SGD(self.lr)\n",
    "            optimizer3 = SGD(self.lr)\n",
    "        \n",
    "        \n",
    "        if self.activation == \"tanh\":\n",
    "            self.activation1 = Tanh()\n",
    "            self.activation2 = Tanh()\n",
    "            \n",
    "        elif self.activation == \"softmax\":\n",
    "            self.activation1 = SoftMax()\n",
    "            self.activation2 = SoftMax()\n",
    "        elif self.activation == \"relu\":\n",
    "            self.activation1 = ReLU()\n",
    "            self.activation2 = ReLU()        \n",
    "        elif self.activation == \"sigmoid\":\n",
    "            self.activation1 = Sigmoid()\n",
    "            self.activation2 = Sigmoid()   \n",
    "        \n",
    "        if self.inital == \"simple\":\n",
    "            initial = SimpleInitializer(self.sigma)\n",
    "        elif self.inital == \"xavier\":\n",
    "            initial = XavierInitializer()\n",
    "        elif self.inital == \"he\":\n",
    "            initial = He()\n",
    "        \n",
    "        # 3層のみsoftmax\n",
    "        self.activation3 = Softmax(batch_size=self.batch_size)\n",
    "        \n",
    "        # 層の形成\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, initial, optimizer1) # for分の中だと初期化だめ\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initial, optimizer2)\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, initial, optimizer3)\n",
    "        \n",
    "        # one-hot\n",
    "        y_hot = y.reshape(-1, 1) == np.arange(10)\n",
    "        y_val_hot = y_val.reshape(-1,1) == np.arange(10)\n",
    "                \n",
    "        # epoch\n",
    "        for epoch in range(self.epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y_hot, batch_size=20)\n",
    "            train_loss = 0\n",
    "            accuracy_scores = 0\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                train_loss += self.activation3.log_loss(Z3, mini_y_train)\n",
    "                y_pred = self.predict(Z3)\n",
    "                y_true = np.argmax(mini_y_train, axis=1)\n",
    "                accuracy_scores += len(y_pred[y_pred == y_true]) / len(mini_y_train)\n",
    "                \n",
    "                # Z3 = (20,10)\n",
    "                dA3 = self.activation3.backward(mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "            \n",
    "            train_loss / len(get_mini_batch)\n",
    "            accuracy_scores = accuracy_scores / len(get_mini_batch)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"train_loss{}, accuracy{}\".format(train_loss, accuracy_scores))\n",
    "                \n",
    "    def predict(self, Z3):\n",
    "           return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全てのクラスを繋げる\n",
    "class FC:    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initial, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initial = initial\n",
    "        self.W = self.initial.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initial.B(self.n_nodes2)\n",
    "        \n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        pass\n",
    "    def forward(self, X, batch_size=20):\n",
    "        \n",
    "        # self.X = Z2, Z1,mini_X_trainを保存\n",
    "        self.X = X\n",
    "        A = X@self.W + self.B \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        dZ = dA@self.W.T \n",
    "        self.dW = self.X.T@dA \n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        \n",
    "        self.optimizer.update(self) # update(self)のselfはこのクラス（FC）のインスタンスを渡したもの\n",
    "        return dZ\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, lr=0.01):\n",
    "        layer.W -= self.lr * layer.dW# (784, 20) (20,400) (784, 400)        \n",
    "        layer.B -= self.lr * layer.dB # (400,)\n",
    "        return \n",
    "\n",
    "# bais、重みの初期値\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_features, n_nodes1):\n",
    "        W = self.sigma * np.random.randn(n_features, n_nodes1) #例１層目 (784, 400)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes1):\n",
    "        B = self.sigma * np.random.randn(n_nodes1)\n",
    "        return B\n",
    "    \n",
    "# シグモイド関数\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        self.s = 1/(1 + np.exp(-X))\n",
    "        return self.s\n",
    "    \n",
    "    def backward(self, X):\n",
    "        self.s_f = X * (1 - self.s) * self.s\n",
    "        return self.s_f\n",
    "      \n",
    "# ソフトマックス    \n",
    "class Softmax:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # オーバーフロー対策 \n",
    "        X_max = np.max(X) # (20,10)\n",
    "        self.sft = np.exp(X - X_max)/ np.sum(np.exp(X - X_max), axis=1, keepdims=True)\n",
    "        return self.sft\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        self.batch_size = Y.shape[0]\n",
    "        self.sft_f = self.sft - Y \n",
    "        return self.sft_f\n",
    "    \n",
    "    def log_loss(self, Z3, y):\n",
    "        self.loss = (-1/self.batch_size) * np.sum(np.sum(y*np.log(Z3), axis=1)) # self.activation3.soft_max(A3)\n",
    "        return self.loss\n",
    "    \n",
    "# ハイパポリックタンジェント\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, a):\n",
    "        self.t = np.tanh(a)\n",
    "        return self.t\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        self.t_f =  dz*(1 - self.t**2)\n",
    "        return self.t_f\n",
    "\n",
    "#　ReLU\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, a):\n",
    "        self.f = np.maximum(0, a)\n",
    "        return self.f\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        self.f_x = np.where(dZ > 0, 1, 0)\n",
    "        return self.f_x\n",
    "\n",
    "    \n",
    "# sgmoid、tanhの初期の重み、バイアスはxavierが飽和しにくい\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def W(self, n_features, n_nodes1):\n",
    "        W =  np.random.randn(n_features, n_nodes1) / np.sqrt(n_nodes1) #例１層目 (784, 400)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes1):\n",
    "        B = np.random.randn(n_nodes1) / np.sqrt(n_nodes1)\n",
    "        return B\n",
    "\n",
    "# reluの初期の重み、バイアスはheが飽和しにくい\n",
    "class He:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def W(self, n_features, n_nodes1):\n",
    "        W = np.random.randn(n_features, n_nodes1) / np.sqrt(2*n_nodes1) # 例１層目 (784, 400)\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes1):\n",
    "        B = np.random.randn(n_nodes1) / np.sqrt(2*n_nodes1)\n",
    "        return B\n",
    "        \n",
    "# adagrad\n",
    "class AdaGlad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.h_W = 1\n",
    "        self.h_B = 1\n",
    "        \n",
    "    def update(self, layer):\n",
    "        self.h_W +=  layer.dW**2\n",
    "        self.h_B += layer.dB**2\n",
    "        layer.W -= self.lr * 1/(np.sqrt(self.h_W)) * layer.dW\n",
    "        layer.B -= self.lr * 1/(np.sqrt(self.h_B)) * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題9】学習と推定\n",
    "\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# DL\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 画像データを2次元に変換\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "\n",
    "# ワンホットに修正\n",
    "y_train_hot = y_train.reshape(-1,1) == np.arange(10)\n",
    "y_test_hot = y_test.reshape(-1,1) == np.arange(10)\n",
    "\n",
    "# 0~!の数値に修正\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False,  True, False]])"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train.reshape(-1,1) == np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52943387,  0.54564183, -0.50442022],\n",
       "       [ 0.44532683,  0.01008582, -0.65516497]])"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2,3)/ np.sqrt(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 2 1 1 2 2 1 1]\n",
      "[1 1 1 2 1 2 2 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.random.randint(1,3,10)\n",
    "mini_y_train =np.random.randint(1,3,10)\n",
    "print(y_pred)\n",
    "print(mini_y_train)\n",
    "y_pred[y_pred == mini_y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss659.4967207980785, accuracy0.9388833333333314\n",
      "train_loss278.73020152344094, accuracy0.9731499999999941\n",
      "train_loss206.52246605024857, accuracy0.9810833333333271\n",
      "train_loss165.91392845343154, accuracy0.9853333333333276\n",
      "train_loss138.6810199358271, accuracy0.9884999999999945\n",
      "train_loss118.82269499456542, accuracy0.9904666666666614\n",
      "train_loss103.5787296765123, accuracy0.9921833333333286\n",
      "train_loss91.46029831671515, accuracy0.9935166666666629\n",
      "train_loss81.58268237363603, accuracy0.9946999999999968\n",
      "train_loss73.38661357645425, accuracy0.9956499999999973\n",
      "train_loss66.49495336629286, accuracy0.9965166666666647\n",
      "train_loss60.6383622697975, accuracy0.9972499999999983\n",
      "train_loss55.61518581732014, accuracy0.9978333333333322\n",
      "train_loss51.26774053404802, accuracy0.998216666666666\n",
      "train_loss47.47284589264382, accuracy0.9984166666666664\n",
      "train_loss44.13419429572302, accuracy0.9986833333333329\n",
      "train_loss41.17568286181387, accuracy0.9988166666666664\n",
      "train_loss38.53696922321123, accuracy0.9989499999999997\n",
      "train_loss36.17005547826806, accuracy0.9990999999999998\n",
      "train_loss34.03644924641583, accuracy0.9991999999999999\n"
     ]
    }
   ],
   "source": [
    "nn = ScratchDeepNeuralNetrowkClassifier(n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, batch_size=20, sigma=0.01, lr=0.01, epoch=20,optimizer=\"ada\",activation = \"tanh\",inital = \"xavier\",verbose=True)\n",
    "nn.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#sigmoid ◯\n",
    "#softmax ◯\n",
    "#relu ◯\n",
    "#tanh ◯\n",
    "# xavier ◯\n",
    "# he ◯\n",
    "# simple　◯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
