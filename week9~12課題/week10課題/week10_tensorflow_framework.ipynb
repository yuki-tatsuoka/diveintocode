{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "（例）<br>\n",
    "重みを初期化する必要があった<br>\n",
    "エポックのループが必要だった<br>\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iysZBwln_1kH"
   },
   "outputs": [],
   "source": [
    "# 問題1\n",
    "# エポック、イテレーション、バッチサイズ\n",
    "# 重みとバイアス\n",
    "# 重みとバイアスの初期化\n",
    "# 活性化関数\n",
    "# forward\n",
    "# アルゴリズム\n",
    "# loss\n",
    "# backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1D0UUUsk_use",
    "outputId": "1ccbfe38-c597-4d95-d454-7a1f9d636170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
      "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 49.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 50.0MB/s \n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (54.0.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=c82aa7b364bccede30f456976d1d50834ef820b1f9d2ae60dd8a97c037530563\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow\n",
      "  Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "  Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Found existing installation: tensorflow 2.4.1\n",
      "    Uninstalling tensorflow-2.4.1:\n",
      "      Successfully uninstalled tensorflow-2.4.1\n",
      "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "gast",
         "tensorboard",
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1系準備\n",
    "!pip install --upgrade tensorflow==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hoBiqCvBPJe",
    "outputId": "e27c7de8-3fd9-4a05-e12b-852a8f5a8055"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50     Iris-versicolor\n",
       "51     Iris-versicolor\n",
       "52     Iris-versicolor\n",
       "53     Iris-versicolor\n",
       "54     Iris-versicolor\n",
       "            ...       \n",
       "145     Iris-virginica\n",
       "146     Iris-virginica\n",
       "147     Iris-virginica\n",
       "148     Iris-virginica\n",
       "149     Iris-virginica\n",
       "Name: Species, Length: 100, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "iris = pd.read_csv(\"Iris.csv\")\n",
    "iris[iris[\"Species\"] == \"Iris-versicolor\"]\n",
    "iris[iris[\"Species\"] == \"Iris-virginica\"]\n",
    "\n",
    "iris2 = iris[\"Species\"][50:]\n",
    "iris2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsXx_akkCQVl",
    "outputId": "b0bf70c5-e76e-43fb-dbce-a5907cdb340e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 6.0437, val_loss : 25.4480, acc : 0.625\n",
      "Epoch 1, loss : 2.4252, val_loss : 5.6690, acc : 0.312\n",
      "Epoch 2, loss : 1.2547, val_loss : 17.3184, acc : 0.375\n",
      "Epoch 3, loss : 1.2396, val_loss : 10.4903, acc : 0.375\n",
      "Epoch 4, loss : 0.5408, val_loss : 2.5693, acc : 0.562\n",
      "Epoch 5, loss : 0.4709, val_loss : 2.3007, acc : 0.438\n",
      "Epoch 6, loss : 0.3533, val_loss : 2.3549, acc : 0.438\n",
      "Epoch 7, loss : 0.1935, val_loss : 0.4953, acc : 0.750\n",
      "Epoch 8, loss : 0.1442, val_loss : 1.0494, acc : 0.562\n",
      "Epoch 9, loss : 0.0750, val_loss : 0.1496, acc : 0.938\n",
      "Epoch 10, loss : 0.0582, val_loss : 0.3923, acc : 0.812\n",
      "Epoch 11, loss : 0.0349, val_loss : 0.0535, acc : 1.000\n",
      "Epoch 12, loss : 0.0318, val_loss : 0.0843, acc : 1.000\n",
      "Epoch 13, loss : 0.0249, val_loss : 0.0468, acc : 1.000\n",
      "Epoch 14, loss : 0.0197, val_loss : 0.0371, acc : 1.000\n",
      "Epoch 15, loss : 0.0174, val_loss : 0.0413, acc : 1.000\n",
      "Epoch 16, loss : 0.0140, val_loss : 0.0388, acc : 1.000\n",
      "Epoch 17, loss : 0.0120, val_loss : 0.0427, acc : 1.000\n",
      "Epoch 18, loss : 0.0106, val_loss : 0.0463, acc : 1.000\n",
      "Epoch 19, loss : 0.0090, val_loss : 0.0511, acc : 1.000\n",
      "Epoch 20, loss : 0.0083, val_loss : 0.0541, acc : 1.000\n",
      "Epoch 21, loss : 0.0075, val_loss : 0.0567, acc : 1.000\n",
      "Epoch 22, loss : 0.0069, val_loss : 0.0591, acc : 1.000\n",
      "Epoch 23, loss : 0.0065, val_loss : 0.0602, acc : 1.000\n",
      "Epoch 24, loss : 0.0060, val_loss : 0.0614, acc : 1.000\n",
      "Epoch 25, loss : 0.0057, val_loss : 0.0625, acc : 1.000\n",
      "Epoch 26, loss : 0.0054, val_loss : 0.0634, acc : 1.000\n",
      "Epoch 27, loss : 0.0052, val_loss : 0.0633, acc : 1.000\n",
      "Epoch 28, loss : 0.0050, val_loss : 0.0626, acc : 1.000\n",
      "Epoch 29, loss : 0.0048, val_loss : 0.0620, acc : 1.000\n",
      "Epoch 30, loss : 0.0046, val_loss : 0.0615, acc : 1.000\n",
      "Epoch 31, loss : 0.0045, val_loss : 0.0610, acc : 1.000\n",
      "Epoch 32, loss : 0.0043, val_loss : 0.0599, acc : 1.000\n",
      "Epoch 33, loss : 0.0042, val_loss : 0.0584, acc : 1.000\n",
      "Epoch 34, loss : 0.0041, val_loss : 0.0574, acc : 1.000\n",
      "Epoch 35, loss : 0.0040, val_loss : 0.0566, acc : 1.000\n",
      "Epoch 36, loss : 0.0039, val_loss : 0.0558, acc : 1.000\n",
      "Epoch 37, loss : 0.0038, val_loss : 0.0552, acc : 1.000\n",
      "Epoch 38, loss : 0.0038, val_loss : 0.0545, acc : 1.000\n",
      "Epoch 39, loss : 0.0037, val_loss : 0.0538, acc : 1.000\n",
      "Epoch 40, loss : 0.0036, val_loss : 0.0529, acc : 1.000\n",
      "Epoch 41, loss : 0.0035, val_loss : 0.0517, acc : 1.000\n",
      "Epoch 42, loss : 0.0035, val_loss : 0.0508, acc : 1.000\n",
      "Epoch 43, loss : 0.0034, val_loss : 0.0502, acc : 1.000\n",
      "Epoch 44, loss : 0.0034, val_loss : 0.0496, acc : 1.000\n",
      "Epoch 45, loss : 0.0033, val_loss : 0.0490, acc : 1.000\n",
      "Epoch 46, loss : 0.0033, val_loss : 0.0485, acc : 1.000\n",
      "Epoch 47, loss : 0.0032, val_loss : 0.0478, acc : 1.000\n",
      "Epoch 48, loss : 0.0032, val_loss : 0.0472, acc : 1.000\n",
      "Epoch 49, loss : 0.0031, val_loss : 0.0466, acc : 1.000\n",
      "Epoch 50, loss : 0.0031, val_loss : 0.0460, acc : 1.000\n",
      "Epoch 51, loss : 0.0030, val_loss : 0.0452, acc : 1.000\n",
      "Epoch 52, loss : 0.0030, val_loss : 0.0441, acc : 1.000\n",
      "Epoch 53, loss : 0.0029, val_loss : 0.0434, acc : 1.000\n",
      "Epoch 54, loss : 0.0029, val_loss : 0.0428, acc : 1.000\n",
      "Epoch 55, loss : 0.0029, val_loss : 0.0423, acc : 1.000\n",
      "Epoch 56, loss : 0.0028, val_loss : 0.0418, acc : 1.000\n",
      "Epoch 57, loss : 0.0028, val_loss : 0.0413, acc : 1.000\n",
      "Epoch 58, loss : 0.0027, val_loss : 0.0408, acc : 1.000\n",
      "Epoch 59, loss : 0.0027, val_loss : 0.0404, acc : 1.000\n",
      "Epoch 60, loss : 0.0027, val_loss : 0.0399, acc : 1.000\n",
      "Epoch 61, loss : 0.0026, val_loss : 0.0393, acc : 1.000\n",
      "Epoch 62, loss : 0.0026, val_loss : 0.0389, acc : 1.000\n",
      "Epoch 63, loss : 0.0026, val_loss : 0.0384, acc : 1.000\n",
      "Epoch 64, loss : 0.0025, val_loss : 0.0379, acc : 1.000\n",
      "Epoch 65, loss : 0.0025, val_loss : 0.0373, acc : 1.000\n",
      "Epoch 66, loss : 0.0025, val_loss : 0.0365, acc : 1.000\n",
      "Epoch 67, loss : 0.0025, val_loss : 0.0359, acc : 1.000\n",
      "Epoch 68, loss : 0.0024, val_loss : 0.0355, acc : 1.000\n",
      "Epoch 69, loss : 0.0024, val_loss : 0.0351, acc : 1.000\n",
      "Epoch 70, loss : 0.0024, val_loss : 0.0347, acc : 1.000\n",
      "Epoch 71, loss : 0.0024, val_loss : 0.0344, acc : 1.000\n",
      "Epoch 72, loss : 0.0023, val_loss : 0.0340, acc : 1.000\n",
      "Epoch 73, loss : 0.0023, val_loss : 0.0336, acc : 1.000\n",
      "Epoch 74, loss : 0.0023, val_loss : 0.0333, acc : 1.000\n",
      "Epoch 75, loss : 0.0022, val_loss : 0.0328, acc : 1.000\n",
      "Epoch 76, loss : 0.0022, val_loss : 0.0324, acc : 1.000\n",
      "Epoch 77, loss : 0.0022, val_loss : 0.0321, acc : 1.000\n",
      "Epoch 78, loss : 0.0022, val_loss : 0.0317, acc : 1.000\n",
      "Epoch 79, loss : 0.0022, val_loss : 0.0313, acc : 1.000\n",
      "Epoch 80, loss : 0.0021, val_loss : 0.0309, acc : 1.000\n",
      "Epoch 81, loss : 0.0021, val_loss : 0.0303, acc : 1.000\n",
      "Epoch 82, loss : 0.0021, val_loss : 0.0299, acc : 1.000\n",
      "Epoch 83, loss : 0.0021, val_loss : 0.0296, acc : 1.000\n",
      "Epoch 84, loss : 0.0021, val_loss : 0.0294, acc : 1.000\n",
      "Epoch 85, loss : 0.0020, val_loss : 0.0292, acc : 1.000\n",
      "Epoch 86, loss : 0.0020, val_loss : 0.0290, acc : 1.000\n",
      "Epoch 87, loss : 0.0020, val_loss : 0.0288, acc : 1.000\n",
      "Epoch 88, loss : 0.0020, val_loss : 0.0285, acc : 1.000\n",
      "Epoch 89, loss : 0.0020, val_loss : 0.0283, acc : 1.000\n",
      "Epoch 90, loss : 0.0020, val_loss : 0.0281, acc : 1.000\n",
      "Epoch 91, loss : 0.0020, val_loss : 0.0279, acc : 1.000\n",
      "Epoch 92, loss : 0.0019, val_loss : 0.0277, acc : 1.000\n",
      "Epoch 93, loss : 0.0019, val_loss : 0.0274, acc : 1.000\n",
      "Epoch 94, loss : 0.0019, val_loss : 0.0271, acc : 1.000\n",
      "Epoch 95, loss : 0.0019, val_loss : 0.0266, acc : 1.000\n",
      "Epoch 96, loss : 0.0019, val_loss : 0.0263, acc : 1.000\n",
      "Epoch 97, loss : 0.0019, val_loss : 0.0261, acc : 1.000\n",
      "Epoch 98, loss : 0.0019, val_loss : 0.0259, acc : 1.000\n",
      "Epoch 99, loss : 0.0018, val_loss : 0.0258, acc : 1.000\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# ラベルを数値に変換\n",
    "y[y == \"Iris-versicolor\"] = 0\n",
    "y[y == \"Iris-virginica\"] = 1\n",
    "y = y.astype(np.int64)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits：クロスエントロピーの値\n",
    "# Y = (n, n_class)\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op) #?????\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "  \n",
    "# エポック、イテレーション、バッチサイズ\n",
    "# ⇨エポック： for epoch in range(num_epochs):\n",
    "# ⇨イテレーション：get_mini_batch_train\n",
    "# ⇨バッチサイズ：total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "# ほぼNNと同じ\n",
    "\n",
    "# 重みとバイアス\n",
    "# ⇨重み　:　tf.Variable(tf.random_normal([n_input, n_hidden1]))\n",
    "# ⇨バイアス：tf.Variable(tf.random_normal([n_hidden1]))\n",
    "# NNとは違いtf.Variableを使っているが、基本は同じ。\n",
    "# numpyでもnp.random.randn(n_input, n_hidden1)と同じ。\n",
    "\n",
    "# 重みとバイアスの初期化\n",
    "# ⇨初期化：tf.global_variables_initializer()\n",
    "# NNでは最初に上記のnp.random.randn(n_input, n_hidden1)で求める\n",
    "# テンソルでは短くて便利\n",
    "\n",
    "# 活性化関数\n",
    "# ⇨tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# クロスエントロピーとかぶっているがほとんど同じ。\n",
    "\n",
    "# forward\n",
    "# ⇨sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "# NNと比べて非常に楽。\n",
    "# train_opがAdamなので勾配計算のアルゴリズムをエポック分回せば学習が可能な点が非常に強い\n",
    "\n",
    "# アルゴリズム\n",
    "# ⇨optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# NNとほとんど差はない。\n",
    "\n",
    "# loss\n",
    "# ⇨oss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# lossでは、基本y_pred/y_trueの式だがtfでは平均でlossを取っている\n",
    "\n",
    "# backward !!!!!!!!!!!!!!!!!!!!!!!!!!!▽▽黒●\n",
    "# ⇨train_op = optimizer.minimize(loss_op) \n",
    "# バックワードもlossを入れて簡単に処理している\n",
    "\n",
    "# 全体的な意見\n",
    "# スクラッチと比べコード量が大幅に少なくなって、とても使いやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbYH3xBCCiEZ"
   },
   "source": [
    "# 【問題3】3種類すべての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類すべてを分類できるモデルを作成してください。\n",
    "\n",
    "\n",
    "Iris Species\n",
    "\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYb0XWKwQImV",
    "outputId": "82ab08eb-c9de-4c2a-f2d3-bad47dfa33a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 問題3\n",
    "# ワンほっとを使っていく\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "y = iris.loc[:,\"Species\"]\n",
    "X = iris.iloc[:, 1:5]\n",
    "\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# ラベルを数値に変換\n",
    "y[y == \"Iris-setosa\"] = 0 \n",
    "y[y == \"Iris-versicolor\"] = 1 \n",
    "y[y == \"Iris-virginica\"] = 2 \n",
    "\n",
    "# one-hotのためにreshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# hot\n",
    "scaler = OneHotEncoder()\n",
    "y_hot = scaler.fit_transform(y).toarray()\n",
    "print(y_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "OTPcV_lXchiq"
   },
   "outputs": [],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_hot, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1QAqWT2yj6Y",
    "outputId": "17ce061e-b974-4562-8c7d-0904e1dd3319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 29.8890, val_loss : 268.8247, acc : 0.292\n",
      "Epoch 1, loss : 22.0287, val_loss : 189.8217, acc : 0.292\n",
      "Epoch 2, loss : 14.4276, val_loss : 112.7800, acc : 0.292\n",
      "Epoch 3, loss : 7.0381, val_loss : 40.8063, acc : 0.458\n",
      "Epoch 4, loss : 2.6262, val_loss : 27.7799, acc : 0.375\n",
      "Epoch 5, loss : 0.9484, val_loss : 13.8942, acc : 0.708\n",
      "Epoch 6, loss : 0.4914, val_loss : 6.8456, acc : 0.708\n",
      "Epoch 7, loss : 0.1057, val_loss : 3.5524, acc : 0.792\n",
      "Epoch 8, loss : 0.0637, val_loss : 2.8730, acc : 0.833\n",
      "Epoch 9, loss : 0.0694, val_loss : 2.9574, acc : 0.833\n",
      "Epoch 10, loss : 0.0650, val_loss : 3.2462, acc : 0.875\n",
      "Epoch 11, loss : 0.0616, val_loss : 3.2697, acc : 0.875\n",
      "Epoch 12, loss : 0.0594, val_loss : 3.1311, acc : 0.875\n",
      "Epoch 13, loss : 0.0586, val_loss : 3.0505, acc : 0.875\n",
      "Epoch 14, loss : 0.0579, val_loss : 3.0476, acc : 0.875\n",
      "Epoch 15, loss : 0.0567, val_loss : 3.0499, acc : 0.875\n",
      "Epoch 16, loss : 0.0555, val_loss : 3.0033, acc : 0.875\n",
      "Epoch 17, loss : 0.0545, val_loss : 2.9387, acc : 0.875\n",
      "Epoch 18, loss : 0.0537, val_loss : 2.8938, acc : 0.875\n",
      "Epoch 19, loss : 0.0529, val_loss : 2.8644, acc : 0.875\n",
      "Epoch 20, loss : 0.0519, val_loss : 2.8287, acc : 0.875\n",
      "Epoch 21, loss : 0.0510, val_loss : 2.7888, acc : 0.875\n",
      "Epoch 22, loss : 0.0501, val_loss : 2.7533, acc : 0.875\n",
      "Epoch 23, loss : 0.0493, val_loss : 2.7198, acc : 0.875\n",
      "Epoch 24, loss : 0.0484, val_loss : 2.6844, acc : 0.875\n",
      "Epoch 25, loss : 0.0475, val_loss : 2.6492, acc : 0.875\n",
      "Epoch 26, loss : 0.0466, val_loss : 2.6148, acc : 0.875\n",
      "Epoch 27, loss : 0.0457, val_loss : 2.5821, acc : 0.875\n",
      "Epoch 28, loss : 0.0447, val_loss : 2.5473, acc : 0.875\n",
      "Epoch 29, loss : 0.0438, val_loss : 2.5148, acc : 0.875\n",
      "Epoch 30, loss : 0.0429, val_loss : 2.4854, acc : 0.875\n",
      "Epoch 31, loss : 0.0420, val_loss : 2.4549, acc : 0.875\n",
      "Epoch 32, loss : 0.0411, val_loss : 2.4298, acc : 0.875\n",
      "Epoch 33, loss : 0.0401, val_loss : 2.3944, acc : 0.875\n",
      "Epoch 34, loss : 0.0392, val_loss : 2.3672, acc : 0.875\n",
      "Epoch 35, loss : 0.0384, val_loss : 2.3547, acc : 0.917\n",
      "Epoch 36, loss : 0.0375, val_loss : 2.3385, acc : 0.917\n",
      "Epoch 37, loss : 0.0367, val_loss : 2.3254, acc : 0.917\n",
      "Epoch 38, loss : 0.0359, val_loss : 2.3163, acc : 0.917\n",
      "Epoch 39, loss : 0.0351, val_loss : 2.3065, acc : 0.917\n",
      "Epoch 40, loss : 0.0344, val_loss : 2.3018, acc : 0.917\n",
      "Epoch 41, loss : 0.0337, val_loss : 2.3006, acc : 0.917\n",
      "Epoch 42, loss : 0.0331, val_loss : 2.2996, acc : 0.917\n",
      "Epoch 43, loss : 0.0324, val_loss : 2.3001, acc : 0.917\n",
      "Epoch 44, loss : 0.0318, val_loss : 2.3021, acc : 0.917\n",
      "Epoch 45, loss : 0.0312, val_loss : 2.3045, acc : 0.917\n",
      "Epoch 46, loss : 0.0306, val_loss : 2.3082, acc : 0.917\n",
      "Epoch 47, loss : 0.0301, val_loss : 2.3110, acc : 0.917\n",
      "Epoch 48, loss : 0.0295, val_loss : 2.3099, acc : 0.917\n",
      "Epoch 49, loss : 0.0289, val_loss : 2.3131, acc : 0.917\n",
      "Epoch 50, loss : 0.0284, val_loss : 2.3177, acc : 0.917\n",
      "Epoch 51, loss : 0.0278, val_loss : 2.3197, acc : 0.917\n",
      "Epoch 52, loss : 0.0273, val_loss : 2.3199, acc : 0.917\n",
      "Epoch 53, loss : 0.0267, val_loss : 2.3173, acc : 0.917\n",
      "Epoch 54, loss : 0.0262, val_loss : 2.3097, acc : 0.917\n",
      "Epoch 55, loss : 0.0257, val_loss : 2.3022, acc : 0.917\n",
      "Epoch 56, loss : 0.0252, val_loss : 2.2935, acc : 0.917\n",
      "Epoch 57, loss : 0.0247, val_loss : 2.2811, acc : 0.917\n",
      "Epoch 58, loss : 0.0241, val_loss : 2.2579, acc : 0.917\n",
      "Epoch 59, loss : 0.0238, val_loss : 2.2446, acc : 0.917\n",
      "Epoch 60, loss : 0.0234, val_loss : 2.2293, acc : 0.917\n",
      "Epoch 61, loss : 0.0229, val_loss : 2.2084, acc : 0.917\n",
      "Epoch 62, loss : 0.0225, val_loss : 2.1907, acc : 0.917\n",
      "Epoch 63, loss : 0.0220, val_loss : 2.1735, acc : 0.917\n",
      "Epoch 64, loss : 0.0215, val_loss : 2.1454, acc : 0.917\n",
      "Epoch 65, loss : 0.0213, val_loss : 2.1318, acc : 0.917\n",
      "Epoch 66, loss : 0.0208, val_loss : 2.1137, acc : 0.917\n",
      "Epoch 67, loss : 0.0204, val_loss : 2.0924, acc : 0.917\n",
      "Epoch 68, loss : 0.0200, val_loss : 2.0753, acc : 0.917\n",
      "Epoch 69, loss : 0.0195, val_loss : 2.0478, acc : 0.917\n",
      "Epoch 70, loss : 0.0193, val_loss : 2.0423, acc : 0.917\n",
      "Epoch 71, loss : 0.0188, val_loss : 2.0186, acc : 0.917\n",
      "Epoch 72, loss : 0.0184, val_loss : 2.0007, acc : 0.917\n",
      "Epoch 73, loss : 0.0178, val_loss : 1.9736, acc : 0.917\n",
      "Epoch 74, loss : 0.0178, val_loss : 1.9819, acc : 0.917\n",
      "Epoch 75, loss : 0.0172, val_loss : 1.9425, acc : 0.917\n",
      "Epoch 76, loss : 0.0169, val_loss : 1.9389, acc : 0.917\n",
      "Epoch 77, loss : 0.0163, val_loss : 1.8993, acc : 0.917\n",
      "Epoch 78, loss : 0.0164, val_loss : 1.9280, acc : 0.917\n",
      "Epoch 79, loss : 0.0156, val_loss : 1.8585, acc : 0.917\n",
      "Epoch 80, loss : 0.0155, val_loss : 1.8852, acc : 0.917\n",
      "Epoch 81, loss : 0.0151, val_loss : 1.8493, acc : 0.917\n",
      "Epoch 82, loss : 0.0148, val_loss : 1.8450, acc : 0.917\n",
      "Epoch 83, loss : 0.0142, val_loss : 1.7962, acc : 0.917\n",
      "Epoch 84, loss : 0.0143, val_loss : 1.8433, acc : 0.917\n",
      "Epoch 85, loss : 0.0135, val_loss : 1.7489, acc : 0.917\n",
      "Epoch 86, loss : 0.0139, val_loss : 1.8599, acc : 0.917\n",
      "Epoch 87, loss : 0.0126, val_loss : 1.6702, acc : 0.917\n",
      "Epoch 88, loss : 0.0138, val_loss : 1.9072, acc : 0.917\n",
      "Epoch 89, loss : 0.0111, val_loss : 1.4587, acc : 0.917\n",
      "Epoch 90, loss : 0.0162, val_loss : 2.1416, acc : 0.917\n",
      "Epoch 91, loss : 0.0147, val_loss : 1.3703, acc : 0.917\n",
      "Epoch 92, loss : 0.0176, val_loss : 2.1544, acc : 0.917\n",
      "Epoch 93, loss : 0.0163, val_loss : 1.4600, acc : 0.917\n",
      "Epoch 94, loss : 0.0162, val_loss : 1.9417, acc : 0.917\n",
      "Epoch 95, loss : 0.0097, val_loss : 1.2191, acc : 0.917\n",
      "Epoch 96, loss : 0.0172, val_loss : 2.1249, acc : 0.917\n",
      "Epoch 97, loss : 0.0141, val_loss : 1.3908, acc : 0.917\n",
      "Epoch 98, loss : 0.0139, val_loss : 1.9198, acc : 0.917\n",
      "Epoch 99, loss : 0.0074, val_loss : 1.1162, acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits：クロスエントロピーの値\n",
    "# Y = (n, n_class)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "a = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)\n",
    "#b = tf.argmax(tf.nn.softmax(logits), axis=1)\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op) \n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(tf.argmax(Y, axis=1) - 1), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1) - 1))\n",
    "\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            # print(mini_batch_y)\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aq = sess.run(a, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bq = sess.run(b, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #yq = sess.run(Y, feed_dict={Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        #print(aq)\n",
    "        #print(aq.shape)\n",
    "        #print(yq)\n",
    "        #print(bq)\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "\n",
    "# 考察\n",
    "# 変更点は２箇所\n",
    "# f.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# tf.sign(tf.argmax(Y, axis=1) - 1), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1) - 1))\n",
    "# 2値分類から多クラス分類が必要なので、多クラス分類を探した結果、ソフトマックス関数とセットのものを発見（厳密にはsigmoidではセットで作られた関数がなかった）\n",
    "# 多クラスになったので、argmaxで最大のインデックスを所得して差し引くことで整合を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "\n",
    "House Prices: Advanced Regression Techniques\n",
    "\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2k9OsUQe53q0",
    "outputId": "23d75a86-9529-4405-c299-6fe9d3a2ebc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((934, 2), (934, 1))"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/train.csv2\")\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)[:, np.newaxis]\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# 対数変換\n",
    "y_log = np.log(y)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y_log, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lw7taUNBBoJ4",
    "outputId": "dd3bff68-97a5-40e5-b3f9-b70d4d3d6763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 153.7556497708633, val_loss : 431.3733215332031\n",
      "Epoch 1, loss : 22.1956622309552, val_loss : 99.78850555419922\n",
      "Epoch 2, loss : 7.104445298172422, val_loss : 45.34308624267578\n",
      "Epoch 3, loss : 3.5767147122409666, val_loss : 26.44932746887207\n",
      "Epoch 4, loss : 2.2332179395749177, val_loss : 18.665477752685547\n",
      "Epoch 5, loss : 1.6344907493101197, val_loss : 14.6545991897583\n",
      "Epoch 6, loss : 1.303751101330465, val_loss : 12.17841625213623\n",
      "Epoch 7, loss : 1.0963042927247808, val_loss : 10.391828536987305\n",
      "Epoch 8, loss : 0.9405969475780871, val_loss : 8.962387084960938\n",
      "Epoch 9, loss : 0.8114712729699096, val_loss : 7.748530387878418\n",
      "Epoch 10, loss : 0.7036697711372784, val_loss : 6.726406097412109\n",
      "Epoch 11, loss : 0.614531769997558, val_loss : 5.882800579071045\n",
      "Epoch 12, loss : 0.5393752776070376, val_loss : 5.183596611022949\n",
      "Epoch 13, loss : 0.47532694434200673, val_loss : 4.599796772003174\n",
      "Epoch 14, loss : 0.42085585859654034, val_loss : 4.105802059173584\n",
      "Epoch 15, loss : 0.3746441612917555, val_loss : 3.6944282054901123\n",
      "Epoch 16, loss : 0.33464247703041716, val_loss : 3.354597568511963\n",
      "Epoch 17, loss : 0.30168935458716284, val_loss : 3.0660133361816406\n",
      "Epoch 18, loss : 0.2729581590638171, val_loss : 2.8288519382476807\n",
      "Epoch 19, loss : 0.24888983898550845, val_loss : 2.616272211074829\n",
      "Epoch 20, loss : 0.22806308464779332, val_loss : 2.438004493713379\n",
      "Epoch 21, loss : 0.21038815143021578, val_loss : 2.272763729095459\n",
      "Epoch 22, loss : 0.1947195741808644, val_loss : 2.1308462619781494\n",
      "Epoch 23, loss : 0.18115904659224272, val_loss : 2.0007543563842773\n",
      "Epoch 24, loss : 0.16921926821069494, val_loss : 1.8845354318618774\n",
      "Epoch 25, loss : 0.15873882034095502, val_loss : 1.7784279584884644\n",
      "Epoch 26, loss : 0.1491890430641889, val_loss : 1.6816645860671997\n",
      "Epoch 27, loss : 0.1407347533381215, val_loss : 1.5967118740081787\n",
      "Epoch 28, loss : 0.13294343746065837, val_loss : 1.5159634351730347\n",
      "Epoch 29, loss : 0.1258231855817219, val_loss : 1.4415104389190674\n",
      "Epoch 30, loss : 0.1193785587010598, val_loss : 1.369976282119751\n",
      "Epoch 31, loss : 0.11339717691792155, val_loss : 1.2988741397857666\n",
      "Epoch 32, loss : 0.10769080279726033, val_loss : 1.2364721298217773\n",
      "Epoch 33, loss : 0.10265822912819636, val_loss : 1.1797499656677246\n",
      "Epoch 34, loss : 0.09811732744124002, val_loss : 1.1287001371383667\n",
      "Epoch 35, loss : 0.09389734016113159, val_loss : 1.08049738407135\n",
      "Epoch 36, loss : 0.08972454045518308, val_loss : 1.0318797826766968\n",
      "Epoch 37, loss : 0.08577692457644281, val_loss : 0.9865620732307434\n",
      "Epoch 38, loss : 0.08205430512765235, val_loss : 0.9477518200874329\n",
      "Epoch 39, loss : 0.07854216965413961, val_loss : 0.9067023992538452\n",
      "Epoch 40, loss : 0.07541650425188036, val_loss : 0.8727840185165405\n",
      "Epoch 41, loss : 0.07251058057812092, val_loss : 0.8395025730133057\n",
      "Epoch 42, loss : 0.06986670208819673, val_loss : 0.8111429810523987\n",
      "Epoch 43, loss : 0.06739845193874658, val_loss : 0.7839604020118713\n",
      "Epoch 44, loss : 0.06495971948993283, val_loss : 0.7601845264434814\n",
      "Epoch 45, loss : 0.06280876362668371, val_loss : 0.7384514808654785\n",
      "Epoch 46, loss : 0.06069515849716914, val_loss : 0.7183681130409241\n",
      "Epoch 47, loss : 0.05863137771345563, val_loss : 0.7004563808441162\n",
      "Epoch 48, loss : 0.056762575145962404, val_loss : 0.6862663626670837\n",
      "Epoch 49, loss : 0.054917465648125274, val_loss : 0.6731256246566772\n",
      "Epoch 50, loss : 0.052932355832109104, val_loss : 0.6620654463768005\n",
      "Epoch 51, loss : 0.05109566901621012, val_loss : 0.6491867899894714\n",
      "Epoch 52, loss : 0.04951862909490087, val_loss : 0.6368139386177063\n",
      "Epoch 53, loss : 0.04764919812597605, val_loss : 0.62400221824646\n",
      "Epoch 54, loss : 0.046133661327576486, val_loss : 0.6097691059112549\n",
      "Epoch 55, loss : 0.04471649649120245, val_loss : 0.5965082049369812\n",
      "Epoch 56, loss : 0.04344020432910266, val_loss : 0.5850494503974915\n",
      "Epoch 57, loss : 0.042282684345189205, val_loss : 0.5718541145324707\n",
      "Epoch 58, loss : 0.04098890423455637, val_loss : 0.5601436495780945\n",
      "Epoch 59, loss : 0.0398510088645544, val_loss : 0.5477749705314636\n",
      "Epoch 60, loss : 0.038675874248306555, val_loss : 0.5364952683448792\n",
      "Epoch 61, loss : 0.03787017400661679, val_loss : 0.5274581909179688\n",
      "Epoch 62, loss : 0.03665594895404207, val_loss : 0.5185543298721313\n",
      "Epoch 63, loss : 0.03583505009430634, val_loss : 0.5126011371612549\n",
      "Epoch 64, loss : 0.03497621736925981, val_loss : 0.5065716505050659\n",
      "Epoch 65, loss : 0.0342228739368839, val_loss : 0.5044907927513123\n",
      "Epoch 66, loss : 0.03351893741836895, val_loss : 0.5022346377372742\n",
      "Epoch 67, loss : 0.03287161614674062, val_loss : 0.5009035468101501\n",
      "Epoch 68, loss : 0.03224159577737784, val_loss : 0.4986823499202728\n",
      "Epoch 69, loss : 0.03178293401538943, val_loss : 0.49689820408821106\n",
      "Epoch 70, loss : 0.03131932498811399, val_loss : 0.4938803017139435\n",
      "Epoch 71, loss : 0.031354635515164514, val_loss : 0.49461373686790466\n",
      "Epoch 72, loss : 0.030789199458710385, val_loss : 0.47147828340530396\n",
      "Epoch 73, loss : 0.03026541526682627, val_loss : 0.47116541862487793\n",
      "Epoch 74, loss : 0.030032875961614115, val_loss : 0.46356484293937683\n",
      "Epoch 75, loss : 0.029732873069014742, val_loss : 0.4615592062473297\n",
      "Epoch 76, loss : 0.029287267627884542, val_loss : 0.4611334800720215\n",
      "Epoch 77, loss : 0.029077588395081955, val_loss : 0.4537617266178131\n",
      "Epoch 78, loss : 0.028569578933001074, val_loss : 0.4535842835903168\n",
      "Epoch 79, loss : 0.02893075901352874, val_loss : 0.45724713802337646\n",
      "Epoch 80, loss : 0.028605974165670876, val_loss : 0.449025958776474\n",
      "Epoch 81, loss : 0.02806174277270121, val_loss : 0.4512622058391571\n",
      "Epoch 82, loss : 0.027726498346703977, val_loss : 0.4504959285259247\n",
      "Epoch 83, loss : 0.027486430883918125, val_loss : 0.4536277949810028\n",
      "Epoch 84, loss : 0.0269537594017814, val_loss : 0.44992801547050476\n",
      "Epoch 85, loss : 0.026718306209596882, val_loss : 0.44828835129737854\n",
      "Epoch 86, loss : 0.026570076206639186, val_loss : 0.4501815736293793\n",
      "Epoch 87, loss : 0.026402382263853923, val_loss : 0.4518083930015564\n",
      "Epoch 88, loss : 0.026402669454220786, val_loss : 0.4481687545776367\n",
      "Epoch 89, loss : 0.02593809971685583, val_loss : 0.44450265169143677\n",
      "Epoch 90, loss : 0.02605866561035528, val_loss : 0.45372292399406433\n",
      "Epoch 91, loss : 0.02578891657404522, val_loss : 0.4539882242679596\n",
      "Epoch 92, loss : 0.02565114207294564, val_loss : 0.4527675211429596\n",
      "Epoch 93, loss : 0.025554647003111032, val_loss : 0.4483020007610321\n",
      "Epoch 94, loss : 0.025537629534013777, val_loss : 0.44979557394981384\n",
      "Epoch 95, loss : 0.025000558196860156, val_loss : 0.4511631429195404\n",
      "Epoch 96, loss : 0.02523532274480295, val_loss : 0.4538145661354065\n",
      "Epoch 97, loss : 0.024790509897074813, val_loss : 0.44698861241340637\n",
      "Epoch 98, loss : 0.02496567594258382, val_loss : 0.45595836639404297\n",
      "Epoch 99, loss : 0.024284115224693776, val_loss : 0.45005345344543457\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "objective_function = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, objective_function])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits：クロスエントロピーの値\n",
    "# Y = (n, n_class)\n",
    "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op) \n",
    "# 推定結果\n",
    "#correct_pred = tf.equal(tf.sign(tf.argmax(Y, axis=1) - 1), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1) - 1))\n",
    "\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0 \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            # print(mini_batch_y)\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run([loss_op], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss[0]\n",
    "        total_loss /= n_samples\n",
    "        val_loss = sess.run([loss_op], feed_dict={X: X_val, Y: y_val})\n",
    "        #print(aq)\n",
    "        #print(aq.shape)\n",
    "        #print(yq)\n",
    "        #print(bq)\n",
    "        print(\"Epoch {}, loss : {}, val_loss : {}\".format(epoch, total_loss, val_loss[0]))\n",
    "    #test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "    \n",
    "     total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "\n",
    "\n",
    "# 考察\n",
    "# 分類と回帰の変更点は２箇所\n",
    "# 評価軸の変化：tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# lossの変化：loss_op = tf.reduce_mean(tf.square(logits - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits：クロスエントロピーの値\n",
    "# Y = (n, n_class)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "a = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)\n",
    "#b = tf.argmax(tf.nn.softmax(logits), axis=1)\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op) \n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(tf.argmax(Y, axis=1) - 1), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1) - 1))\n",
    "\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            # print(mini_batch_y)\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aq = sess.run(a, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bq = sess.run(b, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #yq = sess.run(Y, feed_dict={Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        #print(aq)\n",
    "        #print(aq.shape)\n",
    "        #print(yq)\n",
    "        #print(bq)\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "\n",
    "# 考察\n",
    "# 変更点は２箇所\n",
    "# f.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# tf.sign(tf.argmax(Y, axis=1) - 1), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1) - 1))\n",
    "# 2値分類から多クラス分類が必要なので、多クラス分類を探した結果、ソフトマックス関数とセットのものを発見（厳密にはsigmoidではセットで作られた関数がなかった）\n",
    "# 多クラスになったので、argmaxで最大のインデックスを所得して差し引くことで整合を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxM0GzAi3IoS",
    "outputId": "4e2b15a6-86f9-48c6-b12d-602e981fd5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWG1ksXD3SGU",
    "outputId": "405187c2-3ab0-41e6-8bf1-774956eaa068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (5000, 784) (10000, 784)\n",
      "(55000, 10) (5000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# mnistデータ\n",
    "train, valid, test = mnist\n",
    "print(train.images.shape, valid.images.shape, test.images.shape)\n",
    "print(train.labels.shape, valid.labels.shape, test.labels.shape)\n",
    "\n",
    "#必要性はないがvalが少ないのでマージ\n",
    "X = np.concatenate([train.images, valid.images], axis=0)\n",
    "y = np.concatenate([train.labels, valid.labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBK3fTgRG13B",
    "outputId": "2f4da24d-5d19-4ab0-a84f-31d7612675c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38400, 784), (38400, 10))"
      ]
     },
     "execution_count": 130,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twyh9PEQCyr_",
    "outputId": "84b04720-a220-4e6f-e89e-37b852dcbdf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 3.1977, val_loss : 8.3072, acc : 0.976\n",
      "Epoch 1, loss : 0.5534, val_loss : 4.2393, acc : 0.981\n",
      "Epoch 2, loss : 0.2849, val_loss : 2.7110, acc : 0.983\n",
      "Epoch 3, loss : 0.1764, val_loss : 2.0689, acc : 0.988\n",
      "Epoch 4, loss : 0.1217, val_loss : 1.7613, acc : 0.990\n",
      "Epoch 5, loss : 0.0912, val_loss : 1.5421, acc : 0.990\n",
      "Epoch 6, loss : 0.0717, val_loss : 1.3005, acc : 0.991\n",
      "Epoch 7, loss : 0.0594, val_loss : 1.2026, acc : 0.992\n",
      "Epoch 8, loss : 0.0498, val_loss : 1.0969, acc : 0.993\n",
      "Epoch 9, loss : 0.0428, val_loss : 1.0321, acc : 0.993\n",
      "Epoch 10, loss : 0.0373, val_loss : 0.9764, acc : 0.991\n",
      "Epoch 11, loss : 0.0332, val_loss : 0.9575, acc : 0.991\n",
      "Epoch 12, loss : 0.0297, val_loss : 0.9513, acc : 0.992\n",
      "Epoch 13, loss : 0.0274, val_loss : 0.9316, acc : 0.992\n",
      "Epoch 14, loss : 0.0243, val_loss : 0.9105, acc : 0.992\n",
      "Epoch 15, loss : 0.0231, val_loss : 0.9045, acc : 0.991\n",
      "Epoch 16, loss : 0.0213, val_loss : 0.8709, acc : 0.993\n",
      "Epoch 17, loss : 0.0199, val_loss : 0.8724, acc : 0.994\n",
      "Epoch 18, loss : 0.0186, val_loss : 0.8855, acc : 0.992\n",
      "Epoch 19, loss : 0.0171, val_loss : 0.8858, acc : 0.993\n",
      "Epoch 20, loss : 0.0163, val_loss : 0.8859, acc : 0.992\n",
      "Epoch 21, loss : 0.0149, val_loss : 0.9202, acc : 0.993\n",
      "Epoch 22, loss : 0.0145, val_loss : 0.9096, acc : 0.992\n",
      "Epoch 23, loss : 0.0135, val_loss : 0.8701, acc : 0.993\n",
      "Epoch 24, loss : 0.0129, val_loss : 0.9240, acc : 0.992\n",
      "Epoch 25, loss : 0.0124, val_loss : 0.8840, acc : 0.993\n",
      "Epoch 26, loss : 0.0116, val_loss : 0.9142, acc : 0.992\n",
      "Epoch 27, loss : 0.0113, val_loss : 0.8931, acc : 0.994\n",
      "Epoch 28, loss : 0.0106, val_loss : 0.8974, acc : 0.992\n",
      "Epoch 29, loss : 0.0101, val_loss : 0.9062, acc : 0.992\n",
      "Epoch 30, loss : 0.0097, val_loss : 0.8863, acc : 0.993\n",
      "Epoch 31, loss : 0.0099, val_loss : 0.9202, acc : 0.994\n",
      "Epoch 32, loss : 0.0091, val_loss : 0.9087, acc : 0.994\n",
      "Epoch 33, loss : 0.0084, val_loss : 0.9061, acc : 0.993\n",
      "Epoch 34, loss : 0.0085, val_loss : 0.9350, acc : 0.994\n",
      "Epoch 35, loss : 0.0085, val_loss : 0.9323, acc : 0.992\n",
      "Epoch 36, loss : 0.0078, val_loss : 0.9353, acc : 0.994\n",
      "Epoch 37, loss : 0.0072, val_loss : 0.9255, acc : 0.994\n",
      "Epoch 38, loss : 0.0071, val_loss : 0.9066, acc : 0.992\n",
      "Epoch 39, loss : 0.0069, val_loss : 0.9475, acc : 0.993\n",
      "Epoch 40, loss : 0.0067, val_loss : 0.8889, acc : 0.994\n",
      "Epoch 41, loss : 0.0066, val_loss : 0.9436, acc : 0.993\n",
      "Epoch 42, loss : 0.0062, val_loss : 0.9378, acc : 0.993\n",
      "Epoch 43, loss : 0.0062, val_loss : 0.9917, acc : 0.993\n",
      "Epoch 44, loss : 0.0058, val_loss : 0.9716, acc : 0.994\n",
      "Epoch 45, loss : 0.0057, val_loss : 0.9655, acc : 0.993\n",
      "Epoch 46, loss : 0.0053, val_loss : 0.9640, acc : 0.994\n",
      "Epoch 47, loss : 0.0053, val_loss : 1.0100, acc : 0.994\n",
      "Epoch 48, loss : 0.0052, val_loss : 0.9324, acc : 0.995\n",
      "Epoch 49, loss : 0.0052, val_loss : 0.9703, acc : 0.995\n",
      "Epoch 50, loss : 0.0050, val_loss : 1.0275, acc : 0.994\n",
      "Epoch 51, loss : 0.0047, val_loss : 0.9972, acc : 0.995\n",
      "Epoch 52, loss : 0.0049, val_loss : 0.9694, acc : 0.995\n",
      "Epoch 53, loss : 0.0045, val_loss : 0.9931, acc : 0.995\n",
      "Epoch 54, loss : 0.0043, val_loss : 0.9964, acc : 0.995\n",
      "Epoch 55, loss : 0.0044, val_loss : 0.9956, acc : 0.995\n",
      "Epoch 56, loss : 0.0039, val_loss : 1.0603, acc : 0.995\n",
      "Epoch 57, loss : 0.0044, val_loss : 1.0206, acc : 0.995\n",
      "Epoch 58, loss : 0.0040, val_loss : 1.0712, acc : 0.994\n",
      "Epoch 59, loss : 0.0042, val_loss : 1.0313, acc : 0.994\n",
      "Epoch 60, loss : 0.0040, val_loss : 1.0284, acc : 0.994\n",
      "Epoch 61, loss : 0.0038, val_loss : 1.0329, acc : 0.996\n",
      "Epoch 62, loss : 0.0038, val_loss : 1.0481, acc : 0.995\n",
      "Epoch 63, loss : 0.0037, val_loss : 1.0515, acc : 0.994\n",
      "Epoch 64, loss : 0.0039, val_loss : 1.0886, acc : 0.995\n",
      "Epoch 65, loss : 0.0037, val_loss : 1.0811, acc : 0.994\n",
      "Epoch 66, loss : 0.0033, val_loss : 1.0929, acc : 0.996\n",
      "Epoch 67, loss : 0.0033, val_loss : 1.0580, acc : 0.994\n",
      "Epoch 68, loss : 0.0035, val_loss : 1.1180, acc : 0.995\n",
      "Epoch 69, loss : 0.0035, val_loss : 1.1227, acc : 0.995\n",
      "Epoch 70, loss : 0.0032, val_loss : 1.1104, acc : 0.995\n",
      "Epoch 71, loss : 0.0033, val_loss : 1.1434, acc : 0.995\n",
      "Epoch 72, loss : 0.0030, val_loss : 1.1059, acc : 0.995\n",
      "Epoch 73, loss : 0.0030, val_loss : 1.1081, acc : 0.995\n",
      "Epoch 74, loss : 0.0028, val_loss : 1.0706, acc : 0.995\n",
      "Epoch 75, loss : 0.0028, val_loss : 1.1320, acc : 0.994\n",
      "Epoch 76, loss : 0.0029, val_loss : 1.0641, acc : 0.994\n",
      "Epoch 77, loss : 0.0027, val_loss : 1.0954, acc : 0.995\n",
      "Epoch 78, loss : 0.0027, val_loss : 1.0882, acc : 0.995\n",
      "Epoch 79, loss : 0.0027, val_loss : 1.1269, acc : 0.993\n",
      "Epoch 80, loss : 0.0028, val_loss : 1.1785, acc : 0.994\n",
      "Epoch 81, loss : 0.0026, val_loss : 1.1010, acc : 0.995\n",
      "Epoch 82, loss : 0.0026, val_loss : 1.1111, acc : 0.995\n",
      "Epoch 83, loss : 0.0023, val_loss : 1.1271, acc : 0.994\n",
      "Epoch 84, loss : 0.0026, val_loss : 1.1172, acc : 0.994\n",
      "Epoch 85, loss : 0.0023, val_loss : 1.1551, acc : 0.995\n",
      "Epoch 86, loss : 0.0023, val_loss : 1.1617, acc : 0.995\n",
      "Epoch 87, loss : 0.0023, val_loss : 1.1292, acc : 0.995\n",
      "Epoch 88, loss : 0.0024, val_loss : 1.1780, acc : 0.995\n",
      "Epoch 89, loss : 0.0023, val_loss : 1.1910, acc : 0.994\n",
      "Epoch 90, loss : 0.0023, val_loss : 1.1672, acc : 0.995\n",
      "Epoch 91, loss : 0.0021, val_loss : 1.1976, acc : 0.995\n",
      "Epoch 92, loss : 0.0021, val_loss : 1.2122, acc : 0.994\n",
      "Epoch 93, loss : 0.0022, val_loss : 1.1905, acc : 0.995\n",
      "Epoch 94, loss : 0.0021, val_loss : 1.2416, acc : 0.994\n",
      "Epoch 95, loss : 0.0021, val_loss : 1.2190, acc : 0.994\n",
      "Epoch 96, loss : 0.0020, val_loss : 1.2778, acc : 0.995\n",
      "Epoch 97, loss : 0.0022, val_loss : 1.2633, acc : 0.996\n",
      "Epoch 98, loss : 0.0017, val_loss : 1.2065, acc : 0.995\n",
      "Epoch 99, loss : 0.0019, val_loss : 1.2394, acc : 0.995\n",
      "test_acc : 0.993\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits：クロスエントロピーの値\n",
    "# Y = (n, n_class)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "a = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)\n",
    "#b = tf.argmax(tf.nn.softmax(logits), axis=1)\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op) \n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(tf.argmax(Y, axis=1)), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1)))\n",
    "\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            # print(mini_batch_y)\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #aq = sess.run(a, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #bq = sess.run(b, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #yq = sess.run(Y, feed_dict={Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        #print(aq)\n",
    "        #print(aq.shape)\n",
    "        #print(yq)\n",
    "        #print(bq)\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "\n",
    "# 考察\n",
    "# 変更点は２箇所\n",
    "# f.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# tf.sign(tf.argmax(Y, axis=1) - 1), tf.sign(tf.argmax(tf.nn.softmax(logits), axis=1) - 1))\n",
    "# 2値分類から多クラス分類が必要なので、多クラス分類を探した結果、ソフトマックス関数とセットのものを発見（厳密にはsigmoidではセットで作られた関数がなかった）\n",
    "# 多クラスになったので、argmaxで最大のインデックスを所得して差し引くことで整合を確認\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week10_tensorflow_framework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
